{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Target Dataset Hyperparameters Tunning\n",
    "The objective of this notebook is to tune the hyperparameters of the model to obtain the best performance on the sparse dataset. \n",
    "\n",
    "**Summary of the Article**\n",
    "- Description of the dataset.\n",
    "- Hyperparameters tunning:\n",
    "    - Gradient Boosting Regressor.\n",
    "    - SVRegressor.\n",
    "    - Multi-Layer Perceptron.\n",
    "    - Long-Short Term Memory.\n",
    "- Training Models.\n",
    "- Next Steps.\n",
    "\n",
    "## Description of the Sparse Dataset \n",
    "The objective of this master thesis is to forecast the occurance and amplitude of constraints in the electrical grid. Using historical values of active and reactive power it is possible to compute the voltage and current values in the network, thus obtaining the occurance and amplitude of constraints. Since not every timestep containts a constraint, not every time step is woth to be predicted, so it is usefull transform the target features into a sequece of values that better represent the constraints. One way to obtain this target dataset is to set all time steps that do not characterise a constraint to 0, a positive value (with the amplitude of the constraint) otherwise. The following formula states the transformation:\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\text{Target} &= \\begin{cases}\n",
    "            0 & \\text{if} \\; \\text{constraint} \\; \\text{is not violated} \\\\\n",
    "            \\text{amplitude of constraint} & \\text{if} \\; \\text{constraint} \\; \\text{is violated} \\\\\n",
    "        \\end{cases}\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "The resulting dataset is a sparse dataset, since constraints are not as common as regular values. Bellow the dataset for maximum voltage constraints is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from thesis_package import utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns='timestamps')\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "X_max_u_train, X_max_u_test, y_max_u_train, y_max_u_test = utils.split_and_suffle(exogenous_data, y_max_u)\n",
    "data = {'X_train': X_max_u_train, 'X_test': X_max_u_test, 'y_train': y_max_u_train, 'y_test': y_max_u_test}\n",
    "threshold_value = y_max_u_train.loc[:, y_max_u_train.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "threshold_signal = pd.Series(np.ones([len(y_max_u_test)]) * threshold_value)\n",
    "# Plot prediction_gb_max_u\n",
    "sns.set(style='whitegrid')\n",
    "fig, axs = plt.subplots(1, 1, figsize=(30, 10))\n",
    "axs.plot(y_max_u_test.reset_index(drop=True))\n",
    "axs.plot(threshold_signal)\n",
    "axs.set_title('Dataset Sample of Maximum Voltage Constraint', fontsize=30, fontweight='bold')\n",
    "axs.set_xlabel('Timestep', fontsize=18, fontweight='bold')\n",
    "axs.set_ylabel('Constraint Value', fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse boolean dataset is derived from the one above. It represents the time steps with constraints as of the class 1 and the rest as class 0. This datset is used to train the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns='timestamps')\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "X_max_u_sparse_bool_train, X_max_u_sparse_bool_test, y_max_u_sparse_bool_train, y_max_u_sparse_bool_test = utils.split_and_suffle(exogenous_data, y_max_u_sparse_bool)\n",
    "data = {'X_train': X_max_u_sparse_bool_train,\n",
    "        'X_test': X_max_u_sparse_bool_test,\n",
    "        'y_train': utils.convert_df_to_bool(y_max_u_sparse_bool_train),\n",
    "        'y_test': utils.convert_df_to_bool(y_max_u_sparse_bool_test)}\n",
    "# Plot prediction_gb_max_u\n",
    "sns.set(style='whitegrid')\n",
    "fig, axs = plt.subplots(1, 1, figsize=(30, 10))\n",
    "axs.plot(y_max_u_sparse_bool_test.reset_index(drop=True))\n",
    "axs.plot(threshold_signal)\n",
    "axs.set_title('Dataset Sample of Maximum Voltage Constraint Occurrences', fontsize=30, fontweight='bold')\n",
    "axs.set_xlabel('Timestep', fontsize=18, fontweight='bold')\n",
    "axs.set_ylabel('Constraint Value', fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, not all busses have constraints, so it is only possible train classification models for those busses that have positive values in the target dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.cols_with_positive_values(y_max_u_sparse_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from thesis_package import aimodels as my_ai, utils, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Tunning of Models\n",
    "The objective of this section is to tune the hyperparameters of the models to obtain the best performance on the sparse dataset. In order to perfom the hyperparameters tunning, we are going to use the optuna library presented in the `optuna_introduction.ipynb` notebook. The models are the ones implemented in the `aimodel.py` file in the `thesis_package`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters\n",
    "num_trials = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_xgboost_regression_sparse_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_xgboost_regression_sparse_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered dataset\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u[utils.cols_with_positive_values(y_max_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_xgboost_regression_filtered_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERED DATASET\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u[utils.cols_with_positive_values(y_min_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_xgboost_regression_filtered_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Gradient Boosting Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_gradient_boost_regression_sparse_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Gradient Boosting Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_gradient_boost_regression_sparse_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERED DATA\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u[utils.cols_with_positive_values(y_max_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_gradient_boost_regression_filtered_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERED DATA\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u[utils.cols_with_positive_values(y_min_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_gradient_boost_regression_filtered_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement as above but for multi-output classification grandient boost classifier.\n",
    "def objective(trial):\n",
    "    # import data0 \n",
    "    y_max_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    y_max_u_sparse_bool = utils.convert_df_to_bool(y_max_u_sparse_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse_bool[utils.cols_with_positive_values(y_max_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'deviance', 'exponential']) \n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    # evaluate the classification performance f1 score\n",
    "    f1_score = sklearn.metrics.f1_score(valid_y, prediction, average='micro')\n",
    "    return f1_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_gradient_boost_sparse_classifier_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement as above but for multi-output classification grandient boost classifier.\n",
    "def objective(trial):\n",
    "    # import data0 \n",
    "    y_min_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    y_min_u_sparse_bool = utils.convert_df_to_bool(y_min_u_sparse_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse_bool[utils.cols_with_positive_values(y_min_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'deviance', 'exponential']) \n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    # evaluate the classification performance f1 score\n",
    "    f1_score = sklearn.metrics.f1_score(valid_y, prediction, average='micro')\n",
    "    return f1_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_gradient_boost_sparse_classifier_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Support Vector Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=y_max_u.columns)\n",
    "    train_y = pd.DataFrame(train_y, columns=y_max_u.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_support_vector_regression_sparse_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Support Vector Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=y_min_u.columns)\n",
    "    train_y = pd.DataFrame(train_y, columns=y_min_u.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_support_vector_regression_sparse_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERED DATA\n",
    "# Same implementation as above, but for Support Vector Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u[utils.cols_with_positive_values(y_max_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=utils.cols_with_positive_values(y_max_u))\n",
    "    valid_y = pd.DataFrame(valid_y, columns=utils.cols_with_positive_values(y_max_u))\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_support_vector_regression_filtered_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-27 09:46:43,624]\u001b[0m Trial 13 finished with value: 0.46119375456397177 and parameters: {'kernel': 'rbf', 'C': 0.8239229067823421, 'degree': 5, 'gamma': 0.5404210283283881}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 09:56:09,490]\u001b[0m Trial 14 finished with value: 0.10430241805713941 and parameters: {'kernel': 'rbf', 'C': 0.0651343331670824, 'degree': 5, 'gamma': 2.5118150239049213e-08}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:06:52,395]\u001b[0m Trial 15 finished with value: 0.22227426823451502 and parameters: {'kernel': 'rbf', 'C': 0.0002066255409138143, 'degree': 5, 'gamma': 0.0527430404849878}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:10:18,224]\u001b[0m Trial 16 finished with value: 0.41843535453391933 and parameters: {'kernel': 'rbf', 'C': 0.13606149760892977, 'degree': 2, 'gamma': 0.0713819014845733}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:16:40,948]\u001b[0m Trial 17 finished with value: 0.3782182348978474 and parameters: {'kernel': 'rbf', 'C': 0.0008014376862617412, 'degree': 4, 'gamma': 0.13747826438433347}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:23:35,120]\u001b[0m Trial 18 finished with value: 0.37927280239569555 and parameters: {'kernel': 'rbf', 'C': 0.11416642240990532, 'degree': 3, 'gamma': 0.013989222879646216}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:25:27,275]\u001b[0m Trial 19 finished with value: 0.4745112202472615 and parameters: {'kernel': 'rbf', 'C': 0.011694488502112924, 'degree': 5, 'gamma': 0.711323699298081}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:36:04,121]\u001b[0m Trial 20 finished with value: 0.10540330827280892 and parameters: {'kernel': 'rbf', 'C': 3.3239651417296276e-05, 'degree': 2, 'gamma': 5.975704609912849e-07}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:37:48,367]\u001b[0m Trial 21 finished with value: 0.48411858778741046 and parameters: {'kernel': 'rbf', 'C': 0.9922062138618388, 'degree': 5, 'gamma': 0.8060600682943528}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:39:48,080]\u001b[0m Trial 22 finished with value: 0.4358446651969448 and parameters: {'kernel': 'rbf', 'C': 0.6104759104709482, 'degree': 5, 'gamma': 0.14523816629913472}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:43:04,186]\u001b[0m Trial 23 finished with value: 0.43357074415624614 and parameters: {'kernel': 'rbf', 'C': 0.16472624902687832, 'degree': 4, 'gamma': 0.20847538571060867}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:50:32,913]\u001b[0m Trial 24 finished with value: 0.36977055811463666 and parameters: {'kernel': 'rbf', 'C': 0.004333396943104009, 'degree': 5, 'gamma': 0.022596330214002848}. Best is trial 10 with value: 0.5005778886217904.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  25\n",
      "Best trial:\n",
      "  Value: 0.5005778886217904\n",
      "  Params: \n",
      "    kernel: rbf\n",
      "    C: 0.9020866240447369\n",
      "    degree: 5\n",
      "    gamma: 0.9856824581182914\n"
     ]
    }
   ],
   "source": [
    "# DATA FILTERED\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u[utils.cols_with_positive_values(y_min_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=utils.cols_with_positive_values(y_min_u))\n",
    "    train_y = pd.DataFrame(train_y, columns=utils.cols_with_positive_values(y_min_u))\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = valid_y.loc[:, valid_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.hybrid_f1\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_support_vector_regression_filtered_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-27 10:50:33,103]\u001b[0m A new study created in memory with name: no-name-3c6a9ca2-d687-4722-9948-b0dbbc0de166\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:51:41,137]\u001b[0m Trial 0 finished with value: 0.11236610273315244 and parameters: {'booster': 'dart', 'lambda': 3.7578203085843715e-08, 'alpha': 0.13091760057756843, 'subsample': 0.3250664465645433, 'colsample_bytree': 0.2529448463724207, 'max_depth': 3, 'min_child_weight': 2, 'eta': 0.08937854798776496, 'gamma': 0.00030801397288414656, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.03256260418998659, 'skip_drop': 3.1898991425536735e-06}. Best is trial 0 with value: 0.11236610273315244.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:53:21,616]\u001b[0m Trial 1 finished with value: 0.2925091341327546 and parameters: {'booster': 'dart', 'lambda': 1.0257737674341825e-06, 'alpha': 4.804893548147811e-07, 'subsample': 0.755389531998065, 'colsample_bytree': 0.9095410681777893, 'max_depth': 5, 'min_child_weight': 4, 'eta': 0.0028920632792636064, 'gamma': 1.4839867759595449e-06, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 3.116912068219922e-07, 'skip_drop': 1.565320963979516e-07}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:54:37,071]\u001b[0m Trial 2 finished with value: 0.10218880314069827 and parameters: {'booster': 'dart', 'lambda': 1.5617098505546464e-08, 'alpha': 0.0068249597983813534, 'subsample': 0.43501036646905455, 'colsample_bytree': 0.8435420821485182, 'max_depth': 3, 'min_child_weight': 2, 'eta': 7.501957692695335e-08, 'gamma': 0.0016298398018108932, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 6.388348155922195e-05, 'skip_drop': 0.013359061847822153}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:54:54,833]\u001b[0m Trial 3 finished with value: 0.0027210884353741495 and parameters: {'booster': 'gbtree', 'lambda': 6.865426209818425e-06, 'alpha': 0.09476422765569306, 'subsample': 0.31922857549825656, 'colsample_bytree': 0.7417155731236851, 'max_depth': 3, 'min_child_weight': 5, 'eta': 1.1655539042573447e-06, 'gamma': 0.922181028904235, 'grow_policy': 'lossguide'}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:55:16,882]\u001b[0m Trial 4 finished with value: 0.009499442575064495 and parameters: {'booster': 'gbtree', 'lambda': 0.018616590905055155, 'alpha': 8.759572346243594e-06, 'subsample': 0.4140723335247201, 'colsample_bytree': 0.5225436295336477, 'max_depth': 5, 'min_child_weight': 2, 'eta': 6.1011839731551854e-05, 'gamma': 0.0065330669118986995, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:55:44,161]\u001b[0m Trial 5 finished with value: 0.007088557716970681 and parameters: {'booster': 'gbtree', 'lambda': 0.0011383675974686739, 'alpha': 9.37477344852871e-08, 'subsample': 0.6893979093495826, 'colsample_bytree': 0.49895086172044306, 'max_depth': 5, 'min_child_weight': 4, 'eta': 0.00030097390645979504, 'gamma': 0.023296885307255703, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:56:09,556]\u001b[0m Trial 6 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 2.1085960761299943e-08, 'alpha': 9.171422317499692e-07, 'subsample': 0.3933034680701118, 'colsample_bytree': 0.236921511676535, 'max_depth': 7, 'min_child_weight': 2, 'eta': 4.2712504610672944e-08, 'gamma': 0.0001501232094322258, 'grow_policy': 'lossguide'}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:57:38,866]\u001b[0m Trial 7 finished with value: 0.2754379272096905 and parameters: {'booster': 'dart', 'lambda': 0.15762615322156842, 'alpha': 8.19585534101053e-08, 'subsample': 0.403373476889328, 'colsample_bytree': 0.8633281567988809, 'max_depth': 7, 'min_child_weight': 3, 'eta': 0.00021513885328130637, 'gamma': 1.1672846907117162e-08, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.0002408195213986413, 'skip_drop': 0.11413040853369999}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:57:43,237]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 1.4419282312457874e-07, 'alpha': 0.05322033543908178, 'subsample': 0.7618204281471039, 'colsample_bytree': 0.4277285458396331}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:58:11,235]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 3.440485193455087e-08, 'alpha': 0.24380019623176893, 'subsample': 0.49811022838012003, 'colsample_bytree': 0.2819043295217711, 'max_depth': 9, 'min_child_weight': 4, 'eta': 5.0531821805025494e-05, 'gamma': 0.025057230357173013, 'grow_policy': 'lossguide'}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:58:16,131]\u001b[0m Trial 10 finished with value: 0.28245749734010644 and parameters: {'booster': 'gblinear', 'lambda': 5.786828209114232e-06, 'alpha': 0.00011635896000563419, 'subsample': 0.9456544274683643, 'colsample_bytree': 0.6889458224829808}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:58:23,250]\u001b[0m Trial 11 finished with value: 0.27804065855330345 and parameters: {'booster': 'gblinear', 'lambda': 1.4956980832617574e-05, 'alpha': 0.0001563540554074796, 'subsample': 0.9735960279655421, 'colsample_bytree': 0.9970071165014361}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:58:27,836]\u001b[0m Trial 12 finished with value: 0.14166234664104038 and parameters: {'booster': 'gblinear', 'lambda': 1.4198437929667824e-06, 'alpha': 0.0014925295073912115, 'subsample': 0.9347607126246319, 'colsample_bytree': 0.660280536530559}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 10:58:32,391]\u001b[0m Trial 13 finished with value: 0.2325259704758717 and parameters: {'booster': 'gblinear', 'lambda': 0.00024042034250580568, 'alpha': 1.8360701432922254e-05, 'subsample': 0.8209818060146311, 'colsample_bytree': 0.9985981189122559}. Best is trial 1 with value: 0.2925091341327546.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:00:24,502]\u001b[0m Trial 14 finished with value: 0.34237847092747636 and parameters: {'booster': 'dart', 'lambda': 9.554397305934778e-07, 'alpha': 2.947937450003672e-06, 'subsample': 0.8687643606500915, 'colsample_bytree': 0.7791719490929919, 'max_depth': 9, 'min_child_weight': 8, 'eta': 0.42475894001117176, 'gamma': 4.305306271040159e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.2285875321663425e-08, 'skip_drop': 1.5157296688541982e-08}. Best is trial 14 with value: 0.34237847092747636.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:02:08,594]\u001b[0m Trial 15 finished with value: 0.34726036587280573 and parameters: {'booster': 'dart', 'lambda': 3.9910045119516196e-07, 'alpha': 8.925763090425721e-07, 'subsample': 0.5937769480341094, 'colsample_bytree': 0.8113560753899801, 'max_depth': 9, 'min_child_weight': 8, 'eta': 0.5854854635114861, 'gamma': 6.422579110599194e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.2035759365567883e-08, 'skip_drop': 1.0721473622384038e-08}. Best is trial 15 with value: 0.34726036587280573.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:03:47,246]\u001b[0m Trial 16 finished with value: 0.3565422853036869 and parameters: {'booster': 'dart', 'lambda': 3.898106781307988e-07, 'alpha': 3.134560885398455e-06, 'subsample': 0.5385640450024718, 'colsample_bytree': 0.7836921551694438, 'max_depth': 9, 'min_child_weight': 9, 'eta': 0.7631934347360848, 'gamma': 1.0488668118496473e-06, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.3608522827355205e-08, 'skip_drop': 1.6758333044998636e-08}. Best is trial 16 with value: 0.3565422853036869.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:05:48,073]\u001b[0m Trial 17 finished with value: 0.24291972618918387 and parameters: {'booster': 'dart', 'lambda': 5.733335856318991e-05, 'alpha': 1.7157498421901413e-08, 'subsample': 0.5886373112544363, 'colsample_bytree': 0.5986858905995195, 'max_depth': 9, 'min_child_weight': 10, 'eta': 0.024383966060405972, 'gamma': 2.8821504129038335e-06, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.0826452618968847e-08, 'skip_drop': 4.295592856676601e-05}. Best is trial 16 with value: 0.3565422853036869.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:07:24,194]\u001b[0m Trial 18 finished with value: 0.35868327402477035 and parameters: {'booster': 'dart', 'lambda': 2.5205156912118025e-07, 'alpha': 4.3495039622290566e-05, 'subsample': 0.5926557129457888, 'colsample_bytree': 0.7840650212525502, 'max_depth': 7, 'min_child_weight': 8, 'eta': 0.8563734891303493, 'gamma': 5.162916427230179e-08, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 2.0297502208632522e-06, 'skip_drop': 1.4547292758426174e-08}. Best is trial 18 with value: 0.35868327402477035.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:09:01,129]\u001b[0m Trial 19 finished with value: 0.1641272949258474 and parameters: {'booster': 'dart', 'lambda': 0.00034618354724318405, 'alpha': 0.0004857867083713582, 'subsample': 0.2109554007217096, 'colsample_bytree': 0.5992101539327955, 'max_depth': 7, 'min_child_weight': 8, 'eta': 0.010367590208654872, 'gamma': 2.597983358253936e-08, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.9804778049719897e-06, 'skip_drop': 1.5677142563844035e-06}. Best is trial 18 with value: 0.35868327402477035.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:10:33,720]\u001b[0m Trial 20 finished with value: 0.35918702265652536 and parameters: {'booster': 'dart', 'lambda': 0.003063420575512009, 'alpha': 1.9003021085468398e-05, 'subsample': 0.5229147527577294, 'colsample_bytree': 0.7098150896390416, 'max_depth': 7, 'min_child_weight': 10, 'eta': 0.6910312611856446, 'gamma': 1.6977649889061874e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 2.7388226591786135e-06, 'skip_drop': 0.0005988929023034813}. Best is trial 20 with value: 0.35918702265652536.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:12:07,250]\u001b[0m Trial 21 finished with value: 0.3452513535415504 and parameters: {'booster': 'dart', 'lambda': 0.003504817217958439, 'alpha': 2.5671192911731652e-05, 'subsample': 0.5233753382856006, 'colsample_bytree': 0.7166780316620681, 'max_depth': 7, 'min_child_weight': 10, 'eta': 0.9469189718521861, 'gamma': 1.1161820223303198e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 2.7237931059147862e-06, 'skip_drop': 0.004046481639223986}. Best is trial 20 with value: 0.35918702265652536.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:13:40,354]\u001b[0m Trial 22 finished with value: 0.34709470393830355 and parameters: {'booster': 'dart', 'lambda': 0.04255476054183828, 'alpha': 3.1063831242549955e-05, 'subsample': 0.6485453819018341, 'colsample_bytree': 0.89278800256399, 'max_depth': 7, 'min_child_weight': 9, 'eta': 0.08808019688080355, 'gamma': 1.3189995213587581e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.780329576647689e-05, 'skip_drop': 0.0003056251926467483}. Best is trial 20 with value: 0.35918702265652536.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:15:25,722]\u001b[0m Trial 23 finished with value: 0.22219598846230712 and parameters: {'booster': 'dart', 'lambda': 4.033695417014328e-05, 'alpha': 0.0025401498868182675, 'subsample': 0.5218415512512661, 'colsample_bytree': 0.6559404414359096, 'max_depth': 9, 'min_child_weight': 7, 'eta': 0.0024333740513699537, 'gamma': 9.384335481708988e-08, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.0019406571612325103, 'skip_drop': 0.0001813244672161822}. Best is trial 20 with value: 0.35918702265652536.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:16:57,203]\u001b[0m Trial 24 finished with value: 0.33305057774176505 and parameters: {'booster': 'dart', 'lambda': 0.830466053473156, 'alpha': 3.7932608033207366e-06, 'subsample': 0.6415767861998612, 'colsample_bytree': 0.7786379510526649, 'max_depth': 7, 'min_child_weight': 9, 'eta': 0.05982499247881299, 'gamma': 1.273003994592814e-07, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 2.909843773642178e-07, 'skip_drop': 2.673963619341845e-07}. Best is trial 20 with value: 0.35918702265652536.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  25\n",
      "Best trial:\n",
      "  Value: 0.35918702265652536\n",
      "  Params: \n",
      "    booster: dart\n",
      "    lambda: 0.003063420575512009\n",
      "    alpha: 1.9003021085468398e-05\n",
      "    subsample: 0.5229147527577294\n",
      "    colsample_bytree: 0.7098150896390416\n",
      "    max_depth: 7\n",
      "    min_child_weight: 10\n",
      "    eta: 0.6910312611856446\n",
      "    gamma: 1.6977649889061874e-05\n",
      "    grow_policy: depthwise\n",
      "    sample_type: uniform\n",
      "    normalize_type: forest\n",
      "    rate_drop: 2.7388226591786135e-06\n",
      "    skip_drop: 0.0005988929023034813\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    y_max_u = utils.convert_df_to_bool(y_max_u)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u[utils.cols_with_positive_values(y_max_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    f1_score = sklearn.metrics.f1_score(valid_y, prediction, average='macro')\n",
    "    return f1_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_xgboost_sparse_classifier_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-27 11:16:57,407]\u001b[0m A new study created in memory with name: no-name-68dbabbc-ac6d-4f22-9a08-8589e9f534b8\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:17:17,364]\u001b[0m Trial 0 finished with value: 0.008650193597244445 and parameters: {'booster': 'gbtree', 'lambda': 7.241200307390309e-06, 'alpha': 0.8323696690672764, 'subsample': 0.3400884841451709, 'colsample_bytree': 0.47454423183396327, 'max_depth': 5, 'min_child_weight': 6, 'eta': 0.00039642912221090715, 'gamma': 2.002898169197888e-07, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.008650193597244445.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:17:21,264]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 1.5352027601239485e-07, 'alpha': 0.43926341400445895, 'subsample': 0.3331469749897885, 'colsample_bytree': 0.4209093484140642}. Best is trial 0 with value: 0.008650193597244445.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:18:48,603]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 8.547315610968203e-07, 'alpha': 0.015845306273098034, 'subsample': 0.9645922781133149, 'colsample_bytree': 0.3455170932460588, 'max_depth': 5, 'min_child_weight': 9, 'eta': 1.9336404921900997e-08, 'gamma': 7.369013226325127e-07, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.00033799845793564074, 'skip_drop': 1.73171636999593e-06}. Best is trial 0 with value: 0.008650193597244445.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:20:08,421]\u001b[0m Trial 3 finished with value: 0.3851052381240679 and parameters: {'booster': 'dart', 'lambda': 1.2866426322038222e-08, 'alpha': 1.4077234760800385e-08, 'subsample': 0.9000414158522243, 'colsample_bytree': 0.9776369384235559, 'max_depth': 3, 'min_child_weight': 3, 'eta': 0.3908033368168215, 'gamma': 0.00030562541141213387, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 4.215024097563663e-08, 'skip_drop': 0.0024257495864115092}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:20:13,030]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 0.43598014578477184, 'alpha': 0.0004035378389470072, 'subsample': 0.6432495049070868, 'colsample_bytree': 0.49677336213574014}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:21:45,381]\u001b[0m Trial 5 finished with value: 0.203265113050149 and parameters: {'booster': 'dart', 'lambda': 0.000112344167369208, 'alpha': 0.29306269006899677, 'subsample': 0.6456783577650738, 'colsample_bytree': 0.7657771359805605, 'max_depth': 5, 'min_child_weight': 3, 'eta': 2.6883373032495396e-07, 'gamma': 1.9152489316945577e-06, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.0001198879440341287, 'skip_drop': 1.3992329369140053e-08}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:22:17,329]\u001b[0m Trial 6 finished with value: 0.1899500566877816 and parameters: {'booster': 'gbtree', 'lambda': 1.4757607533243655e-06, 'alpha': 6.282817375391111e-06, 'subsample': 0.2942610703901541, 'colsample_bytree': 0.7718034042572111, 'max_depth': 7, 'min_child_weight': 3, 'eta': 1.2209714326521501e-06, 'gamma': 0.00020147918212501457, 'grow_policy': 'lossguide'}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:22:54,918]\u001b[0m Trial 7 finished with value: 0.0590643024181662 and parameters: {'booster': 'gbtree', 'lambda': 2.094062057537879e-06, 'alpha': 0.006842562768140911, 'subsample': 0.981141366063446, 'colsample_bytree': 0.5197478104705802, 'max_depth': 7, 'min_child_weight': 3, 'eta': 3.22000550112797e-08, 'gamma': 0.0016893921092472571, 'grow_policy': 'depthwise'}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:23:23,251]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 4.838145732977221e-07, 'alpha': 1.6629673742546883e-05, 'subsample': 0.5268920385284308, 'colsample_bytree': 0.21278263145115286, 'max_depth': 7, 'min_child_weight': 5, 'eta': 8.640585605801698e-06, 'gamma': 0.06410606103116794, 'grow_policy': 'lossguide'}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:23:27,878]\u001b[0m Trial 9 finished with value: 0.03783441849862514 and parameters: {'booster': 'gblinear', 'lambda': 5.176584279868995e-05, 'alpha': 0.004383422153149239, 'subsample': 0.26852933442162236, 'colsample_bytree': 0.4705505702424205}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:24:47,157]\u001b[0m Trial 10 finished with value: 0.381776148135449 and parameters: {'booster': 'dart', 'lambda': 1.592740422312658e-08, 'alpha': 3.846152296037566e-08, 'subsample': 0.8330123138879995, 'colsample_bytree': 0.9626115060551413, 'max_depth': 3, 'min_child_weight': 10, 'eta': 0.3530581798386966, 'gamma': 0.5238989215506108, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.3159290993393974e-08, 'skip_drop': 0.09031716090605617}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:26:06,820]\u001b[0m Trial 11 finished with value: 0.34292518069850997 and parameters: {'booster': 'dart', 'lambda': 1.7473398241499732e-08, 'alpha': 1.2332808749264075e-08, 'subsample': 0.8034032842020755, 'colsample_bytree': 0.9846552515393627, 'max_depth': 3, 'min_child_weight': 10, 'eta': 0.9350049483795083, 'gamma': 0.9977893039420493, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.0802980051866571e-08, 'skip_drop': 0.15956528793430466}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:27:25,895]\u001b[0m Trial 12 finished with value: 0.31943300217700804 and parameters: {'booster': 'dart', 'lambda': 1.2502723011911691e-08, 'alpha': 1.7755838800971e-08, 'subsample': 0.8270242694504877, 'colsample_bytree': 0.9975649964708979, 'max_depth': 3, 'min_child_weight': 8, 'eta': 0.9421195158972051, 'gamma': 0.007658475548305905, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.2008686650132824e-08, 'skip_drop': 0.055899049609203964}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:28:46,291]\u001b[0m Trial 13 finished with value: 0.25326629127208067 and parameters: {'booster': 'dart', 'lambda': 0.010229626974144026, 'alpha': 3.799844022049709e-07, 'subsample': 0.8221960745126051, 'colsample_bytree': 0.8294856012354529, 'max_depth': 3, 'min_child_weight': 7, 'eta': 0.007549651804904936, 'gamma': 0.6439577536447976, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 4.519588061630371e-07, 'skip_drop': 0.0013486819123380352}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:30:56,349]\u001b[0m Trial 14 finished with value: 0.33551812414299137 and parameters: {'booster': 'dart', 'lambda': 5.9067130694270605e-08, 'alpha': 2.9470508000539773e-07, 'subsample': 0.7470235969265436, 'colsample_bytree': 0.8745119351025892, 'max_depth': 9, 'min_child_weight': 5, 'eta': 0.01813255173569668, 'gamma': 4.165449920709158e-05, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.11764865353057541, 'skip_drop': 0.0006369923547628038}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:32:16,244]\u001b[0m Trial 15 finished with value: 0.2579053960594852 and parameters: {'booster': 'dart', 'lambda': 0.0014085439304804368, 'alpha': 4.461594455703178e-07, 'subsample': 0.9071423601848648, 'colsample_bytree': 0.6711616607044066, 'max_depth': 3, 'min_child_weight': 2, 'eta': 0.02493783270667252, 'gamma': 2.249757184161395e-05, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.1988563089511034e-06, 'skip_drop': 0.5629629978981451}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:33:32,526]\u001b[0m Trial 16 finished with value: 0.25480429800618276 and parameters: {'booster': 'dart', 'lambda': 1.0788616764844177e-08, 'alpha': 6.332746688693727e-08, 'subsample': 0.48640160600584637, 'colsample_bytree': 0.9115268194070885, 'max_depth': 3, 'min_child_weight': 5, 'eta': 0.0008761942723752647, 'gamma': 1.562967849220129e-08, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 8.886364835810894e-07, 'skip_drop': 0.005760408842774168}. Best is trial 3 with value: 0.3851052381240679.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:35:04,561]\u001b[0m Trial 17 finished with value: 0.3863071494270162 and parameters: {'booster': 'dart', 'lambda': 1.5511524217608573e-05, 'alpha': 4.005469751281782e-06, 'subsample': 0.7150096433004351, 'colsample_bytree': 0.6507807637065889, 'max_depth': 5, 'min_child_weight': 10, 'eta': 0.08327940848126554, 'gamma': 0.0043220571632403195, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 2.3509148553591577e-07, 'skip_drop': 3.21791532571753e-05}. Best is trial 17 with value: 0.3863071494270162.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:36:37,353]\u001b[0m Trial 18 finished with value: 0.3806258259487331 and parameters: {'booster': 'dart', 'lambda': 0.0039375504575791245, 'alpha': 1.5668277358498046e-05, 'subsample': 0.7142029652048805, 'colsample_bytree': 0.6480986727715675, 'max_depth': 5, 'min_child_weight': 7, 'eta': 0.07825943993509953, 'gamma': 0.0011996879723786325, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 5.796013906543455e-06, 'skip_drop': 1.6904330628728547e-05}. Best is trial 17 with value: 0.3863071494270162.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:36:41,968]\u001b[0m Trial 19 finished with value: 0.16458078002238102 and parameters: {'booster': 'gblinear', 'lambda': 3.905494406158718e-05, 'alpha': 3.636719670557851e-06, 'subsample': 0.5360483756978439, 'colsample_bytree': 0.5976431262084775}. Best is trial 17 with value: 0.3863071494270162.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:38:37,455]\u001b[0m Trial 20 finished with value: 0.28267071530064913 and parameters: {'booster': 'dart', 'lambda': 0.3456255349308923, 'alpha': 0.00013538577753666764, 'subsample': 0.7017456697481439, 'colsample_bytree': 0.732908719102751, 'max_depth': 9, 'min_child_weight': 4, 'eta': 0.0017687168335954676, 'gamma': 0.01657798720267776, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.0038385742599393773, 'skip_drop': 2.154049308825167e-05}. Best is trial 17 with value: 0.3863071494270162.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:39:57,408]\u001b[0m Trial 21 finished with value: 0.4273398364926093 and parameters: {'booster': 'dart', 'lambda': 1.2196277238623086e-07, 'alpha': 8.872072866566911e-08, 'subsample': 0.852609953518945, 'colsample_bytree': 0.9158639052000911, 'max_depth': 3, 'min_child_weight': 10, 'eta': 0.16082146155580598, 'gamma': 0.08975552616350903, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 6.970741073918815e-08, 'skip_drop': 0.012644258522609655}. Best is trial 21 with value: 0.4273398364926093.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:41:38,642]\u001b[0m Trial 22 finished with value: 0.390401455329003 and parameters: {'booster': 'dart', 'lambda': 1.3512080215535363e-07, 'alpha': 1.2650696647909916e-06, 'subsample': 0.9100187054380208, 'colsample_bytree': 0.8635029593196836, 'max_depth': 5, 'min_child_weight': 9, 'eta': 0.049489866933293436, 'gamma': 0.033574481113432815, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.9720165098685466e-07, 'skip_drop': 0.007168659351360895}. Best is trial 21 with value: 0.4273398364926093.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:43:19,933]\u001b[0m Trial 23 finished with value: 0.40979438936267776 and parameters: {'booster': 'dart', 'lambda': 6.0873381233385e-06, 'alpha': 1.9823728675786626e-06, 'subsample': 0.9138943204745862, 'colsample_bytree': 0.8846528210460274, 'max_depth': 5, 'min_child_weight': 9, 'eta': 0.06288210455378732, 'gamma': 0.05721373509576878, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.8168508619037285e-07, 'skip_drop': 8.504052353651691e-05}. Best is trial 21 with value: 0.4273398364926093.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 11:45:01,454]\u001b[0m Trial 24 finished with value: 0.2864123592033175 and parameters: {'booster': 'dart', 'lambda': 1.417866484557574e-07, 'alpha': 9.325977064661674e-07, 'subsample': 0.905386472098862, 'colsample_bytree': 0.8487243195024987, 'max_depth': 5, 'min_child_weight': 9, 'eta': 5.499431401871806e-05, 'gamma': 0.07081438831203893, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.1461395479671124e-05, 'skip_drop': 0.00025028362819331054}. Best is trial 21 with value: 0.4273398364926093.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  25\n",
      "Best trial:\n",
      "  Value: 0.4273398364926093\n",
      "  Params: \n",
      "    booster: dart\n",
      "    lambda: 1.2196277238623086e-07\n",
      "    alpha: 8.872072866566911e-08\n",
      "    subsample: 0.852609953518945\n",
      "    colsample_bytree: 0.9158639052000911\n",
      "    max_depth: 3\n",
      "    min_child_weight: 10\n",
      "    eta: 0.16082146155580598\n",
      "    gamma: 0.08975552616350903\n",
      "    grow_policy: lossguide\n",
      "    sample_type: uniform\n",
      "    normalize_type: tree\n",
      "    rate_drop: 6.970741073918815e-08\n",
      "    skip_drop: 0.012644258522609655\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    y_min_u = utils.convert_df_to_bool(y_min_u)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u[utils.cols_with_positive_values(y_min_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    f1_score = sklearn.metrics.f1_score(valid_y, prediction, average='macro')\n",
    "    return f1_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_xgboost_sparse_classifier_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-27 14:02:03,077]\u001b[0m A new study created in memory with name: no-name-44ddcced-afd8-4709-bb73-883860534f3b\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:04:19,699]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 3.277034979078225e-06, 'degree': 2, 'gamma': 6.524079612801282e-05}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:05:15,602]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 1.5924358806614007e-07, 'degree': 4, 'gamma': 8.47979272128821e-08}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:07:20,592]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 3.39630111537881e-05, 'degree': 5, 'gamma': 2.242768932761124e-05}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:09:25,434]\u001b[0m Trial 3 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 1.8901539706474655e-07, 'degree': 2, 'gamma': 0.0002957867130862549}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:11:30,650]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.09044180173605737, 'degree': 3, 'gamma': 2.2528814292811956e-05}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:13:35,364]\u001b[0m Trial 5 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.0016570060565371243, 'degree': 2, 'gamma': 0.024241799041872843}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:15:40,615]\u001b[0m Trial 6 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 4.178366122053192e-05, 'degree': 1, 'gamma': 0.0010242669422473349}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:17:44,268]\u001b[0m Trial 7 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.007084124149082108, 'degree': 5, 'gamma': 0.0016809928155417787}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:19:51,809]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.004856376370878909, 'degree': 3, 'gamma': 0.347434564556455}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:21:59,732]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.2726085521499739, 'degree': 5, 'gamma': 4.4938791296426974e-07}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:23:00,118]\u001b[0m Trial 10 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 1.754602504969011e-08, 'degree': 1, 'gamma': 1.5907164993678727e-06}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:24:05,383]\u001b[0m Trial 11 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 1.6546659114533876e-06, 'degree': 4, 'gamma': 1.2552028539742786e-08}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:24:59,737]\u001b[0m Trial 12 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 2.049028295073698e-06, 'degree': 4, 'gamma': 7.634174258726416e-08}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:26:02,578]\u001b[0m Trial 13 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 2.6393494167722118e-08, 'degree': 2, 'gamma': 1.8421969992830032e-06}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:27:02,778]\u001b[0m Trial 14 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 1.3030132920927819e-06, 'degree': 4, 'gamma': 1.4381961791892648e-05}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:28:04,567]\u001b[0m Trial 15 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 2.2605211131797378e-07, 'degree': 3, 'gamma': 1.2089258226736424e-08}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:28:57,090]\u001b[0m Trial 16 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.00020840276198924566, 'degree': 2, 'gamma': 0.010649470551230977}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:29:48,297]\u001b[0m Trial 17 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 4.680730547551411e-06, 'degree': 4, 'gamma': 2.548404162100043e-05}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:30:43,139]\u001b[0m Trial 18 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 2.560224117074699e-07, 'degree': 3, 'gamma': 0.22978482928103286}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:32:49,662]\u001b[0m Trial 19 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.00045642289746038856, 'degree': 2, 'gamma': 0.012414240818878286}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:33:42,528]\u001b[0m Trial 20 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 6.165653626620847e-06, 'degree': 1, 'gamma': 0.00011499189485340715}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:34:57,170]\u001b[0m Trial 21 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 8.316768487327903e-06, 'degree': 3, 'gamma': 0.2728281164118254}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:37:02,178]\u001b[0m Trial 22 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.00036809100228520097, 'degree': 2, 'gamma': 0.05534884011674948}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:39:08,693]\u001b[0m Trial 23 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 2.053632113973055e-05, 'degree': 1, 'gamma': 0.005644744366379661}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:40:04,252]\u001b[0m Trial 24 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 9.73712369214855e-06, 'degree': 1, 'gamma': 0.00038417577240667616}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:42:28,011]\u001b[0m Trial 25 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.00020157911854251694, 'degree': 2, 'gamma': 0.08841039391018982}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:44:26,630]\u001b[0m Trial 26 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 3.1705294057382674e-05, 'degree': 1, 'gamma': 0.0019725920277392003}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:46:22,819]\u001b[0m Trial 27 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 1.5850553270700107e-05, 'degree': 1, 'gamma': 0.0002848176498473057}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:48:20,100]\u001b[0m Trial 28 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 9.623531045897124e-05, 'degree': 2, 'gamma': 5.538344517849415e-06}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:50:20,149]\u001b[0m Trial 29 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.02816487107507415, 'degree': 2, 'gamma': 0.07159514169161452}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:52:17,627]\u001b[0m Trial 30 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 7.54521229021408e-07, 'degree': 1, 'gamma': 0.0017974477663895254}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:54:21,453]\u001b[0m Trial 31 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 7.128802134661688e-05, 'degree': 1, 'gamma': 8.544436264891429e-05}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:56:24,924]\u001b[0m Trial 32 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.05915491939953897, 'degree': 2, 'gamma': 6.466986771198176e-06}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 14:58:19,414]\u001b[0m Trial 33 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.9191023414254542, 'degree': 2, 'gamma': 4.5909654355349215e-06}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:00:13,240]\u001b[0m Trial 34 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 5.512428672635446e-07, 'degree': 1, 'gamma': 7.670455467366178e-05}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:01:10,723]\u001b[0m Trial 35 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.0010871612033556395, 'degree': 3, 'gamma': 0.7826321694446968}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:02:03,063]\u001b[0m Trial 36 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 6.234212428146103e-06, 'degree': 3, 'gamma': 0.05371622323561046}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:02:54,497]\u001b[0m Trial 37 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 3.1175541070048002e-06, 'degree': 3, 'gamma': 0.2715576517813063}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:04:50,790]\u001b[0m Trial 38 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 2.664971346190163e-05, 'degree': 3, 'gamma': 0.006225814031261136}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:06:46,212]\u001b[0m Trial 39 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 7.559242260452816e-08, 'degree': 2, 'gamma': 0.03412126818646749}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:08:40,898]\u001b[0m Trial 40 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.00264774570707875, 'degree': 1, 'gamma': 0.000573567796819446}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:10:36,798]\u001b[0m Trial 41 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.0003408183316542759, 'degree': 2, 'gamma': 0.13018736678221746}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:12:31,950]\u001b[0m Trial 42 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 1.8202286374228967e-05, 'degree': 1, 'gamma': 0.0033699285872403765}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:15:11,725]\u001b[0m Trial 43 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 5.25638971046868e-05, 'degree': 1, 'gamma': 0.0004405639358986057}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:17:33,910]\u001b[0m Trial 44 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.00015289240239888915, 'degree': 1, 'gamma': 0.00024137686947182103}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:19:37,124]\u001b[0m Trial 45 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 1.314843559334411e-05, 'degree': 1, 'gamma': 4.135355010333615e-05}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:21:41,533]\u001b[0m Trial 46 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 9.302852037021929e-05, 'degree': 2, 'gamma': 5.082209993922075e-07}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:23:44,235]\u001b[0m Trial 47 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.0010763236104559405, 'degree': 2, 'gamma': 0.0012909622480418283}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:25:47,051]\u001b[0m Trial 48 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.03697492164140785, 'degree': 2, 'gamma': 1.3577234216528919e-05}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:27:49,828]\u001b[0m Trial 49 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.024692655149654616, 'degree': 2, 'gamma': 0.0002161022316120885}. Best is trial 0 with value: 0.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value: 0.0\n",
      "  Params: \n",
      "    kernel: rbf\n",
      "    C: 3.277034979078225e-06\n",
      "    degree: 2\n",
      "    gamma: 6.524079612801282e-05\n"
     ]
    }
   ],
   "source": [
    "# Same implementation as above, but for Support Vector Classifier.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    y_max_u_sparse_bool = utils.convert_df_to_bool(y_max_u_sparse_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse_bool[utils.cols_with_positive_values(y_max_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    # classification performance\n",
    "    f1_score = sklearn.metrics.f1_score(valid_y, prediction, average='macro')\n",
    "    return f1_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_support_vector_sparse_classifier_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-27 15:27:50,087]\u001b[0m A new study created in memory with name: no-name-c84e3d78-eaeb-4dbc-9aa0-f829d58022c1\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:29:23,815]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 1.3527650003577268e-08, 'degree': 1, 'gamma': 0.00010533356415821395}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:30:58,989]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 8.674591889399704e-07, 'degree': 1, 'gamma': 0.3302898173546117}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:34:36,331]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.010589620741060872, 'degree': 5, 'gamma': 0.0033085892905189316}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:38:08,027]\u001b[0m Trial 3 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 2.0771335159595045e-08, 'degree': 1, 'gamma': 7.63470920051692e-06}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:41:41,817]\u001b[0m Trial 4 finished with value: 0.36220024694066116 and parameters: {'kernel': 'rbf', 'C': 0.09624165012082381, 'degree': 5, 'gamma': 0.684079135847173}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:43:19,212]\u001b[0m Trial 5 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.00014748055401099156, 'degree': 2, 'gamma': 1.2076095554102278e-05}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:44:56,377]\u001b[0m Trial 6 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.0009171356775382736, 'degree': 3, 'gamma': 0.0005044699378466327}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:46:33,449]\u001b[0m Trial 7 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.0016127733670599027, 'degree': 2, 'gamma': 0.0037862379698819183}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:50:34,463]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.32256319896778657, 'degree': 4, 'gamma': 0.009800029578148966}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:52:09,157]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 1.7330882994385734e-08, 'degree': 3, 'gamma': 7.537965374945187e-05}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:55:50,065]\u001b[0m Trial 10 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.20331649052739878, 'degree': 5, 'gamma': 2.106254742033447e-08}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 15:59:24,714]\u001b[0m Trial 11 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 2.8028874883223873e-06, 'degree': 4, 'gamma': 0.8507865173059702}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:03:01,674]\u001b[0m Trial 12 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 9.46835802394173e-06, 'degree': 2, 'gamma': 1.3699191729985885e-07}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:04:40,990]\u001b[0m Trial 13 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.03051671409371457, 'degree': 4, 'gamma': 0.03669990845016871}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:08:18,646]\u001b[0m Trial 14 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 2.554843964249286e-07, 'degree': 1, 'gamma': 4.1902372506013845e-07}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:09:58,863]\u001b[0m Trial 15 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 2.459688022122672e-05, 'degree': 5, 'gamma': 0.06313663194973795}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:11:36,478]\u001b[0m Trial 16 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.9067543001408637, 'degree': 3, 'gamma': 3.2455899528259595e-06}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:15:07,904]\u001b[0m Trial 17 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.0002166192178373246, 'degree': 2, 'gamma': 0.0003397634718394008}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:16:41,892]\u001b[0m Trial 18 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 9.488189103314284e-08, 'degree': 4, 'gamma': 6.652207128876144e-05}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:20:15,838]\u001b[0m Trial 19 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.015781402483652285, 'degree': 3, 'gamma': 6.797602963335108e-07}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:21:49,530]\u001b[0m Trial 20 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 2.1989812141643848e-05, 'degree': 1, 'gamma': 0.18039159928323506}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:23:23,861]\u001b[0m Trial 21 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 1.265545693948301e-07, 'degree': 4, 'gamma': 3.952842497436036e-05}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:26:57,455]\u001b[0m Trial 22 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.02592632688583419, 'degree': 3, 'gamma': 1.0815773711450319e-06}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:28:30,016]\u001b[0m Trial 23 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 2.206485178153003e-05, 'degree': 1, 'gamma': 0.14770143256039642}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:30:04,408]\u001b[0m Trial 24 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 1.3814753354726627e-07, 'degree': 5, 'gamma': 0.0003335118954997753}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:33:50,292]\u001b[0m Trial 25 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.0961998609185671, 'degree': 2, 'gamma': 4.960463272128595e-08}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:35:23,782]\u001b[0m Trial 26 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.0022124969375115085, 'degree': 1, 'gamma': 0.017523797911455377}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:36:58,840]\u001b[0m Trial 27 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 1.3519938264778951e-06, 'degree': 5, 'gamma': 0.0005823790130326438}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:40:39,841]\u001b[0m Trial 28 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.22011140191581569, 'degree': 2, 'gamma': 2.8411630200075695e-08}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:42:13,856]\u001b[0m Trial 29 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.002970813602043549, 'degree': 1, 'gamma': 0.9447140887491247}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:43:48,512]\u001b[0m Trial 30 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 7.959776525361309e-07, 'degree': 5, 'gamma': 0.001238755845966332}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:47:19,244]\u001b[0m Trial 31 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 1.765773242210543e-06, 'degree': 2, 'gamma': 3.170182105352996e-05}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:51:04,924]\u001b[0m Trial 32 finished with value: 0.008504730661341702 and parameters: {'kernel': 'rbf', 'C': 0.0054525957345163715, 'degree': 1, 'gamma': 0.848913917489286}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:54:51,358]\u001b[0m Trial 33 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.003175960659035658, 'degree': 5, 'gamma': 0.8619307451506816}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 16:58:41,151]\u001b[0m Trial 34 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.010835209308646173, 'degree': 1, 'gamma': 0.20279926863680184}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:02:34,608]\u001b[0m Trial 35 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.0005687252141739353, 'degree': 1, 'gamma': 0.09169012096172006}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:04:10,181]\u001b[0m Trial 36 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.07320713101911475, 'degree': 1, 'gamma': 0.3240283662719657}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:07:42,765]\u001b[0m Trial 37 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 5.6375445676715566e-08, 'degree': 4, 'gamma': 1.2537980653934168e-05}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:09:21,849]\u001b[0m Trial 38 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 3.1045378059603147e-08, 'degree': 4, 'gamma': 0.005624345416951982}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:12:54,472]\u001b[0m Trial 39 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.0554679878618906, 'degree': 3, 'gamma': 6.322229020374115e-06}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:16:28,181]\u001b[0m Trial 40 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.8616972116356479, 'degree': 3, 'gamma': 1.0937879978629512e-06}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:18:00,913]\u001b[0m Trial 41 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 1.1742663947912599e-08, 'degree': 1, 'gamma': 0.00018568998077795508}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:19:41,067]\u001b[0m Trial 42 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 7.960876146244601e-05, 'degree': 5, 'gamma': 0.023368073974843327}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:23:17,054]\u001b[0m Trial 43 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.006065694530597169, 'degree': 2, 'gamma': 0.012404947979593298}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:26:48,841]\u001b[0m Trial 44 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 2.68397863606301e-07, 'degree': 2, 'gamma': 0.0018963072258310378}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:30:42,424]\u001b[0m Trial 45 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.09500382560461636, 'degree': 1, 'gamma': 0.05479517072466539}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:34:20,446]\u001b[0m Trial 46 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.00030791929584509715, 'degree': 2, 'gamma': 9.700887828784382e-08}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:36:04,669]\u001b[0m Trial 47 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 0.0015053908280105892, 'degree': 5, 'gamma': 0.0017638174866296185}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:37:41,268]\u001b[0m Trial 48 finished with value: 0.0 and parameters: {'kernel': 'poly', 'C': 5.6710911454384214e-05, 'degree': 1, 'gamma': 0.0007187719418684385}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n",
      "\u001b[32m[I 2022-09-27 17:41:12,539]\u001b[0m Trial 49 finished with value: 0.0 and parameters: {'kernel': 'rbf', 'C': 0.22293403798231998, 'degree': 2, 'gamma': 0.00010937622920557109}. Best is trial 4 with value: 0.36220024694066116.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value: 0.36220024694066116\n",
      "  Params: \n",
      "    kernel: rbf\n",
      "    C: 0.09624165012082381\n",
      "    degree: 5\n",
      "    gamma: 0.684079135847173\n"
     ]
    }
   ],
   "source": [
    "# Same implementation as above, but for Support Vector Classifier.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    y_min_u_sparse_bool = utils.convert_df_to_bool(y_min_u_sparse_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse_bool[utils.cols_with_positive_values(y_min_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    # classification performance\n",
    "    f1_score = sklearn.metrics.f1_score(valid_y, prediction, average='macro')\n",
    "    return f1_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_support_vector_sparse_classifier_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import beepy\n",
    "beepy.beep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ext_grid</th>\n",
       "      <th>bus_1</th>\n",
       "      <th>bus_2</th>\n",
       "      <th>bus_3</th>\n",
       "      <th>bus_4</th>\n",
       "      <th>bus_5</th>\n",
       "      <th>bus_6</th>\n",
       "      <th>bus_7</th>\n",
       "      <th>bus_8</th>\n",
       "      <th>bus_9</th>\n",
       "      <th>...</th>\n",
       "      <th>bus_31</th>\n",
       "      <th>bus_17</th>\n",
       "      <th>bus_21</th>\n",
       "      <th>bus_24</th>\n",
       "      <th>bus_18</th>\n",
       "      <th>bus_23</th>\n",
       "      <th>bus_27</th>\n",
       "      <th>bus_32</th>\n",
       "      <th>bus_33</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 00:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 00:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45211</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-15 22:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45212</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-15 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45213</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-15 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45214</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-15 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45215</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-15 23:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45216 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ext_grid  bus_1  bus_2  bus_3  bus_4  bus_5  bus_6  bus_7  bus_8  \\\n",
       "0             0      0      0      0      0      0      0      0      0   \n",
       "1             0      0      0      0      0      0      0      0      0   \n",
       "2             0      0      0      0      0      0      0      0      0   \n",
       "3             0      0      0      0      0      0      0      0      0   \n",
       "4             0      0      0      0      0      0      0      0      0   \n",
       "...         ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "45211         0      0      0      0      0      0      0      0      0   \n",
       "45212         0      0      0      0      0      0      0      0      0   \n",
       "45213         0      0      0      0      0      0      0      0      0   \n",
       "45214         0      0      0      0      0      0      0      0      0   \n",
       "45215         0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "       bus_9  ...  bus_31  bus_17  bus_21  bus_24  bus_18  bus_23  bus_27  \\\n",
       "0          0  ...       0       0       0       0       0       0       0   \n",
       "1          0  ...       0       0       0       0       0       0       0   \n",
       "2          0  ...       0       0       0       0       0       0       0   \n",
       "3          0  ...       0       0       0       0       0       0       0   \n",
       "4          0  ...       0       0       0       0       0       0       0   \n",
       "...      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "45211      0  ...       0       0       0       0       0       0       0   \n",
       "45212      0  ...       0       0       0       0       0       0       0   \n",
       "45213      0  ...       0       0       0       0       0       0       0   \n",
       "45214      0  ...       0       0       0       0       0       0       0   \n",
       "45215      0  ...       0       0       0       0       0       0       0   \n",
       "\n",
       "       bus_32  bus_33           timestamps  \n",
       "0           0       0  2020-01-01 00:00:00  \n",
       "1           0       0  2020-01-01 00:15:00  \n",
       "2           0       0  2020-01-01 00:30:00  \n",
       "3           0       0  2020-01-01 00:45:00  \n",
       "4           0       0  2020-01-01 01:00:00  \n",
       "...       ...     ...                  ...  \n",
       "45211       0       0  2021-04-15 22:45:00  \n",
       "45212       0       0  2021-04-15 23:00:00  \n",
       "45213       0       0  2021-04-15 23:15:00  \n",
       "45214       0       0  2021-04-15 23:30:00  \n",
       "45215       0       0  2021-04-15 23:45:00  \n",
       "\n",
       "[45216 rows x 35 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
