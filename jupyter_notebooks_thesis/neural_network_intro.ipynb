{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Intro\n",
    "\n",
    "**Summary of Article**\n",
    "- Theoretical Introduction to Neural Networks.\n",
    "- FeedForward Neural Network Implementation for Regression.\n",
    "- FeedForward Neural Network Implementation for Classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Intro\n",
    "### Theoretical Introduction to Neural Networks\n",
    "Neural Networks (NN)  are a class of ML models that are based on the connections of layers of artificial neurons. The connections between the layers are made up of weights and biases, that are updated during the training process. Activation functions are used to determine the output of a neuron. Different activation functions are what allow the NN to learn and generalize expressive results. The following illustration represents the architecture of a neural network. (Only on thesis)\n",
    "** Figure here **.\n",
    "### Training Process \n",
    "The training process of a NN is the process of updating the weights and biases of the neural network to make it better at predicting the output of the input. Backpropagation is a method of updating the weights and biases, where the derivative of the loss fuction with respect to the weights and biases, is used to update the respective values. The training process takes the following steps:\n",
    "\n",
    "- Take a batch of training data.\n",
    "- Forward propagate the batch of data through the neural network.\n",
    "- Compute the loss function for the batch of data.\n",
    "- Backpropagate the loss function to get the gradients.\n",
    "- Update the weights and biases using the gradients.\n",
    "- Repeat the above steps until the loss function is less than a determined threshold.\n",
    "\n",
    "The most common activation function are: \n",
    "- Sigmoid function: $$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "- Tanh: $$ g(z)= \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$$\n",
    "- ReLu: $$ g(z) = \\max(0,z)$$\n",
    "\n",
    "The most common Loss functions for Regression is:\n",
    "- RMSE: $$L(z,y) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - z_i)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network with PyTorch \n",
    "Now Pytorch will be used to train a neural network. The data will be the sparse dataset normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sys; sys.path.append('..')\n",
    "from thesis_package import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create the data set class. Our data set class will extend `torch.utils.data.Dataset`. Afterwards, we will create the data loader class. The data loader class will extend `torch.utils.data.DataLoader`. The data loader is used to separate the data set in batches, shuffle the data set and create an iterator.\n",
    "\n",
    "First the dataset is loaded and prepared in tha same fashion as for the other ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns='timestamps')\n",
    "y_max_u = y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)]\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "X_max_u_bool_train, X_max_u_bool_test, y_max_u_bool_train, y_max_u_bool_test, scaler = utils.split_and_suffle(exogenous_data, y_max_u_bool, scaling=True)\n",
    "data = {'X_train':X_max_u_bool_train.astype(float),\n",
    "        'X_test': X_max_u_bool_test.astype(float),\n",
    "        'y_train':y_max_u_bool_train.astype(float),\n",
    "        'y_test': y_max_u_bool_test.astype(float)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the dataset class is declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9355, 0.4582, 0.4807, 0.4887, 1.0000, 0.8571, 0.4955, 0.4880, 0.9356,\n",
       "         0.7071, 0.4650]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ThesisDataset(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        train_X, train_y = data['X_train'], data['y_train']\n",
    "        test_X, test_y = data['X_test'], data['y_test']\n",
    "        self.X = torch.from_numpy(train_X.values).float()\n",
    "        self.y = torch.from_numpy(train_y.values).float()\n",
    "        self.X_test = torch.from_numpy(test_X.values).float()\n",
    "        self.y_test = torch.from_numpy(test_y.values).float()\n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        return self.X[index], self.y[index]\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "dataset = ThesisDataset(data)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our data loader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 36172, total interations: 1131\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "from math import ceil\n",
    "total_samples = len(dataset)\n",
    "n_iterations = ceil(total_samples / 32)\n",
    "print('Total samples: {}, total interations: {}'.format(total_samples, n_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our training loop will look like the follwing:\n",
    "\n",
    "```python\t\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(dataloader):\n",
    "        # Zero grads, Forward, Backwards and Update\n",
    "    # Compute and print loss\n",
    "    # Evaluate model on validation set\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is necessary to decide on hyperparameters for the neural network. These hyper parameters will later be tunned using optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'input_size': dataset.X.shape[1],\n",
    "    'hidden_size': 32,\n",
    "    'output_size': dataset.y.shape[1],\n",
    "    'n_layers': 4,\n",
    "    'dropout': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(dataset.X.shape[1])\n",
    "print(dataset.y.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to configure the device once it is faster to train the models oh the GPU, if one is available. Later it is necessary to push th tensors into device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device # In my case I don't have a GPU, so I use the CPU ... Sad, so if anyone wants to give me a GPU, hit me up on LinkedIn :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class FeedforwardNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self, model_params, **kwargs):\n",
    "        \"\"\"\n",
    "        n_classes (int)\n",
    "        n_features (int)\n",
    "        hidden_size (int)   \n",
    "        layers (int)\n",
    "        activation_type (str)\n",
    "        dropout (float): dropout probability\n",
    "        As in logistic regression, the __init__ here defines a bunch of\n",
    "        attributes that each FeedforwardNetwork instance has. Note that nn\n",
    "        includes modules for several activation functions and dropout as well.\n",
    "        \"\"\"\n",
    "        super(FeedforwardNetwork, self).__init__()\n",
    "        output_size, input_size, hidden_size, n_leayers, dropout = model_params['output_size'], model_params['input_size'], model_params['hidden_size'], model_params['n_layers'], model_params['dropout']\n",
    "        layers = []\n",
    "        for i in range(n_leayers):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_size))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        self.feedforward_nn = nn.Sequential(*layers)\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        x (batch_size x n_features): a batch of training examples\n",
    "        This method needs to perform all the computation needed to compute\n",
    "        the output logits from x. This will include using various hidden\n",
    "        layers, pointwise nonlinear functions, and dropout.\n",
    "        \"\"\"\n",
    "        return self.feedforward_nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedforwardNetwork(\n",
       "  (feedforward_nn): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=32, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (7): Tanh()\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (10): Tanh()\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "    (12): Linear(in_features=32, out_features=34, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FeedforwardNetwork(model_params)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the `train_batch()` function. This function will be used to train the model on a batch of data. The train_batch has he following steps:\n",
    "- Setting the stored gradient values to zero.\n",
    "- Computes the gradient of the given tensor with respect to the weights and biases.\n",
    "-  Computes the gradient of the given tensor w.r.t. graph leaves.\n",
    "- Updates weights and biases with the optimizer (SGD of ADAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, y, model, optimizer, criterion, **kwargs):\n",
    "    \"\"\"\n",
    "    X (n_examples x n_features)\n",
    "    y (n_examples): gold labels\n",
    "    model: a PyTorch defined model\n",
    "    optimizer: optimizer used in gradient step\n",
    "    criterion: loss function\n",
    "    To train a batch, the model needs to predict outputs for X, compute the\n",
    "    loss between these predictions and the \"gold\" labels y using the criterion,\n",
    "    and compute the gradient of the loss with respect to the model parameters.\n",
    "    Check out https://pytorch.org/docs/stable/optim.html for examples of how\n",
    "    to use an optimizer object to update the parameters.\n",
    "    This function should return the loss (tip: call loss.item()) to get the\n",
    "    loss as a numerical value that is not part of the computation graph.\n",
    "    \"\"\"\n",
    "    # Forward\n",
    "    #print('X shape: {}'.format(X.shape))\n",
    "    output = model(X)  # Computes the gradient of the given tensor w.r.t. the weights/bias\n",
    "    #print('output shape: {}, y_shape: {}'.format(output.shape, y.shape))\n",
    "    loss = criterion(output, y) # cross entropy in this case\n",
    "    # Backwards\n",
    "    optimizer.zero_grad()  # Setting our stored gradients equal to zero\n",
    "    loss.backward() # Computes the gradient of the given tensor w.r.t. graph leaves \n",
    "    optimizer.step() # Updates weights and biases with the optimizer (SGD of ADAM)\n",
    "    return loss.item()\n",
    "    \n",
    "def predict(model, X):\n",
    "    \"\"\"X (n_examples x n_features)\"\"\"\n",
    "    scores = model(X)  # (n_examples x n_classes)\n",
    "    return scores\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    \"\"\"\n",
    "    X (n_examples x n_features)\n",
    "    y (n_examples): gold labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_hat = predict(model, X)\n",
    "    y_hat = torch.heaviside(y_hat, torch.tensor(0.0))\n",
    "    n_correct = (y == y_hat).sum().item()\n",
    "    n_possible = float(y.shape[0])\n",
    "    model.train()\n",
    "    return n_correct / n_possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot(epochs, plottable, ylabel='', title=''):\n",
    "    plt.clf()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.plot(epochs, plottable)\n",
    "    plt.grid()\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "Training loss: 1.9284\n",
      "Training epoch 2\n",
      "Training loss: 1.8939\n",
      "Training epoch 3\n",
      "Training loss: 1.8542\n",
      "Training epoch 4\n",
      "Training loss: 1.8101\n",
      "Training epoch 5\n",
      "Training loss: 1.7636\n",
      "Final Test acc: 25.3266\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data\n",
    "test_X, test_y = dataset.X_test, dataset.y_test\n",
    "# initialize the model    \n",
    "model = FeedforwardNetwork(model_params)\n",
    "# Hyperparameters\n",
    "hyper_params = {'optimizer': 'adam', 'lr': 0.001, 'epochs': 5, 'batch_size': 32}\n",
    "# get an optimizer\n",
    "optims = {\"adam\": torch.optim.Adam, \"sgd\": torch.optim.SGD}\n",
    "optim_cls = optims[\"sgd\"]\n",
    "optimizer = optim_cls(\n",
    "    model.parameters(),\n",
    "    lr=hyper_params[\"lr\"],\n",
    "    weight_decay=0.0)\n",
    "\n",
    "# get a loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training loop\n",
    "epochs = torch.arange(1, hyper_params['epochs'] + 1)\n",
    "train_mean_losses = []\n",
    "valid_accs = []\n",
    "train_losses = []\n",
    "for ii in epochs:\n",
    "    print('Training epoch {}'.format(ii))\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        # X = batch_size x 11, y = batch_size x 34\n",
    "        # Print X_batch.shape, y_batch.shape\n",
    "        # print('X_batch shape: {}, y_batch shape: {}'.format(X_batch.shape, y_batch.shape))\n",
    "        loss = train_batch(\n",
    "            X_batch, y_batch, model, optimizer, criterion)\n",
    "        train_losses.append(loss)\n",
    "    mean_loss = torch.tensor(train_losses).mean().item()\n",
    "    print('Training loss: %.4f' % (mean_loss))\n",
    "\n",
    "    train_mean_losses.append(mean_loss)\n",
    "final_acc = evaluate(model, test_X, test_y)\n",
    "print('Final Test acc: %.4f' % (evaluate(model, test_X, test_y)))\n",
    "# plot\n",
    "plot(epochs, train_mean_losses, ylabel='Loss', title='Loss(Epoch)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(model, test_X)\n",
    "prediction = torch.heaviside(prediction, torch.tensor(0.0))\n",
    "prediction = pd.DataFrame(prediction.detach().numpy(), columns=y_max_u_bool.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ext_grid</th>\n",
       "      <th>bus_1</th>\n",
       "      <th>bus_2</th>\n",
       "      <th>bus_3</th>\n",
       "      <th>bus_4</th>\n",
       "      <th>bus_5</th>\n",
       "      <th>bus_6</th>\n",
       "      <th>bus_7</th>\n",
       "      <th>bus_8</th>\n",
       "      <th>bus_9</th>\n",
       "      <th>...</th>\n",
       "      <th>bus_30</th>\n",
       "      <th>bus_31</th>\n",
       "      <th>bus_17</th>\n",
       "      <th>bus_21</th>\n",
       "      <th>bus_24</th>\n",
       "      <th>bus_18</th>\n",
       "      <th>bus_23</th>\n",
       "      <th>bus_27</th>\n",
       "      <th>bus_32</th>\n",
       "      <th>bus_33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9039</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9040</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9041</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9042</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9043</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9044 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ext_grid  bus_1  bus_2  bus_3  bus_4  bus_5  bus_6  bus_7  bus_8  bus_9  \\\n",
       "0          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "1          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "2          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "3          0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0   \n",
       "4          0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0   \n",
       "...        ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "9039       0.0    1.0    0.0    1.0    0.0    1.0    0.0    0.0    1.0    0.0   \n",
       "9040       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0   \n",
       "9041       0.0    0.0    0.0    1.0    0.0    1.0    0.0    1.0    1.0    1.0   \n",
       "9042       0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0   \n",
       "9043       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "\n",
       "      ...  bus_30  bus_31  bus_17  bus_21  bus_24  bus_18  bus_23  bus_27  \\\n",
       "0     ...     0.0     0.0     0.0     1.0     0.0     1.0     0.0     0.0   \n",
       "1     ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "2     ...     1.0     0.0     0.0     0.0     0.0     1.0     1.0     0.0   \n",
       "3     ...     1.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "4     ...     0.0     0.0     0.0     1.0     1.0     1.0     1.0     0.0   \n",
       "...   ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9039  ...     0.0     0.0     0.0     0.0     0.0     1.0     1.0     0.0   \n",
       "9040  ...     0.0     0.0     0.0     1.0     0.0     1.0     1.0     0.0   \n",
       "9041  ...     1.0     0.0     1.0     1.0     1.0     1.0     1.0     0.0   \n",
       "9042  ...     1.0     1.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "9043  ...     0.0     0.0     0.0     1.0     0.0     1.0     1.0     0.0   \n",
       "\n",
       "      bus_32  bus_33  \n",
       "0        0.0     1.0  \n",
       "1        0.0     0.0  \n",
       "2        0.0     0.0  \n",
       "3        0.0     0.0  \n",
       "4        0.0     1.0  \n",
       "...      ...     ...  \n",
       "9039     0.0     1.0  \n",
       "9040     0.0     1.0  \n",
       "9041     0.0     1.0  \n",
       "9042     0.0     0.0  \n",
       "9043     0.0     1.0  \n",
       "\n",
       "[9044 rows x 34 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08962851231835677"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.f1_score(data['y_test'], prediction, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNetwork(\n",
      "  (feedforward_nn): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=32, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (4): Tanh()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Dropout(p=0.5, inplace=False)\n",
      "    (9): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (10): Tanh()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=34, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unscale the X_max_u_bool_train data for plotting.\n",
    "X_max_u_bool_train_unscaled = scaler['X_train'].inverse_transform(X_max_u_bool_train)\n",
    "# Each data set has its own scaler in order to be possible to inverse scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_max_u_bool_train_unscaled = pd.DataFrame(X_max_u_bool_train_unscaled, columns=exogenous_data.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
