{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunning HyperParameters\n",
    "\n",
    "**Summary of the Article**\n",
    "- High level intro of Optuna and Hyperparams.\n",
    "- Hyperparameter tuning with Optuna.\n",
    "## Intro to HyperParameters\n",
    "\n",
    "HyperParameters are the parameters of a model are values that control the learning process of a given model. They determine how the model parameters will end up being optimized in the training phase. Part of the job of a machine learning engineer is to find the best values for the hyperparameters of a model, so there are several techinques to set up such values:\n",
    "- Not tunning and use common values.\n",
    "- Random Search.\n",
    "- Grid Search.\n",
    "- Using tools like Optuna.\n",
    "\n",
    "## Intro to Optuna \n",
    "Optuna is a hyperparameter optimization framework applicable to machine learning frameworks and black-box optimization solvers. It is a Python library that provides a simple and easy way to optimize hyperparameters of machine learning models. \n",
    "\n",
    "Optuna optimization is divided in two phases, the Sampling Strategy and the Prunning Strategy:\n",
    " - *Sampling Strategy* - Where to look? Focus using Baeasyian filtering to look the most promising hyperparameters.\n",
    " - *Prunning Strategy* - If a trial is not promising, it temrinates early to save time for better trials.\n",
    "\n",
    " Note: An Optuna trial is a single run of a model with a set of hyperparameters.\n",
    "\n",
    " The structure of an Optuna search is as follows:\n",
    " ```python\t\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    ## your code here\n",
    "    return evaluation_score\n",
    "study = optuna.create_study()\n",
    "num_trials = 100\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "```\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-07 15:25:45,501]\u001b[0m A new study created in memory with name: no-name-e72e6130-23ef-48c3-a007-247cd1c54fe3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'thesis_package.aimodels.Context'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-07 15:26:32,622]\u001b[0m Trial 0 finished with value: 0.49984508752822876 and parameters: {'booster': 'gbtree', 'lambda': 1.8406093472937077e-08, 'alpha': 0.07293031730603843, 'subsample': 0.4131826128046079, 'colsample_bytree': 0.3088078275355107, 'max_depth': 9, 'min_child_weight': 9, 'eta': 3.099182894861593e-06, 'gamma': 0.3783100528550611, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.49984508752822876.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'thesis_package.aimodels.Context'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-07 15:27:24,347]\u001b[0m Trial 1 finished with value: 0.43325719237327576 and parameters: {'booster': 'gbtree', 'lambda': 1.9818016585265665e-07, 'alpha': 2.4424849536649444e-06, 'subsample': 0.3742400184110838, 'colsample_bytree': 0.22126096591807515, 'max_depth': 7, 'min_child_weight': 7, 'eta': 0.00143186761836147, 'gamma': 3.8016446963068855e-05, 'grow_policy': 'lossguide'}. Best is trial 1 with value: 0.43325719237327576.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  2\n",
      "Best trial:\n",
      "  Value: 0.43325719237327576\n",
      "  Params: \n",
      "    booster: gbtree\n",
      "    lambda: 1.9818016585265665e-07\n",
      "    alpha: 2.4424849536649444e-06\n",
      "    subsample: 0.3742400184110838\n",
      "    colsample_bytree: 0.22126096591807515\n",
      "    max_depth: 7\n",
      "    min_child_weight: 7\n",
      "    eta: 0.00143186761836147\n",
      "    gamma: 3.8016446963068855e-05\n",
      "    grow_policy: lossguide\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\"\"\"\n",
    "Note: The code bellow is adapted from the optuna repo examples! \n",
    "\n",
    "Optuna example that optimizes a multioutput regressor configuration for\n",
    "electrical grid constraint detection forecasting using XGBoost.\n",
    "In this example, we optimize the validation rmse. We optimize both the choice of booster model and its\n",
    "hyperparameters.\n",
    "\"\"\" \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.datasets\n",
    "import pandas as pd \n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from thesis_package import aimodels as my_ai, utils\n",
    "\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_line_percent_max_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y = utils.split_and_suffle(exogenous_data, y_min_u)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    # evaluate the regression performance\n",
    "    rmse = sklearn.metrics.mean_squared_error(valid_y, prediction, squared=False)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=2, timeout=600)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/optuna_xgboost_regression_results.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
