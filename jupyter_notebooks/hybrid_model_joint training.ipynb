{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Training Hybrid Model\n",
    "The second approach is to have the regression model to incorporate in its training and testing the result of the classification model. The objective is to incorporate the ground truth of of the classification data set into the training data of the regression data set and demonstrated, that is, allow the model to learn to predict the amplitude of a constraint, given that that constraint exists. \n",
    "\n",
    "During testing, the truthful constraint violation Boolean feature is substituted by the prediction of the classification model, and the \n",
    "model will predict the amplitude of the constraint violation, given the prediction of its occurrence. \n",
    "\n",
    "## Model selection\n",
    "Considering the datasets benchmark, the best regression model for max_u and min_u are: \n",
    "- max_u: gb balanced\n",
    "- min_u: gb sparse\n",
    "\n",
    "The best classification model for max_u and min_u are:\n",
    "- max_u: xgb balanced\n",
    "- min_u: gb sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys;sys.path.append('..');from thesis_package import aimodels as my_ai, utils, metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data balanced\n",
    "y_max_u_balanced_class = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_bool_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced_class = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_bool_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "y_max_u_balanced_class = y_max_u_balanced_class[utils.cols_with_positive_values(y_max_u_balanced_class)]\n",
    "y_min_u_balanced_class = y_min_u_balanced_class[utils.cols_with_positive_values(y_min_u_balanced_class)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced_class, test_size=0.2, scaling=True)\n",
    "data_max_u_bool_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced_class, test_size=0.2, scaling=True)\n",
    "data_min_u_bool_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data balanced\n",
    "y_max_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced, test_size=0.2, scaling=True)\n",
    "data_max_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced, test_size=0.2, scaling=True)\n",
    "data_min_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data sparse\n",
    "y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_max_u_bool = y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)]\n",
    "y_min_u_bool = y_min_u_bool[utils.cols_with_positive_values(y_min_u_bool)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_bool, test_size=0.2, scaling=True)\n",
    "data_max_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_bool, test_size=0.2, scaling=True)\n",
    "data_min_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data sparse\n",
    "y_max_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse, test_size=0.2, scaling=True)\n",
    "data_max_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse, test_size=0.2, scaling=True)\n",
    "data_min_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_u_threshold = utils.compute_threshold(y_max_u_sparse)\n",
    "min_u_threshold = utils.compute_threshold(y_min_u_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepared Hybrid data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max u\n",
    "data_max_u_hybrid = {}\n",
    "data_max_u_hybrid['X_train'] = pd.concat([data_max_u_balanced['X_train'], data_max_u_bool_balanced['y_train']], axis=1)\n",
    "data_max_u_hybrid['y_train'] = deepcopy(data_max_u_balanced['y_train'])\n",
    "# min u \n",
    "data_min_u_hybrid = {}\n",
    "data_min_u_hybrid['X_train'] = pd.concat([data_min_u_sparse['X_train'], data_min_u_bool['y_train']], axis=1)\n",
    "data_min_u_hybrid['y_train'] = deepcopy(data_min_u_sparse['y_train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<thesis_package.aimodels.XGBoostClassifierStrategy object at 0x000001C2B2D163A0>\n",
      "<thesis_package.aimodels.GradientBoostClassifierStrategy object at 0x000001C2B51B50D0>\n"
     ]
    }
   ],
   "source": [
    "# max u\n",
    "classifier_max_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_classifier_balanced')\n",
    "max_u_class_xgb = deepcopy(classifier_max_u_balanced.strategies[1])\n",
    "print(classifier_max_u_balanced.strategies[1])\n",
    "class_result_max_u = max_u_class_xgb.predict(data_max_u_bool)\n",
    "data_max_u_hybrid['X_test'] = pd.concat([data_max_u_sparse['X_test'], class_result_max_u], axis=1)\n",
    "data_max_u_hybrid['y_test'] = deepcopy(data_max_u_sparse['y_test'])\n",
    "# min u \n",
    "classifier_min_u_sparse = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_classifier')\n",
    "min_u_class_gb = deepcopy(classifier_min_u_sparse.strategies[1])\n",
    "print(classifier_min_u_sparse.strategies[0])\n",
    "class_result_min_u = min_u_class_gb.predict(data_min_u_bool)\n",
    "data_min_u_hybrid['X_test'] = pd.concat([data_min_u_sparse['X_test'], class_result_min_u], axis=1)\n",
    "data_min_u_hybrid['y_test'] = deepcopy(data_min_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_max_u_hybrid X_train shape:  (5561, 22)\n",
      "data_max_u_hybrid y_train shape:  (5561, 34)\n",
      "data_max_u_hybrid X_test shape:  (9044, 22)\n",
      "data_max_u_hybrid y_test shape:  (9044, 34)\n",
      "\n",
      "\n",
      "data_min_u_hybrid X_train shape:  (36172, 21)\n",
      "data_min_u_hybrid y_train shape:  (36172, 34)\n",
      "data_min_u_hybrid X_test shape:  (9044, 21)\n",
      "data_min_u_hybrid y_test shape:  (9044, 34)\n"
     ]
    }
   ],
   "source": [
    "print('data_max_u_hybrid X_train shape: ', data_max_u_hybrid['X_train'].shape)\n",
    "print('data_max_u_hybrid y_train shape: ', data_max_u_hybrid['y_train'].shape)\n",
    "print('data_max_u_hybrid X_test shape: ', data_max_u_hybrid['X_test'].shape)\n",
    "print('data_max_u_hybrid y_test shape: ', data_max_u_hybrid['y_test'].shape)\n",
    "print('\\n')\n",
    "print('data_min_u_hybrid X_train shape: ', data_min_u_hybrid['X_train'].shape)\n",
    "print('data_min_u_hybrid y_train shape: ', data_min_u_hybrid['y_train'].shape)\n",
    "print('data_min_u_hybrid X_test shape: ', data_min_u_hybrid['X_test'].shape)\n",
    "print('data_min_u_hybrid y_test shape: ', data_min_u_hybrid['y_test'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training max u hybrid model... wait for it...\n"
     ]
    }
   ],
   "source": [
    "balanced_hyper_params = {}\n",
    "sparse_hyper_params = {}\n",
    "for file in os.listdir('hyper_params_results_mcc'):\n",
    "    if file.endswith('.csv') and 'regression_sparse' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'regression_balanced' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        balanced_hyper_params[file] = df\n",
    "    else:\n",
    "        pass\n",
    "import ast\n",
    "def get_hyper_params_from_df(df):\n",
    "    output = {}\n",
    "    for row in df.iterrows():\n",
    "        if row[1]['params'] != 'value':\n",
    "            try:\n",
    "                output[row[1]['params']] = ast.literal_eval(row[1]['value'])\n",
    "            except :\n",
    "                output[row[1]['params']] = row[1]['value']\n",
    "    return output\n",
    "\n",
    "\n",
    "# max_u\n",
    "if 'hybrid_regressor_max_u.pickle' not in os.listdir('pickles\\hybrid_models_benchmark'):\n",
    "    print('training max u hybrid model... wait for it...')\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_gradient_boost_regression_balanced_max_u.csv'])\n",
    "    hybrid_regressor_max_u = my_ai.Context(strategy=my_ai.GradientBoostRegressorStrategy(hyper_params)) \n",
    "    hybrid_regressor_max_u.fit(data=data_max_u_hybrid)\n",
    "    utils.serialize_object('pickles\\hybrid_models_benchmark\\hybrid_regressor_max_u', hybrid_regressor_max_u)\n",
    "else: \n",
    "    print('loading max u hybrid model...')\n",
    "    hybrid_regressor_max_u = utils.deserialize_object('pickles\\hybrid_models_benchmark\\hybrid_regressor_max_u')\n",
    "\n",
    "# min_u \n",
    "if 'hybrid_regressor_min_u.pickle' not in os.listdir('pickles\\hybrid_models_benchmark'):\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_min_u.csv'])\n",
    "    hybrid_regressor_min_u = my_ai.Context(my_ai.GradientBoostRegressorStrategy(hyper_params))\n",
    "    hybrid_regressor_min_u.fit(data=data_min_u_hybrid)\n",
    "    utils.serialize_object('pickles\\hybrid_models_benchmark\\hybrid_regressor_min_u', hybrid_regressor_min_u)\n",
    "else:\n",
    "    print('loading min u hybrid model...')\n",
    "    hybrid_regressor_min_u = utils.deserialize_object('pickles\\hybrid_models_benchmark\\hybrid_regressor_min_u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<thesis_package.aimodels.LinearRegressionStrategy object at 0x000001C2B51F7E20>\n",
      "<thesis_package.aimodels.GradientBoostRegressorStrategy object at 0x000001C2C0D1D760>\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "# max_u\n",
    "regression_max_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_regressor_focused')\n",
    "max_u_reg_lr = deepcopy(regression_max_u_focused.strategies[0])\n",
    "print(max_u_reg_lr)\n",
    "reg_result = max_u_reg_lr.predict(data_max_u_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives_ctr:  2\n",
      "true_negatives_ctr:  288882\n",
      "false_positives_ctr:  13665\n",
      "false_negatives_ctr:  4947\n",
      "6012819352381992129\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'X_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'X_test'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\hybrid_model_joint training.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/hybrid_model_joint%20training.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m reg_evaluation \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries({\u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m: reg_recall, \u001b[39m'\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m'\u001b[39m: reg_precision, \u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m: reg_f1, \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m: reg_accuracy, \u001b[39m'\u001b[39m\u001b[39mmcc\u001b[39m\u001b[39m'\u001b[39m: reg_mcc})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/hybrid_model_joint%20training.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# same for hybrid model \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/hybrid_model_joint%20training.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# max_u\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/hybrid_model_joint%20training.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m hybrid_reg_result \u001b[39m=\u001b[39m hybrid_regressor_max_u\u001b[39m.\u001b[39;49mpredict(data_max_u_hybrid[\u001b[39m'\u001b[39;49m\u001b[39mX_test\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/hybrid_model_joint%20training.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m metric\u001b[39m.\u001b[39mget_prediction_scores(hybrid_reg_result, data_max_u_hybrid[\u001b[39m'\u001b[39m\u001b[39my_test\u001b[39m\u001b[39m'\u001b[39m], threshold\u001b[39m=\u001b[39mscaled_threshold)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/hybrid_model_joint%20training.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m hybrid_reg_accuracy \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mhybrid_accuracy\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\..\\thesis_package\\aimodels.py:32\u001b[0m, in \u001b[0;36mContext.predict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, data: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategies[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpredict(data)\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\..\\thesis_package\\aimodels.py:62\u001b[0m, in \u001b[0;36mGradientBoostRegressorStrategy.predict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, data: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     prediction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(data[\u001b[39m'\u001b[39;49m\u001b[39mX_test\u001b[39;49m\u001b[39m'\u001b[39;49m]), \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     63\u001b[0m     \u001b[39mtry\u001b[39;00m: \n\u001b[0;32m     64\u001b[0m         \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(prediction, columns\u001b[39m=\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39my_test\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'X_test'"
     ]
    }
   ],
   "source": [
    "# max_u\n",
    "metric = metrics.Metrics()\n",
    "_threshold = lambda experiment: max_u_threshold / data_max_u_sparse['scaler']['y'] if 'max_u' in experiment else min_u_threshold/ data_min_u_sparse['scaler']['y']\n",
    "scaled_threshold = _threshold('max_u')\n",
    "metric.get_prediction_scores(reg_result, data_max_u_sparse['y_test'], threshold=scaled_threshold)\n",
    "reg_accuracy = metric.hybrid_accuracy\n",
    "reg_precision = metric.hybrid_precision\n",
    "reg_recall = metric.hybrid_recall\n",
    "reg_f1 = metric.hybrid_f1\n",
    "reg_mcc = metric.hybrid_mcc\n",
    "reg_evaluation = pd.Series({'recall': reg_recall, 'precision': reg_precision, 'f1': reg_f1, 'accuracy': reg_accuracy, 'mcc': reg_mcc})\n",
    "# same for hybrid model \n",
    "# max_u\n",
    "hybrid_reg_result = hybrid_regressor_max_u.predict(data_max_u_hybrid['X_test'])\n",
    "metric.get_prediction_scores(hybrid_reg_result, data_max_u_hybrid['y_test'], threshold=scaled_threshold)\n",
    "hybrid_reg_accuracy = metric.hybrid_accuracy\n",
    "hybrid_reg_precision = metric.hybrid_precision\n",
    "hybrid_reg_recall = metric.hybrid_recall\n",
    "hybrid_reg_f1 = metric.hybrid_f1\n",
    "hybrid_reg_mcc = metric.hybrid_mcc\n",
    "hybrid_reg_evaluation = pd.Series({'recall': hybrid_reg_recall, 'precision': hybrid_reg_precision, 'f1': hybrid_reg_f1, 'accuracy': hybrid_reg_accuracy, 'mcc': hybrid_reg_mcc})\n",
    "pd.DataFrame([reg_evaluation, hybrid_reg_evaluation], index=['regression', 'hybrid_regression'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_u\n",
    "regression_min_u_sparse = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse')\n",
    "min_u_reg_gb = deepcopy(regression_min_u_sparse.strategies[1])\n",
    "print(min_u_reg_gb)\n",
    "reg_result = min_u_reg_gb.predict(data_min_u_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_u\n",
    "scaled_threshold = _threshold('min_u')\n",
    "metric.get_prediction_scores(reg_result, data_min_u_sparse['y_test'], threshold=scaled_threshold)\n",
    "reg_accuracy = metric.hybrid_accuracy\n",
    "reg_precision = metric.hybrid_precision\n",
    "reg_recall = metric.hybrid_recall\n",
    "reg_f1 = metric.hybrid_f1\n",
    "reg_mcc = metric.hybrid_mcc\n",
    "reg_evaluation = pd.Series({'recall': reg_recall, 'precision': reg_precision, 'f1': reg_f1, 'accuracy': reg_accuracy, 'mcc': reg_mcc})\n",
    "# same for hybrid model \n",
    "# min_u\n",
    "hybrid_reg_result = hybrid_regressor_min_u.predict(data_min_u_hybrid['X_test'])\n",
    "metric.get_prediction_scores(hybrid_reg_result, data_min_u_hybrid['y_test'], threshold=scaled_threshold)\n",
    "hybrid_reg_accuracy = metric.hybrid_accuracy\n",
    "hybrid_reg_precision = metric.hybrid_precision\n",
    "hybrid_reg_recall = metric.hybrid_recall\n",
    "hybrid_reg_f1 = metric.hybrid_f1\n",
    "hybrid_reg_mcc = metric.hybrid_mcc\n",
    "hybrid_reg_evaluation = pd.Series({'recall': hybrid_reg_recall, 'precision': hybrid_reg_precision, 'f1': hybrid_reg_f1, 'accuracy': hybrid_reg_accuracy, 'mcc': hybrid_reg_mcc})\n",
    "pd.DataFrame([reg_evaluation, hybrid_reg_evaluation], index=['regression', 'hybrid_regression'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
