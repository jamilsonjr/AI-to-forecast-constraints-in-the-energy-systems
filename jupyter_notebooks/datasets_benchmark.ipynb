{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of this Article** \n",
    "- Loading best hyperparameters for each model\n",
    "- Model training\n",
    "- Results discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading best hyperparameters for each model\n",
    "\n",
    "TODO... explain this model bench mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hyperparameters dataset.\n",
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse hyper params:\n",
      "\n",
      "params_gradient_boost_regression_sparse_max_u.csv :\n",
      "            params                value\n",
      "0   n_estimators                   19\n",
      "1  learning_rate  0.12236030175251943\n",
      "2           loss        squared_error\n",
      "3          value   0.5471179901945081\n",
      "params_gradient_boost_regression_sparse_min_u.csv :\n",
      "            params                value\n",
      "0   n_estimators                  119\n",
      "1  learning_rate  0.17057256340998259\n",
      "2           loss        squared_error\n",
      "3          value   0.5605410379664836\n",
      "params_mlp_regression_sparse_max_u.csv :\n",
      "          params                value\n",
      "0  hidden_size                   31\n",
      "1     n_layers                    2\n",
      "2      dropout  0.10137160649535842\n",
      "3   activation              sigmoid\n",
      "4    optimizer                  sgd\n",
      "5           lr  0.04276419439103241\n",
      "6       epochs                   60\n",
      "7   batch_size                   32\n",
      "8   classifier                False\n",
      "9        value   0.9920083183694078\n",
      "params_support_vector_regression_sparse_max_u.csv :\n",
      "     params                value\n",
      "0  kernel                 poly\n",
      "1       C  0.11969004999078645\n",
      "2  degree                    3\n",
      "3   gamma   0.8134043653053384\n",
      "4   value   0.5948421045024109\n",
      "params_support_vector_regression_sparse_min_u.csv :\n",
      "     params                value\n",
      "0  kernel                  rbf\n",
      "1       C   0.6252662017403572\n",
      "2  degree                    2\n",
      "3   gamma   0.9209593565808238\n",
      "4   value  0.49954691265462037\n",
      "params_xgboost_regression_sparse_max_u.csv :\n",
      "                params                   value\n",
      "0            booster                  gbtree\n",
      "1             lambda    0.007302077546448604\n",
      "2              alpha  0.00023232319288573486\n",
      "3          subsample      0.3989524532499371\n",
      "4   colsample_bytree      0.6379536075193752\n",
      "5          max_depth                       3\n",
      "6   min_child_weight                       8\n",
      "7                eta    0.025369096520541503\n",
      "8              gamma   0.0002987489109575797\n",
      "9        grow_policy               depthwise\n",
      "10             value      0.5682964907629325\n",
      "params_xgboost_regression_sparse_min_u.csv :\n",
      "                params                  value\n",
      "0            booster                 gbtree\n",
      "1             lambda  5.192853611204443e-08\n",
      "2              alpha  0.0024263653333410178\n",
      "3          subsample     0.5244756630973081\n",
      "4   colsample_bytree    0.45008288591532547\n",
      "5          max_depth                      3\n",
      "6   min_child_weight                      8\n",
      "7                eta    0.49950249957850157\n",
      "8              gamma    0.00051534817539108\n",
      "9        grow_policy              lossguide\n",
      "10             value     0.5618463270407074\n",
      "Focused hyper params:\n",
      "\n",
      "params_gradient_boost_regression_balanced_max_u.csv :\n",
      "           params                value\n",
      "0   n_estimators                   84\n",
      "1  learning_rate  0.10196039082902036\n",
      "2           loss        squared_error\n",
      "3          value   0.8047837141310673\n",
      "params_gradient_boost_regression_balanced_min_u.csv :\n",
      "           params                value\n",
      "0   n_estimators                  405\n",
      "1  learning_rate  0.21678392195811974\n",
      "2           loss        squared_error\n",
      "3          value   0.8008882601031363\n",
      "params_gradient_boost_regression_focused_max_u.csv :\n",
      "           params                value\n",
      "0   n_estimators                   10\n",
      "1  learning_rate   0.1277397777263443\n",
      "2           loss        squared_error\n",
      "3          value  0.07828676447892846\n",
      "params_gradient_boost_regression_focused_min_u.csv :\n",
      "           params                 value\n",
      "0   n_estimators                    23\n",
      "1  learning_rate   0.16873340986794932\n",
      "2           loss         squared_error\n",
      "3          value  0.049361328169992044\n",
      "params_mlp_regression_focused_max_u.csv :\n",
      "         params                  value\n",
      "0  hidden_size                     34\n",
      "1     n_layers                      3\n",
      "2      dropout  0.0030412321477918842\n",
      "3   activation                   relu\n",
      "4    optimizer                    sgd\n",
      "5           lr  9.741292351005151e-05\n",
      "6       epochs                     55\n",
      "7   batch_size                      8\n",
      "8   classifier                  False\n",
      "9        value    0.11670139032418816\n",
      "params_mlp_regression_focused_min_u.csv :\n",
      "         params                   value\n",
      "0  hidden_size                      27\n",
      "1     n_layers                       3\n",
      "2      dropout      0.2515211746984172\n",
      "3   activation                    relu\n",
      "4    optimizer                     sgd\n",
      "5           lr  2.9295769171852248e-05\n",
      "6       epochs                      99\n",
      "7   batch_size                       1\n",
      "8   classifier                   False\n",
      "9        value     0.07731086297833932\n",
      "params_support_vector_regression_balanced_max_u.csv :\n",
      "    params               value\n",
      "0  kernel                poly\n",
      "1       C  0.8152518433534094\n",
      "2  degree                   5\n",
      "3   gamma     0.4110716960815\n",
      "4   value  0.8116363255643548\n",
      "params_support_vector_regression_balanced_min_u.csv :\n",
      "    params               value\n",
      "0  kernel                 rbf\n",
      "1       C  0.7099331389181525\n",
      "2  degree                   2\n",
      "3   gamma  0.9790446040825302\n",
      "4   value  0.7814216257890312\n",
      "params_support_vector_regression_focused_max_u.csv :\n",
      "    params                value\n",
      "0  kernel                 poly\n",
      "1       C  0.05863474807242084\n",
      "2  degree                    3\n",
      "3   gamma   0.1783152932465746\n",
      "4   value  0.08036042675248821\n",
      "params_support_vector_regression_focused_min_u.csv :\n",
      "    params                value\n",
      "0  kernel                  rbf\n",
      "1       C  0.16377316609499395\n",
      "2  degree                    3\n",
      "3   gamma   0.9186934763754282\n",
      "4   value  0.04782616960183752\n",
      "params_xgboost_regression_balanced_max_u.csv :\n",
      "               params                   value\n",
      "0            booster                  gbtree\n",
      "1             lambda  2.7672083092013075e-06\n",
      "2              alpha   3.290882841064105e-08\n",
      "3          subsample      0.7299372589310998\n",
      "4   colsample_bytree      0.7697558364196286\n",
      "5          max_depth                       7\n",
      "6   min_child_weight                       3\n",
      "7                eta     0.02942913923800093\n",
      "8              gamma   3.363098771057399e-08\n",
      "9        grow_policy               depthwise\n",
      "10             value      0.8206199571838381\n",
      "params_xgboost_regression_balanced_min_u.csv :\n",
      "               params                   value\n",
      "0            booster                    dart\n",
      "1             lambda  3.0688194641631903e-06\n",
      "2              alpha    0.026682455376267594\n",
      "3          subsample      0.8323024765852791\n",
      "4   colsample_bytree      0.6500191262073954\n",
      "5          max_depth                       7\n",
      "6   min_child_weight                       9\n",
      "7                eta    0.032163120007348896\n",
      "8              gamma   3.740358761831146e-08\n",
      "9        grow_policy               lossguide\n",
      "10       sample_type                 uniform\n",
      "11    normalize_type                    tree\n",
      "12         rate_drop   6.631337451389453e-05\n",
      "13         skip_drop  1.7796717715387674e-05\n",
      "14             value      0.7975742183383274\n",
      "params_xgboost_regression_focused_max_u.csv :\n",
      "              params                   value\n",
      "0           booster                gblinear\n",
      "1            lambda  2.9561150396870038e-05\n",
      "2             alpha   0.0021255751095305736\n",
      "3         subsample      0.7119400343735331\n",
      "4  colsample_bytree      0.2865645333043352\n",
      "5             value     0.07210034785268532\n",
      "params_xgboost_regression_focused_min_u.csv :\n",
      "              params                   value\n",
      "0           booster                gblinear\n",
      "1            lambda  1.5185872809070974e-07\n",
      "2             alpha   8.213823895283899e-08\n",
      "3         subsample     0.38962585233131064\n",
      "4  colsample_bytree      0.6838595461097349\n",
      "5             value     0.04864510260342804\n",
      "Boolean hyper params:\n",
      "\n",
      "params_gradient_boost_balanced_classifier_max_u.csv :\n",
      "           params               value\n",
      "0   n_estimators                  31\n",
      "1  learning_rate  0.1337791953835611\n",
      "2           loss         exponential\n",
      "3          value  0.8267361934107609\n",
      "params_gradient_boost_balanced_classifier_min_u.csv :\n",
      "           params               value\n",
      "0   n_estimators                 872\n",
      "1  learning_rate  0.1641390435327391\n",
      "2           loss         exponential\n",
      "3          value  0.8315464825965634\n",
      "params_gradient_boost_sparse_classifier_max_u.csv :\n",
      "           params               value\n",
      "0   n_estimators                 876\n",
      "1  learning_rate  0.9946850527095437\n",
      "2           loss         exponential\n",
      "3          value  0.5272050688926884\n",
      "params_gradient_boost_sparse_classifier_min_u.csv :\n",
      "           params                value\n",
      "0   n_estimators                   39\n",
      "1  learning_rate  0.16098146977276998\n",
      "2           loss             log_loss\n",
      "3          value   0.6103714600956233\n",
      "params_mlp_balanced_classifier_max_u.csv :\n",
      "         params                  value\n",
      "0  hidden_size                     73\n",
      "1     n_layers                      3\n",
      "2      dropout   0.027925450381307542\n",
      "3   activation                   tanh\n",
      "4    optimizer                   adam\n",
      "5           lr  0.0009714183218071047\n",
      "6       epochs                     78\n",
      "7   batch_size                      4\n",
      "8   classifier                   True\n",
      "9        value      0.666892177589852\n",
      "params_mlp_balanced_classifier_min_u.csv :\n",
      "         params                  value\n",
      "0  hidden_size                     85\n",
      "1     n_layers                      3\n",
      "2      dropout    0.14506881483415743\n",
      "3   activation                sigmoid\n",
      "4    optimizer                   adam\n",
      "5           lr  0.0075935750665140096\n",
      "6       epochs                     89\n",
      "7   batch_size                     32\n",
      "8   classifier                   True\n",
      "9        value      0.610796816569182\n",
      "params_mlp_sparse_classifier_max_u.csv :\n",
      "         params                 value\n",
      "0  hidden_size                    43\n",
      "1     n_layers                     2\n",
      "2      dropout   0.09295593470419183\n",
      "3   activation                  tanh\n",
      "4    optimizer                  adam\n",
      "5           lr  0.002704170984503173\n",
      "6       epochs                    89\n",
      "7   batch_size                   128\n",
      "8   classifier                  True\n",
      "9        value   0.20501329656259235\n",
      "params_mlp_sparse_classifier_min_u.csv :\n",
      "         params                  value\n",
      "0  hidden_size                     58\n",
      "1     n_layers                      1\n",
      "2      dropout    0.39114601772840446\n",
      "3   activation                   relu\n",
      "4    optimizer                   adam\n",
      "5           lr  0.0026213906541996917\n",
      "6       epochs                     87\n",
      "7   batch_size                     64\n",
      "8   classifier                   True\n",
      "9        value    0.22329687038053808\n",
      "params_support_vector_balanced_classifier_max_u.csv :\n",
      "    params                value\n",
      "0  kernel                  rbf\n",
      "1       C  0.41893172852224936\n",
      "2  degree                    2\n",
      "3   gamma   0.8207285355315294\n",
      "4   value   0.5913751754094161\n",
      "params_support_vector_balanced_classifier_min_u.csv :\n",
      "    params               value\n",
      "0  kernel                 rbf\n",
      "1       C  0.7018071100873359\n",
      "2  degree                   2\n",
      "3   gamma  0.8564841524851309\n",
      "4   value   0.605450210300487\n",
      "params_support_vector_sparse_classifier_max_u.csv :\n",
      "    params                  value\n",
      "0  kernel                    rbf\n",
      "1       C  3.277034979078225e-06\n",
      "2  degree                      2\n",
      "3   gamma  6.524079612801282e-05\n",
      "4   value                    0.0\n",
      "params_support_vector_sparse_classifier_min_u.csv :\n",
      "    params                value\n",
      "0  kernel                  rbf\n",
      "1       C  0.09624165012082381\n",
      "2  degree                    5\n",
      "3   gamma    0.684079135847173\n",
      "4   value  0.36220024694066116\n",
      "params_xgboost_balanced_classifier_max_u.csv :\n",
      "               params                   value\n",
      "0            booster                    dart\n",
      "1             lambda  0.00015884090078506437\n",
      "2              alpha     0.19774958488491742\n",
      "3          subsample      0.7634517553183634\n",
      "4   colsample_bytree       0.824966208242803\n",
      "5          max_depth                       5\n",
      "6   min_child_weight                       8\n",
      "7                eta      0.1039306615628526\n",
      "8              gamma      0.6835098793603533\n",
      "9        grow_policy               depthwise\n",
      "10       sample_type                 uniform\n",
      "11    normalize_type                    tree\n",
      "12         rate_drop  4.7305061368641385e-05\n",
      "13         skip_drop  4.6864756927258716e-05\n",
      "14             value       0.665520355433705\n",
      "params_xgboost_balanced_classifier_min_u.csv :\n",
      "               params                   value\n",
      "0            booster                  gbtree\n",
      "1             lambda   4.054592530775095e-05\n",
      "2              alpha   5.174708824228512e-05\n",
      "3          subsample      0.8469092367338825\n",
      "4   colsample_bytree      0.5847478326272141\n",
      "5          max_depth                       5\n",
      "6   min_child_weight                       6\n",
      "7                eta      0.2768740439937659\n",
      "8              gamma  3.1155873290293454e-06\n",
      "9        grow_policy               depthwise\n",
      "10             value      0.6966389919995043\n",
      "params_xgboost_sparse_classifier_max_u.csv :\n",
      "               params                   value\n",
      "0            booster                    dart\n",
      "1             lambda    0.003063420575512009\n",
      "2              alpha  1.9003021085468398e-05\n",
      "3          subsample      0.5229147527577294\n",
      "4   colsample_bytree      0.7098150896390416\n",
      "5          max_depth                       7\n",
      "6   min_child_weight                      10\n",
      "7                eta      0.6910312611856446\n",
      "8              gamma  1.6977649889061874e-05\n",
      "9        grow_policy               depthwise\n",
      "10       sample_type                 uniform\n",
      "11    normalize_type                  forest\n",
      "12         rate_drop  2.7388226591786135e-06\n",
      "13         skip_drop   0.0005988929023034813\n",
      "14             value     0.35918702265652536\n",
      "params_xgboost_sparse_classifier_min_u.csv :\n",
      "               params                   value\n",
      "0            booster                    dart\n",
      "1             lambda  1.2196277238623086e-07\n",
      "2              alpha   8.872072866566911e-08\n",
      "3          subsample       0.852609953518945\n",
      "4   colsample_bytree      0.9158639052000911\n",
      "5          max_depth                       3\n",
      "6   min_child_weight                      10\n",
      "7                eta     0.16082146155580598\n",
      "8              gamma     0.08975552616350903\n",
      "9        grow_policy               lossguide\n",
      "10       sample_type                 uniform\n",
      "11    normalize_type                    tree\n",
      "12         rate_drop   6.970741073918815e-08\n",
      "13         skip_drop    0.012644258522609655\n",
      "14             value      0.4273398364926093\n"
     ]
    }
   ],
   "source": [
    "sparse_hyper_params = {}\n",
    "focused_hyper_params = {}\n",
    "boolean_hyper_params = {}\n",
    "for file in os.listdir('hyper_params_results'):\n",
    "    if file.endswith('.csv') and 'sparse' in file.split('_') and 'classifier' not in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'focused' in file.split('_') and 'classifier' not in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        focused_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'balanced' in file.split('_') and 'classifier' not in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        focused_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'classifier' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        boolean_hyper_params[file] = df\n",
    "print('Sparse hyper params:\\n')\n",
    "for key in sparse_hyper_params.keys():\n",
    "    print(key, ':\\n ',sparse_hyper_params[key])\n",
    "print('Focused hyper params:\\n')\n",
    "for key in focused_hyper_params.keys():\n",
    "    print(key, ':\\n',focused_hyper_params[key])\n",
    "print('Boolean hyper params:\\n')\n",
    "for key in boolean_hyper_params.keys():\n",
    "    print(key, ':\\n',boolean_hyper_params[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_size': 34,\n",
       " 'n_layers': 3,\n",
       " 'dropout': 0.0030412321477918842,\n",
       " 'activation': 'relu',\n",
       " 'optimizer': 'sgd',\n",
       " 'lr': 9.741292351005151e-05,\n",
       " 'epochs': 55,\n",
       " 'batch_size': 8,\n",
       " 'classifier': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "def get_hyper_params_from_df(df):\n",
    "    output = {}\n",
    "    for row in df.iterrows():\n",
    "        if row[1]['params'] != 'value':\n",
    "            try:\n",
    "                output[row[1]['params']] = ast.literal_eval(row[1]['value'])\n",
    "            except :\n",
    "                output[row[1]['params']] = row[1]['value']\n",
    "    return output\n",
    "get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_max_u.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from thesis_package import aimodels as my_ai, utils, metrics\n",
    "from copy import deepcopy\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data sparse\n",
    "y_max_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse, test_size=0.2, scaling=True)\n",
    "data_max_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse, test_size=0.2, scaling=True)\n",
    "data_min_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data\n",
    "y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_max_u_bool = y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)]\n",
    "y_min_u_bool = y_min_u_bool[utils.cols_with_positive_values(y_min_u_bool)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_bool, test_size=0.2, scaling=True)\n",
    "data_max_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_bool, test_size=0.2, scaling=True)\n",
    "data_min_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered data\n",
    "y_max_u_filtered = deepcopy(y_max_u_sparse[utils.cols_with_positive_values(y_max_u_bool)])\n",
    "y_min_u_filtered = deepcopy(y_min_u_sparse[utils.cols_with_positive_values(y_min_u_bool)])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_filtered, test_size=0.2, scaling=True)\n",
    "data_max_u_filtered = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_filtered, test_size=0.2, scaling=True)\n",
    "data_min_u_filtered = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification data size:  (9044, 10)\n",
      "Regression data size:  (9044, 10)\n",
      "Positive in classification data:  5036.0\n",
      "Positive in regression data:  5036\n",
      "Theshhold:  0.001591058368850724\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the classiciation testing data and the filtered testing data\n",
    "print('Classification data size: ', data_max_u_bool['y_test'].shape)\n",
    "print('Regression data size: ', data_max_u_filtered['y_test'].shape)\n",
    "print('Positive in classification data: ', utils.count_positives_class(data_max_u_bool['y_test']))\n",
    "#unscaled_y_test = pd.DataFrame(data_max_u_filtered['scaler']['y'].inverse_transform(data_max_u_filtered['y_test']), columns=data_max_u_filtered['y_test'].columns)\n",
    "unscaled_y_test = utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y'])\n",
    "print('Positive in regression data: ', utils.count_positives_reg(unscaled_y_test, utils.compute_threshold(y_max_u_sparse)))\n",
    "print('Theshhold: ', utils.compute_threshold(y_max_u_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data focused\n",
    "y_max_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_focused_constr.csv')\n",
    "exogenous_data_focused_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_focused.csv')\n",
    "y_min_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_focused_constr.csv')\n",
    "exogenous_data_focused_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_focused.csv')\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_focused_max_u, y_max_u_focused, test_size=0.2, scaling=True)\n",
    "data_max_u_focused = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_focused_min_u, y_min_u_focused, test_size=0.2, scaling=True)\n",
    "data_min_u_focused = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data balanced\n",
    "y_max_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced, test_size=0.2, scaling=True)\n",
    "data_max_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced, test_size=0.2, scaling=True)\n",
    "data_min_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count in classification data max_u : 5036.0\n",
      "Positive count in regression data max_u with threshold 0.001591058368850724 : 5036\n",
      "\n",
      "\n",
      "Positive count in classification data min_u : 6018.0\n",
      "Positive count in regression data min_u with threshold 0.0020242378560612192 : 6018\n",
      "\n",
      "\n",
      "Negative count in classification data max_u : 85404.0\n",
      "Negative count in regression data max_u with threshold 0.001591058368850724 : 85404\n",
      "\n",
      "\n",
      "Negative count in classification data min_u : 84422.0\n",
      "Negative count in regression data min_u with threshold 0.0020242378560612192 : 84422\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.check_positive_count(utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y']), data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')\n",
    "utils.check_positive_count(utils.unscale_df(data_min_u_filtered['y_test'], data_min_u_filtered['scaler']['y']), data_min_u_bool['y_test'], utils.compute_threshold(y_min_u_sparse), experiment='min_u')\n",
    "utils.check_negative_count(utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y']), data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')\n",
    "utils.check_negative_count(utils.unscale_df(data_min_u_filtered['y_test'], data_min_u_filtered['scaler']['y']), data_min_u_bool['y_test'], utils.compute_threshold(y_min_u_sparse), experiment='min_u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "In this section the models will be trained with the hyperparameters loaded above. All the models will be stored in the same `Context` object for later evaluation. The `Context` object is a class that stores all the models and their respective hyperparameters. The `Context` object is defined in the `aimodels.py` file. The `Context` object is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['lr', 'gb', 'xgb', 'svr', 'mlp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['params_gradient_boost_regression_sparse_max_u.csv', 'params_gradient_boost_regression_sparse_min_u.csv', 'params_mlp_regression_sparse_max_u.csv', 'params_support_vector_regression_sparse_max_u.csv', 'params_support_vector_regression_sparse_min_u.csv', 'params_xgboost_regression_sparse_max_u.csv', 'params_xgboost_regression_sparse_min_u.csv'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_hyper_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_u regression sparse\n",
    "if 'max_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression sparse')\n",
    "    # Linear Regression\n",
    "    regressor_max_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_max_u.csv'])\n",
    "    regressor_max_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_xgboost_regression_sparse_max_u.csv']) \n",
    "    regressor_max_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_support_vector_regression_sparse_max_u.csv'])\n",
    "    regressor_max_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_mlp_regressor_sparse_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_sparse['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_sparse['y_train'].shape[1]\n",
    "    regressor_max_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_sparse', regressor_max_u)\n",
    "else:\n",
    "    print('Loading max_u regression sparse') \n",
    "    regressor_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_regressor_sparse')\n",
    "\n",
    "testing_data = {'max_u_regressor_sparse': {}}\n",
    "for model, strategy in zip(models, regressor_max_u.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_sparse['y_test'].columns)\n",
    "    testing_data['max_u_regressor_sparse'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_sparse'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_sparse'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_u regression focused\n",
    "if 'max_u_regressor_focused.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression focused')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_focused = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_gradient_boost_regression_focused_max_u.csv'])\n",
    "    regressor_max_u_focused.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_xgboost_regression_focused_max_u.csv']) \n",
    "    regressor_max_u_focused.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_support_vector_regression_focused_max_u.csv'])\n",
    "    regressor_max_u_focused.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_mlp_regressor_focused_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_focused['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_focused['y_train'].shape[1]\n",
    "    regressor_max_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_focused', regressor_max_u_focused)\n",
    "else: \n",
    "    print('Loading max_u regression focused')\n",
    "    regressor_max_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\\\max_u_regressor_focused')\n",
    "\n",
    "testing_data['max_u_regressor_focused'] = {}\n",
    "for model, strategy in zip(models, regressor_max_u_focused.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_sparse['y_test'].columns)\n",
    "    testing_data['max_u_regressor_focused'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_focused'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_focused'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_u classification\n",
    "if 'max_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u classification')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(boolean_hyper_params['params_gradient_boost_classifier_max_u.csv'])\n",
    "    classifier_max_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(boolean_hyper_params['params_xgboost_classifier_max_u.csv'])\n",
    "    classifier_max_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(boolean_hyper_params['params_support_vector_classifier_max_u.csv'])\n",
    "    classifier_max_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(boolean_hyper_params['params_mlp_classifier_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_bool['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_bool['y_train'].shape[1]\n",
    "    regressor_max_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_bool)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_classifier', classifier_max_u)\n",
    "else: \n",
    "    print('Loading max_u classification')\n",
    "    classifier_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_classifier')\n",
    "\n",
    "testing_data['max_u_classifier'] = {}\n",
    "for model, strategy in zip(models, classifier_max_u.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_bool['y_test'].columns)\n",
    "    testing_data['max_u_classifier'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_classifier'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_classifier'][model]['real'] = deepcopy(data_max_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min u regression training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_u regression sparse\n",
    "if 'min_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression sparse')\n",
    "    # Linear Regression\n",
    "    regressor_min_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_xgboost_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_support_vector_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_mlp_regressor_sparse_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_sparse['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_sparse['y_train'].shape[1]\n",
    "    regressor_max_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_min_u_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse', regressor_min_u)\n",
    "else:\n",
    "    print('Loading min_u regression sparse')\n",
    "    regressor_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse')\n",
    "\n",
    "testing_data['min_u_regressor_sparse'] = {}\n",
    "for model, strategy in zip(models, regressor_min_u.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_sparse['y_test'].columns)\n",
    "    testing_data['min_u_regressor_sparse'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_sparse'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_sparse'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_u regression focused\n",
    "if 'min_u_regressor_focused.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression focused')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_focused = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_gradient_boost_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_xgboost_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_support_vector_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_mlp_regressor_focused_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_focused['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_focused['y_train'].shape[1]\n",
    "    regressor_max_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_min_u_focused)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_focused', regressor_min_u_focused)\n",
    "else:\n",
    "    print('Loading min_u regression focused')\n",
    "    regressor_min_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_focused')\n",
    "\n",
    "testing_data['min_u_regressor_focused'] = {}\n",
    "for model, strategy in zip(models, regressor_min_u_focused.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_sparse['y_test'].columns)\n",
    "    testing_data['min_u_regressor_focused'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_focused'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_focused'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_u classification\n",
    "if 'min_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u classification')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(boolean_hyper_params['params_gradient_boost_classifier_max_u.csv'])\n",
    "    classifier_min_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(boolean_hyper_params['params_xgboost_classifier_min_u.csv'])\n",
    "    classifier_min_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(boolean_hyper_params['params_support_vector_classifier_min_u.csv'])\n",
    "    classifier_min_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_classifier', classifier_min_u)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(boolean_hyper_params['params_mlp_classifier_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_bool['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_bool['y_train'].shape[1]\n",
    "    regressor_max_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_min_u_bool)\n",
    "else: \n",
    "    print('Loading min_u classification')\n",
    "    classifier_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_classifier')\n",
    "\n",
    "testing_data['min_u_classifier'] = {}\n",
    "for model, strategy in zip(models, classifier_min_u.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_bool['y_test'].columns)\n",
    "    testing_data['min_u_classifier'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_classifier'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_classifier'][model]['real'] = deepcopy(data_min_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Discussion\n",
    "In this section the results of the training and testing are presented and compared. The main objectives of this experience is to compare the performance of the regression models in terms of the hybrid metrics confusion matrix and the hybrid metrics rmse. The comparisons will be the following:\n",
    "- Compare the confusion matrices of the classification models and the regression models evaluate with the hybrid metrics.\n",
    "- Compare the error results of the regression models trained with the focused dataset and the sparse dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing all models: Function that receives a dict with the real and predicted values, and outputs a dataframe with the results of the metrics.\n",
    "# Accumulate all the classifications for each bus.\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for bus in testing_data['max_u_classifier']['gb']['predicted'].columns:\n",
    "    # Compute tp, tn, fp, fn\n",
    "    tp += sum((testing_data['max_u_classifier']['gb']['predicted'][bus] == 1) & (testing_data['max_u_classifier']['gb']['real'][bus] == 1))\n",
    "    tn += sum((testing_data['max_u_classifier']['gb']['predicted'][bus] == 0) & (testing_data['max_u_classifier']['gb']['real'][bus] == 0))\n",
    "    fp += sum((testing_data['max_u_classifier']['gb']['predicted'][bus] == 1) & (testing_data['max_u_classifier']['gb']['real'][bus] == 0))\n",
    "    fn += sum((testing_data['max_u_classifier']['gb']['predicted'][bus] == 0) & (testing_data['max_u_classifier']['gb']['real'][bus] == 1))\n",
    "    # try:\n",
    "    #     _tp, _tn, _fp, _fn = confusion_matrix(testing_data['max_u_classifier']['gb']['real'][bus], testing_data['max_u_classifier']['gb']['predicted'][bus]).ravel()\n",
    "    #     tp += _tp; tn += _tn; fp += _fp; fn += _fn\n",
    "    # except: \n",
    "    #     print('Problem with bus: ', bus)\n",
    "print('{} + {} = {} = {} possible positive values.'.format(tp, fn, tp+fn, testing_data['max_u_classifier']['gb']['real'].sum().sum()))\n",
    "print('{} + {} = {} = {} possible negative values.'.format(tn, fp, tn+fp, testing_data['max_u_classifier']['gb']['real'].shape[0]*testing_data['max_u_classifier']['gb']['real'].shape[1] - testing_data['max_u_classifier']['gb']['real'].sum().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a multi-index dataframe with the results of the metrics. The first index is the testing_data.keys(), the second index are the tp, tn, fp, fn, and the columns are the models.\n",
    "columns = ['tp', 'tn', 'fp', 'fn', '(hybrid)accuracy', '(hybrid)precision', '(hybrid)recall', '(hybrid)f1']\n",
    "index = pd.MultiIndex.from_product([testing_data.keys(), ['lr', 'gb', 'xgb', 'svr']], names=['experiment', 'class'])\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "classifier_experiments =[experiment for experiment in testing_data.keys() if 'classifier' in experiment.split('_')]\n",
    "regressor_experiments = [experiment for experiment in testing_data.keys() if 'regressor' in experiment.split('_')]\n",
    "# Classifier experiments\n",
    "for experiment in classifier_experiments:\n",
    "    for model in testing_data[experiment].keys():\n",
    "        for bus in testing_data[experiment][model]['predicted'].columns:\n",
    "            try:\n",
    "                tp += sum((testing_data[experiment][model]['predicted'][bus] == 1) & (testing_data[experiment][model]['real'][bus] == 1))\n",
    "                tn += sum((testing_data[experiment][model]['predicted'][bus] == 0) & (testing_data[experiment][model]['real'][bus] == 0))\n",
    "                fp += sum((testing_data[experiment][model]['predicted'][bus] == 1) & (testing_data[experiment][model]['real'][bus] == 0))\n",
    "                fn += sum((testing_data[experiment][model]['predicted'][bus] == 0) & (testing_data[experiment][model]['real'][bus] == 1))\n",
    "            except: \n",
    "                print('In the experiment ', experiment, ' and model ', model, ' there was a problem with bus: ', bus)\n",
    "                if not testing_data[experiment][model]['real'][bus].any():\n",
    "                    print('Bus {} has no positive data points. Just ignore the little shit.'.format(bus))    \n",
    "        df.loc[(experiment, model), 'tp'] = tp\n",
    "        df.loc[(experiment, model), 'tn'] = tn\n",
    "        df.loc[(experiment, model), 'fp'] = fp\n",
    "        df.loc[(experiment, model), 'fn'] = fn\n",
    "        print('Experiment: {}, model: {}, tp: {}, tn: {}, fp: {}, fn: {}'.format(experiment, model, tp, tn, fp, fn))\n",
    "        if (tp + tn + fp + fn) != 0:\n",
    "            accuracy = (tp + tn ) / (tp + tn + fp + fn)\n",
    "        else: \n",
    "            accuracy = 0\n",
    "        if (tp + fp) != 0:\n",
    "            precision = tp / (tp + fp)\n",
    "        else:\n",
    "            precision = 0\n",
    "        if (tp + fn) != 0:\n",
    "            recall = tp / (tp + fn)\n",
    "        else:\n",
    "            recall = 0\n",
    "        if (precision + recall) != 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        df.loc[(experiment, model), '(hybrid)accuracy'] = accuracy\n",
    "        df.loc[(experiment, model), '(hybrid)precision'] = precision\n",
    "        df.loc[(experiment, model), '(hybrid)recall'] = recall\n",
    "        df.loc[(experiment, model), '(hybrid)f1'] = f1\n",
    "        # print('Experiment: {}, model: {}, accuracy: {}, precision: {}, recall: {}, f1: {}'.format(experiment, model, accuracy, precision, recall, f1))\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0 \n",
    "# Regressor experiments.\n",
    "for experiment in regressor_experiments:\n",
    "    for model in testing_data[experiment].keys():\n",
    "        test_data = testing_data[experiment][model]['real']\n",
    "        threshold = test_data.loc[:, test_data.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "        hybrid_metrics = metrics.Metrics()\n",
    "        hybrid_metrics.get_prediction_scores(testing_data[experiment][model]['predicted'], testing_data[experiment][model]['real'], threshold=threshold)\n",
    "        df.loc[(experiment, model), 'tp'] = hybrid_metrics.true_positives_ctr\n",
    "        df.loc[(experiment, model), 'tn'] = hybrid_metrics.true_negatives_ctr\n",
    "        df.loc[(experiment, model), 'fp'] = hybrid_metrics.false_positives_ctr\n",
    "        df.loc[(experiment, model), 'fn'] = hybrid_metrics.false_negatives_ctr\n",
    "        df.loc[(experiment, model), '(hybrid)accuracy'] = hybrid_metrics.hybrid_accuracy\n",
    "        df.loc[(experiment, model), '(hybrid)precision'] = hybrid_metrics.hybrid_precision\n",
    "        df.loc[(experiment, model), '(hybrid)recall'] = hybrid_metrics.hybrid_recall\n",
    "        df.loc[(experiment, model), '(hybrid)f1'] = hybrid_metrics.hybrid_f1\n",
    "        # print('Experiment: {}, model: {}, tp: {}, tn: {}, fp: {}, fn: {}'.format(experiment, model, hybrid_metrics.true_positives_ctr, hybrid_metrics.true_negatives_ctr, hybrid_metrics.false_positives_ctr, hybrid_metrics.false_negatives_ctr))\n",
    "        # print('Experiment: {}, model: {}, accuracy: {}, precision: {}, recall: {}, f1: {}'.format(experiment, model, hybrid_metrics.hybrid_accuracy, hybrid_metrics.hybrid_precision, hybrid_metrics.hybrid_recall, hybrid_metrics.hybrid_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
