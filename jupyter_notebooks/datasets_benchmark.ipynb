{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of this Article** \n",
    "- Loading best hyperparameters for each model\n",
    "- Model training\n",
    "- Model evaluation\n",
    "- Results discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading best hyperparameters for each model\n",
    "\n",
    "As explained in another notebook, the hyperparameters for each model were tunnned using the Optuna library. For each dataset and model, the hyperparameters have different values. The values for each hyperparameters are seen bellow.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hyperparameters dataset.\n",
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse hyper params:\n",
      "\n",
      "params_gradient_boost_regression_sparse.csv :\n",
      "            params                 value\n",
      "0   n_estimators                   727\n",
      "1  learning_rate     0.347390412877283\n",
      "2           loss         squared_error\n",
      "3          value  0.006486628478827696\n",
      "params_support_vector_regression_sparse.csv :\n",
      "     params                  value\n",
      "0  kernel                    rbf\n",
      "1       C   0.013225080938087181\n",
      "2  degree                      5\n",
      "3   gamma  7.229703411231879e-07\n",
      "4   value   0.005822757995746922\n",
      "params_xgboost_regression_sparse.csv :\n",
      "               params                   value\n",
      "0           booster                gblinear\n",
      "1            lambda  1.4617144209122512e-06\n",
      "2             alpha     0.09865530031009674\n",
      "3         subsample      0.3609716863271228\n",
      "4  colsample_bytree      0.6484747255416106\n",
      "5             value                     0.0\n",
      "Focused hyper params:\n",
      "\n",
      "params_gradient_boost_regression_focused.csv :\n",
      "           params                 value\n",
      "0   n_estimators                    66\n",
      "1  learning_rate   0.13944170214882864\n",
      "2           loss         squared_error\n",
      "3          value  0.004532892486597281\n",
      "params_support_vector_regression_focused.csv :\n",
      "    params                  value\n",
      "0  kernel                    rbf\n",
      "1       C     0.0469010189304341\n",
      "2  degree                      2\n",
      "3   gamma  3.891841123060806e-05\n",
      "4   value   0.005797602032122254\n",
      "params_xgboost_regression_focused.csv :\n",
      "               params                  value\n",
      "0            booster                 gbtree\n",
      "1             lambda  1.238290111033862e-08\n",
      "2              alpha  3.360146445675761e-08\n",
      "3          subsample     0.5365354012863757\n",
      "4   colsample_bytree     0.9295022899601696\n",
      "5          max_depth                      9\n",
      "6   min_child_weight                      6\n",
      "7                eta     0.0547120964136874\n",
      "8              gamma   0.017180352963513716\n",
      "9        grow_policy              lossguide\n",
      "10             value   0.004860134010203519\n",
      "Boolean hyper params:\n",
      "\n",
      "params_gradient_boost_classifier_sparse.csv :\n",
      "           params               value\n",
      "0   n_estimators                  76\n",
      "1  learning_rate  0.2991514046919511\n",
      "2           loss         exponential\n",
      "3          value  0.6268174474959612\n",
      "params_xgboost_classifier_sparse.csv :\n",
      "           params                value\n",
      "0           loss          exponential\n",
      "1  learning_rate   0.4534096237480201\n",
      "2   n_estimators                   94\n",
      "3      subsample  0.22652303068379911\n",
      "4      max_depth                    1\n",
      "5          value  0.43399244212614135\n"
     ]
    }
   ],
   "source": [
    "sparse_hyper_params = {}\n",
    "focused_hyper_params = {}\n",
    "boolean_hyper_params = {}\n",
    "for file in os.listdir('hyper_params_results'):\n",
    "    if file.endswith('.csv') and 'sparse.csv' in file.split('_') and 'classifier' not in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'focused.csv' in file.split('_') and 'classifier' not in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        focused_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'classifier' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        boolean_hyper_params[file] = df\n",
    "print('Sparse hyper params:\\n')\n",
    "for key in sparse_hyper_params.keys():\n",
    "    print(key, ':\\n ',sparse_hyper_params[key])\n",
    "print('Focused hyper params:\\n')\n",
    "for key in focused_hyper_params.keys():\n",
    "    print(key, ':\\n',focused_hyper_params[key])\n",
    "print('Boolean hyper params:\\n')\n",
    "for key in boolean_hyper_params.keys():\n",
    "    print(key, ':\\n',boolean_hyper_params[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyper_params_from_df(df):\n",
    "    output = {}\n",
    "    for row in df.iterrows():\n",
    "        output[row[1]['params']] = row[1]['value']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from thesis_package import aimodels as my_ai, utils, metrics\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "# Regression data sparse\n",
    "y_max_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_max_u_sparse)\n",
    "data_max_u_sparse = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_max_u_sparse, scaling=True)\n",
    "data_max_u_scaled_sparse = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_min_u_sparse)\n",
    "data_min_u_sparse = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_min_u_sparse, scaling=True)\n",
    "data_min_u_scaled_sparse = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "\n",
    "# Regresison data focused\n",
    "y_max_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bal_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_bal_constr.csv').drop(columns=['timestamps'])\n",
    "\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_max_u_focused)\n",
    "data_max_u_focused = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_max_u_focused, scaling=True)\n",
    "data_max_u_scaled_focused = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_min_u_focused)\n",
    "data_min_u_focused = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_min_u_focused, scaling=True)\n",
    "data_min_u_scaled_focused = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "\n",
    "# Classification data\n",
    "y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_bool_constr.csv').drop(columns=['timestamps'])\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_max_u)\n",
    "data_max_u_bool = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_max_u, scaling=True)\n",
    "data_max_u_bool_scaled = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_min_u)\n",
    "data_min_u_bool = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}\n",
    "train_x, test_x, train_y, test_y = utils.split_and_suffle(exogenous_data, y_min_u, scaling=True)\n",
    "data_min_u_bool_scaled = {'X_train': train_x, 'X_test': test_x, 'y_train': train_y, 'y_test': test_y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "In this section the models will be trained with the hyperparameters loaded above. All the models will be stored in the same `Context` object for later evaluation. The `Context` object is a class that stores all the models and their respective hyperparameters. The `Context` object is defined in the `aimodels.py` file. The `Context` object is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_u regression sparse\n",
    "if 'max_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    # Linear Regression\n",
    "    regressor_max_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = {}\n",
    "    regressor_max_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = {}\n",
    "    regressor_max_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = {}\n",
    "    regressor_max_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_scaled_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_spare', regressor_max_u)\n",
    "else: \n",
    "    regressor_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\regressor_max_u_sparse')\n",
    "# Linear Regression\n",
    "prediction_lr_max_u = regressor_max_u.strategies[0].predict(data=data_max_u_sparse)\n",
    "prediction_lr_max_u = pd.DataFrame(prediction_lr_max_u , columns=y_max_u.columns)\n",
    "# Gradient Boost Regression\n",
    "prediction_gb_max_u =  regressor_max_u.strategies[1].predict(data=data_max_u_sparse)\n",
    "prediction_gb_max_u = pd.DataFrame(prediction_gb_max_u, columns=y_max_u.columns)\n",
    "# Extreme GBoost Regression\n",
    "prediction_xgb_max_u =  regressor_max_u.strategies[2].predict(data=data_max_u_sparse)\n",
    "prediction_xgb_max_u = pd.DataFrame(prediction_xgb_max_u, columns=y_max_u.columns)\n",
    "# Support Vector Regression\n",
    "prediction_svr_max_u =  regressor_max_u.strategies[3].predict(data=data_max_u_scaled_sparse)\n",
    "prediction_svr_max_u = pd.DataFrame(prediction_svr_max_u, columns=y_max_u.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_u classification\n",
    "if 'max_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark\\dataset_benchmark'):\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = {}\n",
    "    classifier_max_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = {}\n",
    "    classifier_max_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = {}\n",
    "    classifier_max_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool_scaled)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_classifier', classifier_max_u)\n",
    "else: \n",
    "    classifier_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\classifier_max_u')\n",
    "# Gradient Boost Classifier\n",
    "prediction_gb_max_u = classifier_max_u.strategies[0].predict(data=data_max_u_bool)\n",
    "prediction_gb_max_u = pd.DataFrame(prediction_gb_max_u, columns=y_max_u.columns)\n",
    "# Extreme GBoost Classifier\n",
    "prediction_xgb_max_u = classifier_max_u.strategies[1].predict(data=data_max_u_bool)\n",
    "prediction_xgb_max_u = pd.DataFrame(prediction_xgb_max_u, columns=y_max_u.columns)\n",
    "# Support Vector Classifier\n",
    "prediction_svr_max_u = classifier_max_u.strategies[2].predict(data=data_max_u_bool_scaled)\n",
    "prediction_svr_max_u = pd.DataFrame(prediction_svr_max_u, columns=y_max_u.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min u regression training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_u regression sparse\n",
    "if 'min_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    # Linear Regression\n",
    "    regressor_min_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = {}\n",
    "    regressor_min_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = {}\n",
    "    regressor_min_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = {}\n",
    "    regressor_min_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_scaled_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse', regressor_min_u)\n",
    "else:\n",
    "    regressor_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse')\n",
    "# Linear Regression\n",
    "prediction_lr_min_u = regressor_min_u.strategies[0].predict(data=data_min_u_sparse)\n",
    "prediction_lr_min_u = pd.DataFrame(prediction_lr_min_u , columns=y_min_u.columns)\n",
    "# Gradient Boost Regression\n",
    "prediction_gb_min_u =  regressor_min_u.strategies[1].predict(data=data_min_u_sparse)\n",
    "prediction_gb_min_u = pd.DataFrame(prediction_gb_min_u, columns=y_min_u.columns)\n",
    "# Extreme GBoost Regression\n",
    "prediction_xgb_min_u =  regressor_min_u.strategies[2].predict(data=data_min_u_sparse)\n",
    "prediction_xgb_min_u = pd.DataFrame(prediction_xgb_min_u, columns=y_min_u.columns)\n",
    "# Support Vector Regression\n",
    "prediction_svr_min_u =  regressor_min_u.strategies[3].predict(data=data_min_u_scaled_sparse)\n",
    "prediction_svr_min_u = pd.DataFrame(prediction_svr_min_u, columns=y_min_u.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# min_u classification\n",
    "if 'min_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = {}\n",
    "    classifier_min_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = {}\n",
    "    classifier_min_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = {}\n",
    "    classifier_min_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool_scaled)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_classifier', classifier_min_u)\n",
    "else: \n",
    "    classifier_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\classifier_min_u')\n",
    "# Gradient Boost Classifier\n",
    "prediction_gb_min_u = classifier_min_u.strategies[0].predict(data=data_min_u_bool)\n",
    "prediction_gb_min_u = pd.DataFrame(prediction_gb_min_u, columns=y_min_u.columns)\n",
    "# Extreme GBoost Classifier\n",
    "prediction_xgb_min_u = classifier_min_u.strategies[1].predict(data=data_min_u_bool)\n",
    "prediction_xgb_min_u = pd.DataFrame(prediction_xgb_min_u, columns=y_min_u.columns)\n",
    "# Support Vector Classifier \n",
    "prediction_svr_min_u = classifier_min_u.strategies[2].predict(data=data_min_u_bool_scaled)\n",
    "prediction_svr_min_u = pd.DataFrame(prediction_svr_min_u, columns=y_min_u.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
