{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of this Article** \n",
    "- Loading best hyperparameters for each model\n",
    "- Model training\n",
    "- Results discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading best hyperparameters for each model\n",
    "\n",
    "TODO... explain this model bench mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hyperparameters dataset.\n",
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse hyperparameters: 8/8\n",
      "Focused hyperparameters: 8/8\n",
      "Balanced hyperparameters: 8/8\n",
      "Filtered hyperparameters: 8/8\n",
      "Sparse classifier hyperparameters: 8/8\n",
      "Balanced classifier hyperparameters: 8/8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparse_hyper_params = {}\n",
    "focused_hyper_params = {}\n",
    "balanced_hyper_params = {}\n",
    "filtered_hyper_params = {}\n",
    "sparse_class_hyper_params = {}\n",
    "balanced_class_hyper_params = {}\n",
    "for file in os.listdir('hyper_params_results'):\n",
    "    if file.endswith('.csv') and 'regression_sparse' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'regression_focused' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        focused_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'regression_balanced' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        balanced_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'filtered' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        filtered_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'sparse_classifier' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_class_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'balanced_classifier' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        balanced_class_hyper_params[file] = df\n",
    "print('Sparse hyperparameters: {}/8'.format(len(sparse_hyper_params)))\n",
    "print('Focused hyperparameters: {}/8'.format(len(focused_hyper_params)))\n",
    "print('Balanced hyperparameters: {}/8'.format(len(balanced_hyper_params)))\n",
    "print('Filtered hyperparameters: {}/8'.format(len(filtered_hyper_params)))\n",
    "print('Sparse classifier hyperparameters: {}/8'.format(len(sparse_class_hyper_params)))\n",
    "print('Balanced classifier hyperparameters: {}/8'.format(len(balanced_class_hyper_params)))\n",
    "print('\\n')\n",
    "# print('Sparse hyper params:\\n')\n",
    "# for key in sparse_hyper_params.keys():\n",
    "#     print(key, ':\\n ',sparse_hyper_params[key])\n",
    "# print('Focused hyper params:\\n')\n",
    "# for key in focused_hyper_params.keys():\n",
    "#     print(key, ':\\n',focused_hyper_params[key])\n",
    "# print('Boolean hyper params:\\n')\n",
    "# for key in sparse_class_hyper_params.keys():\n",
    "#     print(key, ':\\n',sparse_class_hyper_params[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_size': 34,\n",
       " 'n_layers': 3,\n",
       " 'dropout': 0.0030412321477918842,\n",
       " 'activation': 'relu',\n",
       " 'optimizer': 'sgd',\n",
       " 'lr': 9.741292351005151e-05,\n",
       " 'epochs': 55,\n",
       " 'batch_size': 8,\n",
       " 'classifier': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "def get_hyper_params_from_df(df):\n",
    "    output = {}\n",
    "    for row in df.iterrows():\n",
    "        if row[1]['params'] != 'value':\n",
    "            try:\n",
    "                output[row[1]['params']] = ast.literal_eval(row[1]['value'])\n",
    "            except :\n",
    "                output[row[1]['params']] = row[1]['value']\n",
    "    return output\n",
    "get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_max_u.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..');from thesis_package import aimodels as my_ai, utils, metrics\n",
    "from copy import deepcopy\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data sparse\n",
    "y_max_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse, test_size=0.2, scaling=True)\n",
    "data_max_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse, test_size=0.2, scaling=True)\n",
    "data_min_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data sparse\n",
    "y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_max_u_bool = y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)]\n",
    "y_min_u_bool = y_min_u_bool[utils.cols_with_positive_values(y_min_u_bool)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_bool, test_size=0.2, scaling=True)\n",
    "data_max_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_bool, test_size=0.2, scaling=True)\n",
    "data_min_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered data\n",
    "y_max_u_filtered = deepcopy(y_max_u_sparse[utils.cols_with_positive_values(y_max_u_bool)])\n",
    "y_min_u_filtered = deepcopy(y_min_u_sparse[utils.cols_with_positive_values(y_min_u_bool)])\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_filtered, test_size=0.2, scaling=True)\n",
    "data_max_u_filtered = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_filtered, test_size=0.2, scaling=True)\n",
    "data_min_u_filtered = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification data size:  (9044, 11)\n",
      "Regression data size:  (9044, 11)\n",
      "Positive in classification data:  4949.0\n",
      "Positive in regression data:  4949\n",
      "Theshhold:  0.0016733255333549746\n",
      "\n",
      "\n",
      "Classification data size:  (9044, 10)\n",
      "Regression data size:  (9044, 10)\n",
      "Positive in classification data:  6022.0\n",
      "Positive in regression data:  6022\n",
      "Theshhold:  0.002022118621573741\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the classiciation testing data and the filtered testing data\n",
    "print('Classification data size: ', data_max_u_bool['y_test'].shape)\n",
    "print('Regression data size: ', data_max_u_filtered['y_test'].shape)\n",
    "print('Positive in classification data: ', utils.count_positives_class(data_max_u_bool['y_test']))\n",
    "#unscaled_y_test = pd.DataFrame(data_max_u_filtered['scaler']['y'].inverse_transform(data_max_u_filtered['y_test']), columns=data_max_u_filtered['y_test'].columns)\n",
    "unscaled_y_test = data_max_u_filtered['y_test'] * data_max_u_sparse['scaler']['y']\n",
    "print('Positive in regression data: ', utils.count_positives_reg(unscaled_y_test, utils.compute_threshold(y_max_u_sparse)))\n",
    "print('Theshhold: ', utils.compute_threshold(y_max_u_sparse))\n",
    "# Same for min_u\n",
    "print('\\n')\n",
    "print('Classification data size: ', data_min_u_bool['y_test'].shape)\n",
    "print('Regression data size: ', data_min_u_filtered['y_test'].shape)\n",
    "print('Positive in classification data: ', utils.count_positives_class(data_min_u_bool['y_test']))\n",
    "#unscaled_y_test = pd.DataFrame(data_min_u_filtered['scaler']['y'].inverse_transform(data_min_u_filtered['y_test']), columns=data_min_u_filtered['y_test'].columns)\n",
    "unscaled_y_test = data_min_u_filtered['y_test'] * data_min_u_sparse['scaler']['y']\n",
    "print('Positive in regression data: ', utils.count_positives_reg(unscaled_y_test, utils.compute_threshold(y_min_u_sparse)))\n",
    "print('Theshhold: ', utils.compute_threshold(y_min_u_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data focused\n",
    "y_max_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_focused_constr.csv')\n",
    "exogenous_data_focused_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_focused.csv').drop(columns=['date'])\n",
    "y_min_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_focused_constr.csv')\n",
    "exogenous_data_focused_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_focused.csv').drop(columns=['date'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_focused_max_u, y_max_u_focused, test_size=0.2, scaling=True)\n",
    "data_max_u_focused = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_focused_min_u, y_min_u_focused, test_size=0.2, scaling=True)\n",
    "data_min_u_focused = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data balanced\n",
    "y_max_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced, test_size=0.2, scaling=True)\n",
    "data_max_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced, test_size=0.2, scaling=True)\n",
    "data_min_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data balanced\n",
    "y_max_u_balanced_class = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_bool_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced_class = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_bool_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "y_max_u_balanced_class = y_max_u_balanced_class[utils.cols_with_positive_values(y_max_u_balanced_class)]\n",
    "y_min_u_balanced_class = y_min_u_balanced_class[utils.cols_with_positive_values(y_min_u_balanced_class)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced_class, test_size=0.2, scaling=True)\n",
    "data_max_u_bool_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced_class, test_size=0.2, scaling=True)\n",
    "data_min_u_bool_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count in classification data max_u : 4949.0\n",
      "Positive count in regression data max_u with threshold 0.0016733255333549746 : 4949\n",
      "\n",
      "\n",
      "Positive count in classification data min_u : 6022.0\n",
      "Positive count in regression data min_u with threshold 0.002022118621573741 : 6022\n",
      "\n",
      "\n",
      "Negative count in classification data max_u : 94535.0\n",
      "Negative count in regression data max_u with threshold 0.0016733255333549746 : 94535\n",
      "\n",
      "\n",
      "Negative count in classification data min_u : 84418.0\n",
      "Negative count in regression data min_u with threshold 0.002022118621573741 : 84418\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.check_positive_count(data_max_u_filtered['y_test']* data_max_u_filtered['scaler']['y'], data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')\n",
    "utils.check_positive_count(data_min_u_filtered['y_test']* data_min_u_filtered['scaler']['y'], data_min_u_bool['y_test'], utils.compute_threshold(y_min_u_sparse), experiment='min_u')\n",
    "utils.check_negative_count(data_max_u_filtered['y_test']* data_max_u_filtered['scaler']['y'], data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')\n",
    "utils.check_negative_count(data_min_u_filtered['y_test']* data_min_u_filtered['scaler']['y'], data_min_u_bool['y_test'], utils.compute_threshold(y_min_u_sparse), experiment='min_u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count in classification data max_u : 4949.0\n",
      "Positive count in regression data max_u with threshold 0.0016733255333549746 : 4949\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.check_positive_count(data_max_u_filtered['y_test'] * data_max_u_filtered['scaler']['y'], data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "In this section the models will be trained with the hyperparameters loaded above. All the models will be stored in the same `Context` object for later evaluation. The `Context` object is a class that stores all the models and their respective hyperparameters. The `Context` object is defined in the `aimodels.py` file. The `Context` object is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_models = ['lr', 'gb', 'xgb', 'svr', 'mlp']\n",
    "class_models =  ['gb', 'xgb', 'svr', 'mlp']\n",
    "max_u_threshold = utils.compute_threshold(y_max_u_sparse)\n",
    "min_u_threshold = utils.compute_threshold(y_min_u_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_filtered(df, scaler):\n",
    "    for bus in df.columns:\n",
    "        idx = list(scaler.feature_names_in_).index(bus)\n",
    "        df[bus] = scaler.max_abs_[idx] * df[bus]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['params_gradient_boost_regression_sparse_max_u.csv', 'params_gradient_boost_regression_sparse_min_u.csv', 'params_mlp_regression_sparse_max_u.csv', 'params_mlp_regression_sparse_min_u.csv', 'params_support_vector_regression_sparse_max_u.csv', 'params_support_vector_regression_sparse_min_u.csv', 'params_xgboost_regression_sparse_max_u.csv', 'params_xgboost_regression_sparse_min_u.csv'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_hyper_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression sparse\n"
     ]
    }
   ],
   "source": [
    "# max_u regression sparse\n",
    "if 'max_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression sparse')\n",
    "    # Linear Regression\n",
    "    regressor_max_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_max_u.csv'])\n",
    "    regressor_max_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_xgboost_regression_sparse_max_u.csv']) \n",
    "    regressor_max_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_support_vector_regression_sparse_max_u.csv'])\n",
    "    regressor_max_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_mlp_regression_sparse_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_sparse['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_sparse['y_train'].shape[1]\n",
    "    regressor_max_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_sparse', regressor_max_u)\n",
    "else:\n",
    "    print('Loading max_u regression sparse') \n",
    "    regressor_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_regressor_sparse')\n",
    "\n",
    "testing_data = {'max_u_regressor_sparse': {}}\n",
    "for model, strategy in zip(reg_models, regressor_max_u.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_sparse['y_test'].columns)\n",
    "    testing_data['max_u_regressor_sparse'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_sparse'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_sparse'][model]['real'] = deepcopy(data_max_u_sparse['y_test'].reset_index(drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ext_grid</th>\n",
       "      <th>bus_1</th>\n",
       "      <th>bus_2</th>\n",
       "      <th>bus_3</th>\n",
       "      <th>bus_4</th>\n",
       "      <th>bus_5</th>\n",
       "      <th>bus_6</th>\n",
       "      <th>bus_7</th>\n",
       "      <th>bus_8</th>\n",
       "      <th>bus_9</th>\n",
       "      <th>...</th>\n",
       "      <th>bus_30</th>\n",
       "      <th>bus_31</th>\n",
       "      <th>bus_17</th>\n",
       "      <th>bus_21</th>\n",
       "      <th>bus_24</th>\n",
       "      <th>bus_18</th>\n",
       "      <th>bus_23</th>\n",
       "      <th>bus_27</th>\n",
       "      <th>bus_32</th>\n",
       "      <th>bus_33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778937</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708369</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124318</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682806</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.653279</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9039</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.837245</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9040</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.844286</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9041</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494064</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9042</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554783</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9043</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.712643</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9044 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ext_grid  bus_1  bus_2  bus_3  bus_4  bus_5  bus_6     bus_7  bus_8  \\\n",
       "0          0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    1.0   \n",
       "1          0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    1.0   \n",
       "2          0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.124318    1.0   \n",
       "3          0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    1.0   \n",
       "4          0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    1.0   \n",
       "...        ...    ...    ...    ...    ...    ...    ...       ...    ...   \n",
       "9039       0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    1.0   \n",
       "9040       0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.027256    1.0   \n",
       "9041       0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.140394    1.0   \n",
       "9042       0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    1.0   \n",
       "9043       0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    1.0   \n",
       "\n",
       "      bus_9  ...  bus_30  bus_31  bus_17  bus_21  bus_24  bus_18  bus_23  \\\n",
       "0       1.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "1       1.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "2       1.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "3       1.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "4       1.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "...     ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9039    1.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "9040    1.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "9041    1.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "9042    1.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "9043    1.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "\n",
       "      bus_27    bus_32  bus_33  \n",
       "0        0.0  0.778937     0.0  \n",
       "1        0.0  0.708369     0.0  \n",
       "2        0.0  0.682806     0.0  \n",
       "3        0.0  0.549579     0.0  \n",
       "4        0.0  0.653279     0.0  \n",
       "...      ...       ...     ...  \n",
       "9039     0.0  0.837245     0.0  \n",
       "9040     0.0  0.844286     0.0  \n",
       "9041     0.0  0.494064     0.0  \n",
       "9042     0.0  0.554783     0.0  \n",
       "9043     0.0  0.712643     0.0  \n",
       "\n",
       "[9044 rows x 34 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# figure size of the plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 8))\n",
    "testing_data['max_u_regressor_sparse']['mlp']['predicted']['bus_1'].plot()\n",
    "testing_data['max_u_regressor_sparse']['mlp']['real']['bus_1'].plot()\n",
    "# plot a line with the threshold\n",
    "# Add legend\n",
    "plt.legend(['predicted', 'real'])\n",
    "testing_data['max_u_regressor_sparse']['mlp']['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives_ctr:  5534\n",
      "true_negatives_ctr:  202642\n",
      "false_positives_ctr:  1\n",
      "false_negatives_ctr:  99319\n",
      "35512475510025640665\n"
     ]
    }
   ],
   "source": [
    "metric = metrics.Metrics()\n",
    "metric.get_prediction_scores(testing_data['max_u_regressor_sparse']['mlp']['real'], testing_data['max_u_regressor_sparse']['mlp']['predicted'], max_u_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beepy import beep\n",
    "beep(sound=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression focused\n"
     ]
    }
   ],
   "source": [
    "# max_u regression focused\n",
    "if 'max_u_regressor_focused.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression focused')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_focused = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_gradient_boost_regression_focused_max_u.csv'])\n",
    "    regressor_max_u_focused.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_xgboost_regression_focused_max_u.csv']) \n",
    "    regressor_max_u_focused.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_support_vector_regression_focused_max_u.csv'])\n",
    "    regressor_max_u_focused.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_focused['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_focused['y_train'].shape[1]\n",
    "    regressor_max_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_focused', regressor_max_u_focused)\n",
    "else: \n",
    "    print('Loading max_u regression focused')\n",
    "    regressor_max_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\\\max_u_regressor_focused')\n",
    "\n",
    "testing_data['max_u_regressor_focused'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_focused.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    testing_data['max_u_regressor_focused'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_focused'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_focused'][model]['real'] = deepcopy(data_max_u_sparse['y_test'].reset_index(drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training max_u regression filtered\n"
     ]
    }
   ],
   "source": [
    "# max_u regression filtered\n",
    "if 'max_u_filtered_regressor.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression filtered')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_filtered = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_gradient_boost_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_xgboost_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_support_vector_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_mlp_regression_filtered_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_filtered['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_filtered['y_train'].shape[1]\n",
    "    regressor_max_u_filtered.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_filtered_regressor', regressor_max_u_filtered)\n",
    "else: \n",
    "    print('Loading max_u filtered regression')\n",
    "    regressor_max_u_filtered = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_filtered_regressor')\n",
    "\n",
    "testing_data['max_u_filtered_regressor'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_filtered.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_filtered['y_test'].columns)\n",
    "    testing_data['max_u_filtered_regressor'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_filtered_regressor'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_filtered_regressor'][model]['real'] = deepcopy(data_max_u_sparse['y_test'][utils.cols_with_positive_values(prediction)].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9044, 11)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy = regressor_max_u_filtered.strategies[0]\n",
    "prediction = strategy.predict(data=data_max_u_sparse)\n",
    "prediction.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression balanced\n"
     ]
    }
   ],
   "source": [
    "# max u regression balanced\n",
    "if 'max_u_regressor_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression balanced')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_balanced = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_gradient_boost_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_xgboost_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_support_vector_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_mlp_regression_balanced_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_balanced['y_train'].shape[1]\n",
    "    regressor_max_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_balanced', regressor_max_u_balanced)\n",
    "else: \n",
    "    print('Loading max_u regression balanced')\n",
    "    regressor_max_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_regressor_balanced')\n",
    "\n",
    "testing_data['max_u_regressor_balanced'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    testing_data['max_u_regressor_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_balanced'][model]['real'] = deepcopy(data_max_u_sparse['y_test'].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u classification\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (9044, 10), indices imply (9044, 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\datasets_benchmark.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X40sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m testing_data[\u001b[39m'\u001b[39m\u001b[39mmax_u_classifier\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X40sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m model, strategy \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(class_models, classifier_max_u\u001b[39m.\u001b[39mstrategies):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X40sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     prediction \u001b[39m=\u001b[39m strategy\u001b[39m.\u001b[39;49mpredict(data\u001b[39m=\u001b[39;49mdata_max_u_bool)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X40sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     testing_data[\u001b[39m'\u001b[39m\u001b[39mmax_u_classifier\u001b[39m\u001b[39m'\u001b[39m][model] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mreal\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mpredicted\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X40sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     testing_data[\u001b[39m'\u001b[39m\u001b[39mmax_u_classifier\u001b[39m\u001b[39m'\u001b[39m][model][\u001b[39m'\u001b[39m\u001b[39mpredicted\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m deepcopy(prediction)\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\..\\thesis_package\\aimodels.py:122\u001b[0m, in \u001b[0;36mGradientBoostClassifierStrategy.predict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, data: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39;49mDataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(data[\u001b[39m'\u001b[39;49m\u001b[39mX_test\u001b[39;49m\u001b[39m'\u001b[39;49m]), columns\u001b[39m=\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39my_test\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    685\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    686\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    692\u001b[0m         )\n\u001b[0;32m    693\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    695\u001b[0m             data,\n\u001b[0;32m    696\u001b[0m             index,\n\u001b[0;32m    697\u001b[0m             columns,\n\u001b[0;32m    698\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    699\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    700\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    701\u001b[0m         )\n\u001b[0;32m    703\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\internals\\construction.py:351\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[39m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    347\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[0;32m    348\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[0;32m    349\u001b[0m )\n\u001b[1;32m--> 351\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    355\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\internals\\construction.py:422\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    420\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[0;32m    421\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[1;32m--> 422\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (9044, 10), indices imply (9044, 11)"
     ]
    }
   ],
   "source": [
    "# max_u classification\n",
    "if 'max_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u classification')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_gradient_boost_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_xgboost_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_support_vector_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_mlp_sparse_classifier_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_bool['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_bool['y_train'].shape[1]\n",
    "    classifier_max_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_classifier', classifier_max_u)\n",
    "else: \n",
    "    print('Loading max_u classification')\n",
    "    classifier_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_classifier')\n",
    "\n",
    "testing_data['max_u_classifier'] = {}\n",
    "for model, strategy in zip(class_models, classifier_max_u.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_bool)\n",
    "    testing_data['max_u_classifier'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_classifier'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_classifier'][model]['real'] = deepcopy(data_max_u_bool['y_test'].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'xgb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\datasets_benchmark.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m testing_data[\u001b[39m'\u001b[39;49m\u001b[39mmax_u_classifier\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mxgb\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mpredicted\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mbus_15\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mplot()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m testing_data[\u001b[39m'\u001b[39m\u001b[39mmax_u_classifier\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mxgb\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mreal\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mbus_15\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mplot()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'xgb'"
     ]
    }
   ],
   "source": [
    "testing_data['max_u_classifier']['xgb']['predicted']['bus_15'].plot()\n",
    "testing_data['max_u_classifier']['xgb']['real']['bus_15'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u classification balanced\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (9044, 10), indices imply (9044, 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\datasets_benchmark.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m testing_data[\u001b[39m'\u001b[39m\u001b[39mmax_u_classifier_balanced\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m model, strategy \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(class_models, classifier_max_u_balanced\u001b[39m.\u001b[39mstrategies):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     prediction \u001b[39m=\u001b[39m strategy\u001b[39m.\u001b[39;49mpredict(data\u001b[39m=\u001b[39;49mdata_max_u_bool)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X42sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     testing_data[\u001b[39m'\u001b[39m\u001b[39mmax_u_classifier_balanced\u001b[39m\u001b[39m'\u001b[39m][model] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mreal\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mpredicted\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X42sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     testing_data[\u001b[39m'\u001b[39m\u001b[39mmax_u_classifier_balanced\u001b[39m\u001b[39m'\u001b[39m][model][\u001b[39m'\u001b[39m\u001b[39mpredicted\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m deepcopy(prediction)\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\..\\thesis_package\\aimodels.py:122\u001b[0m, in \u001b[0;36mGradientBoostClassifierStrategy.predict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, data: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39;49mDataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(data[\u001b[39m'\u001b[39;49m\u001b[39mX_test\u001b[39;49m\u001b[39m'\u001b[39;49m]), columns\u001b[39m=\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39my_test\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    685\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    686\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    692\u001b[0m         )\n\u001b[0;32m    693\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    695\u001b[0m             data,\n\u001b[0;32m    696\u001b[0m             index,\n\u001b[0;32m    697\u001b[0m             columns,\n\u001b[0;32m    698\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    699\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    700\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    701\u001b[0m         )\n\u001b[0;32m    703\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\internals\\construction.py:351\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[39m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    347\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[0;32m    348\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[0;32m    349\u001b[0m )\n\u001b[1;32m--> 351\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    355\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\internals\\construction.py:422\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    420\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[0;32m    421\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[1;32m--> 422\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (9044, 10), indices imply (9044, 11)"
     ]
    }
   ],
   "source": [
    "# max_u classification balanced\n",
    "if 'max_u_classifier_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u classification balanced')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_gradient_boost_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_xgboost_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_support_vector_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_mlp_balanced_classifier_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_bool_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_bool_balanced['y_train'].shape[1]\n",
    "    classifier_max_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_classifier_balanced', classifier_max_u_balanced)\n",
    "else: \n",
    "    print('Loading max_u classification balanced')\n",
    "    classifier_max_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_classifier_balanced')\n",
    "\n",
    "testing_data['max_u_classifier_balanced'] = {}\n",
    "for model, strategy in zip(class_models, classifier_max_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_bool)\n",
    "    testing_data['max_u_classifier_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_classifier_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_classifier_balanced'][model]['real'] = deepcopy(data_max_u_bool['y_test'].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<thesis_package.aimodels.GradientBoostClassifierStrategy at 0x197f8061070>,\n",
       " <thesis_package.aimodels.XGBoostClassifierStrategy at 0x197f81299d0>,\n",
       " <thesis_package.aimodels.SupportVectorClassifierStrategy at 0x197f8133580>,\n",
       " <thesis_package.aimodels.MultilayerPerceptronStrategy at 0x197f813c5e0>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_max_u_balanced.strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min u regression training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression sparse\n"
     ]
    }
   ],
   "source": [
    "# min_u regression sparse\n",
    "if 'min_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression sparse')\n",
    "    # Linear Regression\n",
    "    regressor_min_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_xgboost_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_support_vector_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_mlp_regression_sparse_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_sparse['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_sparse['y_train'].shape[1]\n",
    "    regressor_min_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse', regressor_min_u)\n",
    "else:\n",
    "    print('Loading min_u regression sparse')\n",
    "    regressor_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse')\n",
    "\n",
    "testing_data['min_u_regressor_sparse'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    testing_data['min_u_regressor_sparse'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_sparse'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_sparse'][model]['real'] = deepcopy(data_min_u_sparse['y_test'].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression focused\n"
     ]
    }
   ],
   "source": [
    "# min_u regression focused\n",
    "if 'min_u_regressor_focused.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression focused')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_focused = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_gradient_boost_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_xgboost_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_support_vector_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_focused['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_focused['y_train'].shape[1]\n",
    "    regressor_min_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_focused', regressor_min_u_focused)\n",
    "else:\n",
    "    print('Loading min_u regression focused')\n",
    "    regressor_min_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_focused')\n",
    "\n",
    "testing_data['min_u_regressor_focused'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_focused.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    testing_data['min_u_regressor_focused'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_focused'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_focused'][model]['real'] = deepcopy(data_min_u_sparse['y_test'].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u filtered regression\n"
     ]
    }
   ],
   "source": [
    "# min u regression filtered\n",
    "if 'min_u_filtered_regressor.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression filtered')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_filtered = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_gradient_boost_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_xgboost_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_support_vector_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_mlp_regression_filtered_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_filtered['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_filtered['y_train'].shape[1]\n",
    "    regressor_min_u_filtered.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_filtered_regressor', regressor_min_u_filtered)\n",
    "else: \n",
    "    print('Loading min_u filtered regression')\n",
    "    regressor_min_u_filtered = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_filtered_regressor')\n",
    "\n",
    "testing_data['min_u_filtered_regressor'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_filtered.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_filtered['y_test'].columns)\n",
    "    testing_data['min_u_filtered_regressor'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_filtered_regressor'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_filtered_regressor'][model]['real'] = deepcopy(data_min_u_sparse['y_test'][utils.cols_with_positive_values(prediction)].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression balanced\n"
     ]
    }
   ],
   "source": [
    "# min u regression balanced\n",
    "if 'min_u_regressor_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression balanced')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_balanced = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_gradient_boost_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_xgboost_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_support_vector_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_mlp_regression_balanced_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_balanced['y_train'].shape[1]\n",
    "    regressor_min_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_balanced', regressor_min_u_balanced)\n",
    "else: \n",
    "    print('Loading min_u regression balanced')\n",
    "    regressor_min_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_balanced')\n",
    "\n",
    "testing_data['min_u_regressor_balanced'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    testing_data['min_u_regressor_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_balanced'][model]['real'] = deepcopy(data_min_u_sparse['y_test'].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u classification\n"
     ]
    }
   ],
   "source": [
    "# min_u classification\n",
    "if 'min_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u classification')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_gradient_boost_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_xgboost_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_support_vector_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_mlp_sparse_classifier_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_bool['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_bool['y_train'].shape[1]\n",
    "    classifier_min_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_classifier', classifier_min_u)\n",
    "else: \n",
    "    print('Loading min_u classification')\n",
    "    classifier_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_classifier')\n",
    "\n",
    "testing_data['min_u_classifier'] = {}\n",
    "for model, strategy in zip(class_models, classifier_min_u.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_bool)\n",
    "    testing_data['min_u_classifier'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_classifier'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_classifier'][model]['real'] = deepcopy(data_min_u_bool['y_test'].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<thesis_package.aimodels.GradientBoostClassifierStrategy at 0x197fb667d00>,\n",
       " <thesis_package.aimodels.XGBoostClassifierStrategy at 0x197f803f370>,\n",
       " <thesis_package.aimodels.SupportVectorClassifierStrategy at 0x197f8049f70>,\n",
       " <thesis_package.aimodels.MultilayerPerceptronStrategy at 0x197f806c5b0>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_min_u.strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u classification balanced\n"
     ]
    }
   ],
   "source": [
    "# min u classification balanced\n",
    "if 'min_u_classifier_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u classification balanced')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_gradient_boost_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_xgboost_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_support_vector_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_mlp_balanced_classifier_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_bool_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_bool_balanced['y_train'].shape[1]\n",
    "    classifier_min_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_classifier_balanced', classifier_min_u_balanced)\n",
    "else: \n",
    "    print('Loading min_u classification balanced')\n",
    "    classifier_min_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_classifier_balanced')\n",
    "\n",
    "testing_data['min_u_classifier_balanced'] = {}\n",
    "for model, strategy in zip(class_models, classifier_min_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_bool)\n",
    "    testing_data['min_u_classifier_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_classifier_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_classifier_balanced'][model]['real'] = deepcopy(data_min_u_bool['y_test'].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "In this section the results of the training and testing are presented and compared. The main objectives of this experience is to compare the performance of the regression models in terms of the hybrid metrics confusion matrix and the hybrid metrics rmse. The comparisons will be the following:\n",
    "- Compare the confusion matrices of the classification models and the regression models evaluate with the hybrid metrics.\n",
    "- Compare the error results of the regression models trained with the focused dataset and the sparse dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_u_regressor_sparse :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_regressor_focused :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_filtered_regressor :  dict_keys(['lr'])\n",
      "max_u_regressor_balanced :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_classifier :  dict_keys([])\n",
      "min_u_regressor_focused :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_regressor_balanced :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_classifier :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_classifier_balanced :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_classifier_balanced :  dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "for experience in testing_data.keys():\n",
    "    print(experience,': ', testing_data[experience].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing all models: Function that receives a dict with the real and predicted values, and outputs a dataframe with the results of the metrics.\n",
    "# Accumulate all the classifications for each bus.\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for bus in testing_data['max_u_regressor_sparse']['mlp']['predicted'].columns:\n",
    "    # Compute tp, tn, fp, fn\n",
    "    tp += sum((testing_data['max_u_regressor_sparse']['mlp']['predicted'][bus] == 1) & (testing_data['max_u_regressor_sparse']['mlp']['real'][bus] == 1))\n",
    "    tn += sum((testing_data['max_u_regressor_sparse']['mlp']['predicted'][bus] == 0) & (testing_data['max_u_regressor_sparse']['mlp']['real'][bus] == 0))\n",
    "    fp += sum((testing_data['max_u_regressor_sparse']['mlp']['predicted'][bus] == 1) & (testing_data['max_u_regressor_sparse']['mlp']['real'][bus] == 0))\n",
    "    fn += sum((testing_data['max_u_regressor_sparse']['mlp']['predicted'][bus] == 0) & (testing_data['max_u_regressor_sparse']['mlp']['real'][bus] == 1))\n",
    "print('{} + {} = {} = {} possible positive values.'.format(tp, fn, tp+fn, testing_data['max_u_regressor_sparse']['mlp']['real'].sum().sum()))\n",
    "print('{} + {} = {} = {} possible negative values.'.format(tn, fp, tn+fp, testing_data['max_u_regressor_sparse']['mlp']['real'].shape[0]*testing_data['max_u_regressor_sparse']['mlp']['real'].shape[1] - testing_data['max_u_regressor_sparse']['mlp']['real'].sum().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beepy import beep; beep(sound=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: max_u_regressor_sparse, model: lr, threshold: 0.05260425830867303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\datasets_benchmark.ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X61sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mExperiment: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, model: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, threshold: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(experiment, model, threshold))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X61sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m hybrid_metrics \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mMetrics()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X61sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m hybrid_metrics\u001b[39m.\u001b[39;49mget_prediction_scores(testing_data[experiment][model][\u001b[39m'\u001b[39;49m\u001b[39mpredicted\u001b[39;49m\u001b[39m'\u001b[39;49m], testing_data[experiment][model][\u001b[39m'\u001b[39;49m\u001b[39mreal\u001b[39;49m\u001b[39m'\u001b[39;49m], threshold\u001b[39m=\u001b[39;49mthreshold)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X61sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m df\u001b[39m.\u001b[39mloc[(experiment, model), \u001b[39m'\u001b[39m\u001b[39mtp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m hybrid_metrics\u001b[39m.\u001b[39mtrue_positives_ctr\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/datasets_benchmark.ipynb#X61sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m df\u001b[39m.\u001b[39mloc[(experiment, model), \u001b[39m'\u001b[39m\u001b[39mtn\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m hybrid_metrics\u001b[39m.\u001b[39mtrue_negatives_ctr\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\..\\thesis_package\\metrics.py:46\u001b[0m, in \u001b[0;36mMetrics.get_prediction_scores\u001b[1;34m(self, y_pred, y_test, threshold)\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfalse_positives_ctr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     45\u001b[0m             \u001b[39melse\u001b[39;00m: \u001b[39m#_y_pred.iloc[i, j] < 0:\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m                 true_negatives_sse \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _squared_error\u001b[39m.\u001b[39;49miloc[i, j]\n\u001b[0;32m     47\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrue_negatives_ctr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[39m# rmse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\indexing.py:960\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    958\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[0;32m    959\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m--> 960\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_get_value(\u001b[39m*\u001b[39;49mkey, takeable\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_takeable)\n\u001b[0;32m    961\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m    962\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\frame.py:3613\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3611\u001b[0m \u001b[39mif\u001b[39;00m takeable:\n\u001b[0;32m   3612\u001b[0m     series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ixs(col, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m-> 3613\u001b[0m     \u001b[39mreturn\u001b[39;00m series\u001b[39m.\u001b[39;49m_values[index]\n\u001b[0;32m   3615\u001b[0m series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_item_cache(col)\n\u001b[0;32m   3616\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_engine\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\series.py:719\u001b[0m, in \u001b[0;36mSeries._values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    688\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_values\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    689\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[39m    Return the internal repr of this data (defined by Block.interval_values).\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[39m    This are the values as stored in the Block (ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    717\u001b[0m \n\u001b[0;32m    718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 719\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49minternal_values()\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1850\u001b[0m, in \u001b[0;36mSingleBlockManager.internal_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1848\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minternal_values\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1849\u001b[0m     \u001b[39m\"\"\"The array that Series._values returns\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1850\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_block\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\_libs\\properties.pyx:37\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1794\u001b[0m, in \u001b[0;36mSingleBlockManager._block\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[39m@cache_readonly\u001b[39m\n\u001b[0;32m   1793\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_block\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Block:\n\u001b[1;32m-> 1794\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import sqrt \n",
    "# Build a multi-index dataframe with the results of the metrics. The first index is the testing_data.keys(), the second index are the tp, tn, fp, fn, and the columns are the models.\n",
    "columns = ['tp', 'tn', 'fp', 'fn', '(hybrid)accuracy', '(hybrid)precision', '(hybrid)recall', '(hybrid)f1']\n",
    "index = pd.MultiIndex.from_product([testing_data.keys(), ['lr', 'gb', 'xgb', 'svr', 'mlp']], names=['experiment', 'class'])\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "classifier_experiments =[experiment for experiment in testing_data.keys() if 'classifier' in experiment.split('_')] # TODO confirm this\n",
    "regressor_experiments = [experiment for experiment in testing_data.keys() if 'regressor' in experiment.split('_')]\n",
    "# Classifier experiments\n",
    "class_metrics = metrics.Metrics()\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for experiment in classifier_experiments:\n",
    "    for model in testing_data[experiment].keys():\n",
    "        for bus in testing_data[experiment][model]['predicted'].columns:\n",
    "            try:\n",
    "                tp += sum((testing_data[experiment][model]['predicted'][bus] == 1) & (testing_data[experiment][model]['real'][bus] == 1))\n",
    "                tn += sum((testing_data[experiment][model]['predicted'][bus] == 0) & (testing_data[experiment][model]['real'][bus] == 0))\n",
    "                fp += sum((testing_data[experiment][model]['predicted'][bus] == 1) & (testing_data[experiment][model]['real'][bus] == 0))\n",
    "                fn += sum((testing_data[experiment][model]['predicted'][bus] == 0) & (testing_data[experiment][model]['real'][bus] == 1))\n",
    "            except: \n",
    "                print('In the experiment ', experiment, ' and model ', model, ' there was a problem with bus: ', bus)\n",
    "                if not testing_data[experiment][model]['real'][bus].any():\n",
    "                    print('Bus {} has no positive data points. Just ignore the little shit.'.format(bus))    \n",
    "        df.loc[(experiment, model), 'tp'] = tp\n",
    "        df.loc[(experiment, model), 'tn'] = tn\n",
    "        df.loc[(experiment, model), 'fp'] = fp\n",
    "        df.loc[(experiment, model), 'fn'] = fn\n",
    "        #print('Experiment: {}, model: {}, tp: {}, tn: {}, fp: {}, fn: {}'.format(experiment, model, tp, tn, fp, fn))\n",
    "        recall = class_metrics.compute_recall(tp, fn)\n",
    "        precision = class_metrics.compute_precision(tp, fp)\n",
    "        f1 = class_metrics.compute_f1(recall, precision)\n",
    "        accuracy = class_metrics.compute_accuracy(tp, tn, fp, fn)\n",
    "        mcc = class_metrics.compute_mcc(tp, tn, fp, fn)\n",
    "        df.loc[(experiment, model), '(hybrid)accuracy'] = accuracy\n",
    "        df.loc[(experiment, model), '(hybrid)precision'] = precision\n",
    "        df.loc[(experiment, model), '(hybrid)recall'] = recall\n",
    "        df.loc[(experiment, model), '(hybrid)f1'] = f1\n",
    "        df.loc[(experiment, model), '(hybrid)mcc'] = mcc\n",
    "        # print('Experiment: {}, model: {}, accuracy: {}, precision: {}, recall: {}, f1: {}'.format(experiment, model, accuracy, precision, recall, f1))\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0 \n",
    "# Regressor experiments\n",
    "_threshold = lambda experiment: max_u_threshold / data_max_u_balanced['scaler']['y'] if 'max_u' in experiment else min_u_threshold/ data_min_u_balanced['scaler']['y']\n",
    "# _threshold = lambda experiment: max_u_threshold if 'max_u' in experiment else min_u_threshold\n",
    "for experiment in regressor_experiments:\n",
    "    for model in testing_data[experiment].keys():\n",
    "        # try:\n",
    "        threshold = _threshold(experiment)\n",
    "        print('Experiment: {}, model: {}, threshold: {}'.format(experiment, model, threshold))\n",
    "        hybrid_metrics = metrics.Metrics()\n",
    "        hybrid_metrics.get_prediction_scores(testing_data[experiment][model]['predicted'], testing_data[experiment][model]['real'], threshold=threshold)\n",
    "        df.loc[(experiment, model), 'tp'] = hybrid_metrics.true_positives_ctr\n",
    "        df.loc[(experiment, model), 'tn'] = hybrid_metrics.true_negatives_ctr\n",
    "        df.loc[(experiment, model), 'fp'] = hybrid_metrics.false_positives_ctr\n",
    "        df.loc[(experiment, model), 'fn'] = hybrid_metrics.false_negatives_ctr\n",
    "        df.loc[(experiment, model), '(hybrid)accuracy'] = hybrid_metrics.hybrid_accuracy_rmse\n",
    "        df.loc[(experiment, model), '(hybrid)precision'] = hybrid_metrics.hybrid_precision_rmse\n",
    "        df.loc[(experiment, model), '(hybrid)recall'] = hybrid_metrics.hybrid_recall_rmse\n",
    "        df.loc[(experiment, model), '(hybrid)f1'] = hybrid_metrics.hybrid_f1_rmse\n",
    "        df.loc[(experiment, model), '(hybrid)mcc'] = hybrid_metrics.hybrid_mcc_rmse\n",
    "        # print('Experiment: {}, model: {}, tp: {}, tn: {}, fp: {}, fn: {}'.format(experiment, model, hybrid_metrics.true_positives_ctr, hybrid_metrics.true_negatives_ctr, hybrid_metrics.false_positives_ctr, hybrid_metrics.false_negatives_ctr))\n",
    "        # print('Experiment: {}, model: {}, accuracy: {}, precision: {}, recall: {}, f1: {}'.format(experiment, model, hybrid_metrics.hybrid_accuracy_rmse, hybrid_metrics.hybrid_precision_rmse, hybrid_metrics.hybrid_recall_rmse, hybrid_metrics.hybrid_f1_rmse))\n",
    "        # except(Exception) as e:\n",
    "        #     print('In the experiment ', experiment, ' and model ', model, ' there was a problem')\n",
    "        #     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmation_df = pd.DataFrame()\n",
    "confirmation_df['possible_positives'] = df['tp'] + df['fn']\n",
    "confirmation_df['possible_negatives'] = df['fp'] + df['tn']\n",
    "confirmation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unscale everything\n",
    "testing_data['max_u_regressor_sparse'][model]['predicted'] = testing_data['max_u_regressor_sparse'][model]['predicted'] * data_max_u_sparse['scaler']['y']\n",
    "testing_data['max_u_regressor_sparse'][model]['real'] = testing_data['max_u_regressor_sparse'][model]['real'] * data_max_u_sparse['scaler']['y']\n",
    "testing_data['max_u_regressor_focused'][model]['predicted'] = testing_data['max_u_regressor_focused'][model]['predicted'] * data_max_u_sparse['scaler']['y']\n",
    "testing_data['max_u_regressor_focused'][model]['real'] = testing_data['max_u_regressor_focused'][model]['real'] * data_max_u_sparse['scaler']['y']\n",
    "testing_data['max_u_filtered_regressor'][model]['predicted'] = testing_data['max_u_filtered_regressor'][model]['predicted'] * data_max_u_sparse['scaler']['y']\n",
    "testing_data['max_u_filtered_regressor'][model]['real'] = testing_data['max_u_filtered_regressor'][model]['real'] * data_max_u_sparse['scaler']['y']\n",
    "testing_data['max_u_regressor_balanced'][model]['predicted'] = testing_data['max_u_regressor_balanced'][model]['predicted'] * data_max_u_balanced['scaler']['y']\n",
    "testing_data['max_u_regressor_balanced'][model]['real'] = testing_data['max_u_regressor_balanced'][model]['real'] * data_max_u_sparse['scaler']['y']\n",
    "testing_data['min_u_regressor_sparse'][model]['predicted'] = testing_data['min_u_regressor_sparse'][model]['predicted'] * data_min_u_sparse['scaler']['y']\n",
    "testing_data['min_u_regressor_sparse'][model]['real'] = testing_data['min_u_regressor_sparse'][model]['real'] * data_min_u_sparse['scaler']['y']\n",
    "testing_data['min_u_regressor_focused'][model]['predicted'] = testing_data['min_u_regressor_focused'][model]['predicted'] * data_min_u_focused['scaler']['y']\n",
    "testing_data['min_u_regressor_focused'][model]['real'] = testing_data['min_u_regressor_focused'][model]['real'] * data_min_u_sparse['scaler']['y']\n",
    "testing_data['min_u_filtered_regressor'][model]['predicted'] = testing_data['min_u_filtered_regressor'][model]['predicted'] * data_min_u_sparse['scaler']['y'] \n",
    "testing_data['min_u_filtered_regressor'][model]['real'] = testing_data['min_u_filtered_regressor'][model]['real'][utils.cols_with_positive_values(prediction)] * data_min_u_sparse['scaler']['y']\n",
    "testing_data['min_u_regressor_balanced'][model]['predicted'] = testing_data['min_u_regressor_balanced'][model]['predicted'] * data_min_u_sparse['scaler']['y']\n",
    "testing_data['min_u_regressor_balanced'][model]['real'] = testing_data['min_u_regressor_balanced'][model]['real'] * data_min_u_sparse['scaler']['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to evaluate all the results obtained above. This benchmarking has the objective of obtaining the answer following questions:\n",
    "- What is the optimum number of rows for the training data set? What is the respective model?\n",
    "    - sparse reg. vs balanced reg. vs focused reg.\n",
    "- What is the optimum number present busses in regresison?\n",
    "    - sparse reg. vs filtered reg.\n",
    "- Regression vs Classification\n",
    "    - filtered reg. vs sparse class.\n",
    "- What is the optimum number of rows in class?\n",
    "    - sparse class. vs balanced class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimum proportion of P/N rows for the training data set? What is the respective model?\n",
    "In order to understand the optinum number of rows for the training set of the regression data set the data sets used will be:\n",
    "\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Maximum Voltage Constraints|Balanced Regression|6971|34|\n",
    "|Maximum Voltage Constraints|Focused Regression|3486|34|\n",
    "|Minimum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Minimum Voltage Constraints|Balanced Regression|13917|34|\n",
    "|Minimum Voltage Constraints|Focused Regression|6958|34|\n",
    "\n",
    "The **Sparse Regression data set** is generated directly from the power flow results. The important moments are those where the constraints are violated, so the output feature contains null values for when there is no constraint, and positive value for when there is a constraint. The positive values represent the amplitude of the constraint violation. It can be expressed as follows:\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\text{Target} &= \\begin{cases}\n",
    "            0 & \\text{if} \\; \\text{constraint} \\; \\text{is not violated} \\\\\n",
    "            \\text{amplitude of constraint} & \\text{if} \\; \\text{constraint} \\; \\text{is violated} \\\\\n",
    "        \\end{cases}\n",
    "    \\end{align}\n",
    "$$\n",
    "In our case, the constraints are being considered as the following:\n",
    "- Minimal voltage on bus: $v_bus < 0.95 \\text{ [pu]}$ (constraint is violated if the voltage is below $0.95 \\text{ [pu]} $)\n",
    "- Maximal voltage on bus: $v_bus > 1.05 \\text{ [pu]}$ (constraint is violated if the voltage is above $1.05 \\text{ [pu]} $)\n",
    "- Maximal current on line: $i_{line} > 1 \\text{ [kA]}$ (constraint is violated if the current is above $1 \\text{ [kA]} $)\n",
    "\n",
    "The **Balanced Regression** data set is created from the **Sparse Regression data set**. It is created by taking all the rows that containt at least one constraint violation and then taking the same number of rows that do not contain any constraint violation. Finally, the **Focused Regression data set** is created by taking all the rows that contain at least one constraint violation.\n",
    "\n",
    "Since these data sets have the same number of possible negative and possible positives, all the metrics can be used to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['max_u_regressor_sparse', 'max_u_regressor_balanced', 'max_u_regressor_focused']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best alternative is to use the focused data set with a the linear regression model, because it presents a the best values for F1 and MCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['min_u_regressor_sparse', 'min_u_regressor_balanced', 'min_u_regressor_focused']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best sparse, with gradient boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimum number present busses in regresison?\n",
    "In order to understand the optinum number of busses for the training set of the regression data set the data sets used will be:\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Maximum Voltage Constraints|Filtered Regression|45216|10|\n",
    "|Minimum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Minimum Voltage Constraints|Filtered Regression|45216|10|\n",
    "\n",
    "The **Filtered Regression data set** is created from the Sparse Regression data set, but only keeping the columns that contain at least one time step with a constraint violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['max_u_regressor_sparse', 'max_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse, with GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['min_u_regressor_sparse', 'min_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse, with GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression vs Classification\n",
    "In order to understand the optinum number of busses for the training set of the regression data set the data sets used will be:\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Filtered Regression|45216|10|\n",
    "|Maximum Voltage Constraints|Sparse Classification|45216|10|\n",
    "|Minimum Voltage Constraints|Filtered Regression|45216|10|\n",
    "|Minimum Voltage Constraints|Sparse Classification|45216|10|\n",
    "\n",
    "The **Sparse Classification data set** is created from the Sparse Regression data set, but instead of having the target feature as the amplitude of the constraint violation, it is a binary feature that indicates if there is a constraint violation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['max_u_classifier', 'max_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_u_filtered regressor with the gradient boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['min_u_classifier', 'min_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_u_classifier with the gradient boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimum number of rows in class?\n",
    "In order to understand the optinum number of rows for the training set of the classification data set the data sets used will be:\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Sparse Classification|45216|10|\n",
    "|Maximum Voltage Constraints|Balanced Classification|6971|10|\n",
    "|Minimum Voltage Constraints|Sparse Classification|45216|10|\n",
    "|Minimum Voltage Constraints|Balanced Classification|13917|10|\n",
    "\n",
    "The **Balanced Classification data set** is created from the Sparse Classification data set. It is created by taking all the rows that containt at least one constraint violation and then taking the same number of rows that do not contain any constraint violation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['max_u_classifier', 'max_u_classifier_balanced']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier balanced with xgb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['min_u_classifier', 'min_u_classifier_balanced']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier balanced with svr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "In order to understand how good our predictions really are, we can compare it to a random classifier. [source](https://inside.getyourguide.com/blog/2020/9/30/what-makes-a-good-f1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['q'] =  (df['tp'] + df['fn']) / (df['fp'] + df['tn'] + df['tp'] + df['fn'])\n",
    "df['f1_coin'] = (2*df['q'])/(df['q']+1)\n",
    "# write df to csv in this directory, with the name dataset_benchmark.csv\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[('min_u_classifier_balanced', 'mlp'), 'tp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = _threshold(experiment)\n",
    "experiment = 'max_u_regressor_sparse'\n",
    "model = 'mlp' \n",
    "threshold = _threshold(experiment)\n",
    "print('Experiment: {}, model: {}, threshold: {}'.format(experiment, model, threshold))\n",
    "hybrid_metrics = metrics.Metrics()\n",
    "hybrid_metrics.get_prediction_scores(testing_data[experiment][model]['predicted'][['bus_15', 'bus_16']], testing_data[experiment][model]['real'][['bus_15', 'bus_16']], threshold=threshold)\n",
    "print('hybrid_metrics.true_positives_ctr: ', hybrid_metrics.true_positives_ctr)\n",
    "print('hybrid_metrics.true_negatives_ctr: ', hybrid_metrics.true_negatives_ctr)\n",
    "print('hybrid_metrics.false_positives_ctr: ', hybrid_metrics.false_positives_ctr)\n",
    "print('hybrid_metrics.false_negatives_ctr: ', hybrid_metrics.false_negatives_ctr)\n",
    "print('\\n')\n",
    "print('hybrid_true_positives_rmse', hybrid_metrics.hybrid_true_positives_rmse)\n",
    "print('hybrid_true_negatives_rmse', hybrid_metrics.hybrid_true_negatives_rmse)\n",
    "print('hybrid_false_positives_rmse', hybrid_metrics.hybrid_false_positives_rmse)\n",
    "print('hybrid_false_negatives_rmse', hybrid_metrics.hybrid_false_negatives_rmse)\n",
    "print('\\n')\n",
    "print('true_positives_rmse', hybrid_metrics.true_positives_rmse)\n",
    "print('true_negatives_rmse', hybrid_metrics.true_negatives_rmse)\n",
    "print('false_positives_rmse', hybrid_metrics.false_positives_rmse)\n",
    "print('false_negatives_rmse', hybrid_metrics.false_negatives_rmse)\n",
    "print('\\n')\n",
    "print('hybrid_metrics.hybrid_accuracy_rmse: ', hybrid_metrics.hybrid_accuracy_rmse)\n",
    "print('hybrid_metrics.hybrid_precision_rmse: ', hybrid_metrics.hybrid_precision_rmse)\n",
    "print('hybrid_metrics.hybrid_recall_rmse: ', hybrid_metrics.hybrid_recall_rmse)\n",
    "print('hybrid_metrics.hybrid_f1_rmse: ', hybrid_metrics.hybrid_f1_rmse)\n",
    "print('hybrid_metrics.hybrid_mcc_rmse: ', hybrid_metrics.hybrid_mcc_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure size of the plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 10))\n",
    "testing_data['max_u_classifier']['mlp']['predicted']['bus_16'].plot()\n",
    "#testing_data['max_u_classifier']['mlp']['real']['bus_16'].plot()\n",
    "\n",
    "# plot a line with the threshold\n",
    "_threshold = lambda experiment: max_u_threshold / data_max_u_balanced['scaler']['y'] if 'max_u' in experiment else min_u_threshold/ data_min_u_balanced['scaler']['y']\n",
    "threshold = _threshold('max_u_regressor_sparse')\n",
    "plt.axhline(y=threshold, color='r', linestyle='-')\n",
    "# Add legend\n",
    "plt.legend(['predicted', 'real', 'threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data['max_u_regressor_sparse']['mlp']['predicted'].apply(lambda x: x.apply(lambda y: min(1.0, max(0.0, y))))['bus_1'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data['max_u_classifier']['mlp']['predicted'].astype('float64')['bus_16'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn df to numpy array\n",
    "testing_data['max_u_classifier']['gb']['predicted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment: min_u_classifier, model: gb, max: 1.0, min: 0.0\n",
      "experiment: min_u_classifier, model: xgb, max: 1, min: 0\n",
      "experiment: min_u_classifier, model: svr, max: 1.0, min: 0.0\n",
      "experiment: min_u_classifier, model: mlp, max: 1.0, min: 0.0\n",
      "experiment: min_u_classifier_balanced, model: gb, max: 1.0, min: 0.0\n",
      "experiment: min_u_classifier_balanced, model: xgb, max: 1, min: 0\n",
      "experiment: min_u_classifier_balanced, model: svr, max: 1.0, min: 0.0\n",
      "experiment: min_u_classifier_balanced, model: mlp, max: 1.0, min: 0.0\n",
      "experiment: max_u_regressor_sparse, model: lr, max: 0.13619870924844507, min: 0.0\n",
      "experiment: max_u_regressor_sparse, model: gb, max: 0.3810799686594865, min: 0.0\n",
      "experiment: max_u_regressor_sparse, model: xgb, max: 0.3127909302711487, min: 0.024557016789913177\n",
      "experiment: max_u_regressor_sparse, model: svr, max: 0.4946822431673944, min: 0.0\n",
      "experiment: max_u_regressor_sparse, model: mlp, max: 1.0, min: 0.0\n",
      "experiment: max_u_regressor_focused, model: lr, max: 0.6501631250830311, min: 0.0\n",
      "experiment: max_u_regressor_focused, model: gb, max: 0.42120136723128043, min: 0.0\n",
      "experiment: max_u_regressor_focused, model: xgb, max: 0.42610928416252136, min: 0.0\n",
      "experiment: max_u_regressor_focused, model: svr, max: 0.46197486433850854, min: 0.0\n",
      "experiment: max_u_regressor_focused, model: mlp, max: 0.577883243560791, min: 0.0\n",
      "experiment: max_u_filtered_regressor, model: lr, max: 0.12660268004533026, min: 0.0\n",
      "experiment: max_u_regressor_balanced, model: lr, max: 0.4010267617457135, min: 0.0\n",
      "experiment: max_u_regressor_balanced, model: gb, max: 0.7517199729659615, min: 0.0\n",
      "experiment: max_u_regressor_balanced, model: xgb, max: 0.5393862724304199, min: 0.0\n",
      "experiment: max_u_regressor_balanced, model: svr, max: 0.787608227817534, min: 0.0\n",
      "experiment: max_u_regressor_balanced, model: mlp, max: 1.0, min: 0.0\n",
      "experiment: min_u_regressor_focused, model: lr, max: 0.5855579684553072, min: 0.0\n",
      "experiment: min_u_regressor_focused, model: gb, max: 0.5932988934689641, min: 0.0\n",
      "experiment: min_u_regressor_focused, model: xgb, max: 0.5121873617172241, min: 0.0\n",
      "experiment: min_u_regressor_focused, model: svr, max: 0.6045885499738398, min: 0.0\n",
      "experiment: min_u_regressor_focused, model: mlp, max: 0.34603604674339294, min: 0.0\n",
      "experiment: min_u_regressor_balanced, model: lr, max: 0.34171831351797666, min: 0.0\n",
      "experiment: min_u_regressor_balanced, model: gb, max: 0.5165846932947002, min: 0.0\n",
      "experiment: min_u_regressor_balanced, model: xgb, max: 0.4210992753505707, min: 0.0\n",
      "experiment: min_u_regressor_balanced, model: svr, max: 0.6300204477633999, min: 0.0\n",
      "experiment: min_u_regressor_balanced, model: mlp, max: 1.0, min: 0.0\n"
     ]
    }
   ],
   "source": [
    "classifier_experiments =[experiment for experiment in testing_data.keys() if 'classifier' in experiment.split('_')] # TODO confirm this\n",
    "regressor_experiments = [experiment for experiment in testing_data.keys() if 'regressor' in experiment.split('_')]\n",
    "# Classifier experiments\n",
    "for experiment in classifier_experiments:\n",
    "    for model in testing_data[experiment].keys():\n",
    "        try:\n",
    "            print('experiment: {}, model: {}, max: {}, min: {}'.format(experiment, model, testing_data[experiment][model]['predicted'].max().max(), testing_data[experiment][model]['predicted'].min().min()))\n",
    "        except:\n",
    "            print('experiment {} model {} failed'.format(experiment, model))\n",
    "for experiment in regressor_experiments:\n",
    "    for model in testing_data[experiment].keys():\n",
    "        try:\n",
    "            print('experiment: {}, model: {}, max: {}, min: {}'.format(experiment, model, testing_data[experiment][model]['predicted'].max().max(), testing_data[experiment][model]['predicted'].min().min()))\n",
    "        except:\n",
    "            print('experiment {} model {} failed'.format(experiment, model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
