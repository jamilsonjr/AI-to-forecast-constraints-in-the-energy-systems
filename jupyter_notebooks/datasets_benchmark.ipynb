{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of this Article** \n",
    "- Loading best hyperparameters for each model\n",
    "- Model training\n",
    "- Results discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading best hyperparameters for each model\n",
    "\n",
    "TODO... explain this model bench mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hyperparameters dataset.\n",
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse hyperparameters: 8/8\n",
      "Focused hyperparameters: 8/8\n",
      "Balanced hyperparameters: 8/8\n",
      "Filtered hyperparameters: 8/8\n",
      "Sparse classifier hyperparameters: 8/8\n",
      "Balanced classifier hyperparameters: 8/8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparse_hyper_params = {}\n",
    "focused_hyper_params = {}\n",
    "balanced_hyper_params = {}\n",
    "filtered_hyper_params = {}\n",
    "sparse_class_hyper_params = {}\n",
    "balanced_class_hyper_params = {}\n",
    "for file in os.listdir('hyper_params_results'):\n",
    "    if file.endswith('.csv') and 'regression_sparse' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'regression_focused' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        focused_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'regression_balanced' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        balanced_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'filtered' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        filtered_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'sparse_classifier' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_class_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'balanced_classifier' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        balanced_class_hyper_params[file] = df\n",
    "print('Sparse hyperparameters: {}/8'.format(len(sparse_hyper_params)))\n",
    "print('Focused hyperparameters: {}/8'.format(len(focused_hyper_params)))\n",
    "print('Balanced hyperparameters: {}/8'.format(len(balanced_hyper_params)))\n",
    "print('Filtered hyperparameters: {}/8'.format(len(filtered_hyper_params)))\n",
    "print('Sparse classifier hyperparameters: {}/8'.format(len(sparse_class_hyper_params)))\n",
    "print('Balanced classifier hyperparameters: {}/8'.format(len(balanced_class_hyper_params)))\n",
    "print('\\n')\n",
    "# print('Sparse hyper params:\\n')\n",
    "# for key in sparse_hyper_params.keys():\n",
    "#     print(key, ':\\n ',sparse_hyper_params[key])\n",
    "# print('Focused hyper params:\\n')\n",
    "# for key in focused_hyper_params.keys():\n",
    "#     print(key, ':\\n',focused_hyper_params[key])\n",
    "# print('Boolean hyper params:\\n')\n",
    "# for key in sparse_class_hyper_params.keys():\n",
    "#     print(key, ':\\n',sparse_class_hyper_params[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_size': 34,\n",
       " 'n_layers': 3,\n",
       " 'dropout': 0.0030412321477918842,\n",
       " 'activation': 'relu',\n",
       " 'optimizer': 'sgd',\n",
       " 'lr': 9.741292351005151e-05,\n",
       " 'epochs': 55,\n",
       " 'batch_size': 8,\n",
       " 'classifier': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "def get_hyper_params_from_df(df):\n",
    "    output = {}\n",
    "    for row in df.iterrows():\n",
    "        if row[1]['params'] != 'value':\n",
    "            try:\n",
    "                output[row[1]['params']] = ast.literal_eval(row[1]['value'])\n",
    "            except :\n",
    "                output[row[1]['params']] = row[1]['value']\n",
    "    return output\n",
    "get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_max_u.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..');from thesis_package import aimodels as my_ai, utils, metrics\n",
    "from copy import deepcopy\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data sparse\n",
    "y_max_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse, test_size=0.2, scaling=True)\n",
    "data_max_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse, test_size=0.2, scaling=True)\n",
    "data_min_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data sparse\n",
    "y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_max_u_bool = y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)]\n",
    "y_min_u_bool = y_min_u_bool[utils.cols_with_positive_values(y_min_u_bool)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_bool, test_size=0.2, scaling=True)\n",
    "data_max_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_bool, test_size=0.2, scaling=True)\n",
    "data_min_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered data\n",
    "y_max_u_filtered = deepcopy(y_max_u_sparse[utils.cols_with_positive_values(y_max_u_bool)])\n",
    "y_min_u_filtered = deepcopy(y_min_u_sparse[utils.cols_with_positive_values(y_min_u_bool)])\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_filtered, test_size=0.2, scaling=True)\n",
    "data_max_u_filtered = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_filtered, test_size=0.2, scaling=True)\n",
    "data_min_u_filtered = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification data size:  (9044, 10)\n",
      "Regression data size:  (9044, 10)\n",
      "Positive in classification data:  5036.0\n",
      "Positive in regression data:  5036\n",
      "Theshhold:  0.001591058368850724\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the classiciation testing data and the filtered testing data\n",
    "print('Classification data size: ', data_max_u_bool['y_test'].shape)\n",
    "print('Regression data size: ', data_max_u_filtered['y_test'].shape)\n",
    "print('Positive in classification data: ', utils.count_positives_class(data_max_u_bool['y_test']))\n",
    "#unscaled_y_test = pd.DataFrame(data_max_u_filtered['scaler']['y'].inverse_transform(data_max_u_filtered['y_test']), columns=data_max_u_filtered['y_test'].columns)\n",
    "unscaled_y_test = utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y'])\n",
    "print('Positive in regression data: ', utils.count_positives_reg(unscaled_y_test, utils.compute_threshold(y_max_u_sparse)))\n",
    "print('Theshhold: ', utils.compute_threshold(y_max_u_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data focused\n",
    "y_max_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_focused_constr.csv')\n",
    "exogenous_data_focused_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_focused.csv').drop(columns=['date'])\n",
    "y_min_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_focused_constr.csv')\n",
    "exogenous_data_focused_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_focused.csv').drop(columns=['date'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_focused_max_u, y_max_u_focused, test_size=0.2, scaling=True)\n",
    "data_max_u_focused = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_focused_min_u, y_min_u_focused, test_size=0.2, scaling=True)\n",
    "data_min_u_focused = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data balanced\n",
    "y_max_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced, test_size=0.2, scaling=True)\n",
    "data_max_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced, test_size=0.2, scaling=True)\n",
    "data_min_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data balanced\n",
    "y_max_u_balanced_class = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_bool_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced_class = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_bool_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "y_max_u_balanced_class = y_max_u_balanced_class[utils.cols_with_positive_values(y_max_u_balanced_class)]\n",
    "y_min_u_balanced_class = y_min_u_balanced_class[utils.cols_with_positive_values(y_min_u_balanced_class)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced_class, test_size=0.2, scaling=True)\n",
    "data_max_u_bool_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced_class, test_size=0.2, scaling=True)\n",
    "data_min_u_bool_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count in classification data max_u : 5036.0\n",
      "Positive count in regression data max_u with threshold 0.001591058368850724 : 5036\n",
      "\n",
      "\n",
      "Positive count in classification data min_u : 6018.0\n",
      "Positive count in regression data min_u with threshold 0.0020242378560612192 : 6018\n",
      "\n",
      "\n",
      "Negative count in classification data max_u : 85404.0\n",
      "Negative count in regression data max_u with threshold 0.001591058368850724 : 85404\n",
      "\n",
      "\n",
      "Negative count in classification data min_u : 84422.0\n",
      "Negative count in regression data min_u with threshold 0.0020242378560612192 : 84422\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.check_positive_count(utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y']), data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')\n",
    "utils.check_positive_count(utils.unscale_df(data_min_u_filtered['y_test'], data_min_u_filtered['scaler']['y']), data_min_u_bool['y_test'], utils.compute_threshold(y_min_u_sparse), experiment='min_u')\n",
    "utils.check_negative_count(utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y']), data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')\n",
    "utils.check_negative_count(utils.unscale_df(data_min_u_filtered['y_test'], data_min_u_filtered['scaler']['y']), data_min_u_bool['y_test'], utils.compute_threshold(y_min_u_sparse), experiment='min_u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count in classification data max_u : 5036.0\n",
      "Positive count in regression data max_u with threshold 0.001591058368850724 : 5036\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.check_positive_count(utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y']), data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "In this section the models will be trained with the hyperparameters loaded above. All the models will be stored in the same `Context` object for later evaluation. The `Context` object is a class that stores all the models and their respective hyperparameters. The `Context` object is defined in the `aimodels.py` file. The `Context` object is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_models = ['lr', 'gb', 'xgb', 'svr', 'mlp']\n",
    "class_models =  ['gb', 'xgb', 'svr', 'mlp']\n",
    "max_u_threshold = utils.compute_threshold(y_max_u_sparse)\n",
    "min_u_threshold = utils.compute_threshold(y_min_u_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_filtered(df, scaler):\n",
    "    for bus in df.columns:\n",
    "        idx = list(scaler.feature_names_in_).index(bus)\n",
    "        df[bus] = scaler.max_abs_[idx] * df[bus]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['params_gradient_boost_regression_sparse_max_u.csv', 'params_gradient_boost_regression_sparse_min_u.csv', 'params_mlp_regression_sparse_max_u.csv', 'params_mlp_regression_sparse_min_u.csv', 'params_support_vector_regression_sparse_max_u.csv', 'params_support_vector_regression_sparse_min_u.csv', 'params_xgboost_regression_sparse_max_u.csv', 'params_xgboost_regression_sparse_min_u.csv'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_hyper_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression sparse\n"
     ]
    }
   ],
   "source": [
    "# max_u regression sparse\n",
    "if 'max_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression sparse')\n",
    "    # Linear Regression\n",
    "    regressor_max_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_max_u.csv'])\n",
    "    regressor_max_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_xgboost_regression_sparse_max_u.csv']) \n",
    "    regressor_max_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_support_vector_regression_sparse_max_u.csv'])\n",
    "    regressor_max_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_mlp_regression_sparse_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_sparse['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_sparse['y_train'].shape[1]\n",
    "    regressor_max_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_sparse', regressor_max_u)\n",
    "else:\n",
    "    print('Loading max_u regression sparse') \n",
    "    regressor_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_regressor_sparse')\n",
    "\n",
    "testing_data = {'max_u_regressor_sparse': {}}\n",
    "for model, strategy in zip(reg_models, regressor_max_u.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_sparse['y_test'].columns)\n",
    "    testing_data['max_u_regressor_sparse'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_sparse'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_sparse'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])\n",
    "    # Unscale\n",
    "    testing_data['max_u_regressor_sparse'][model]['predicted'] = utils.unscale_df(testing_data['max_u_regressor_sparse'][model]['predicted'],\\\n",
    "                                                                                data_max_u_sparse['scaler']['y'])\n",
    "    testing_data['max_u_regressor_sparse'][model]['real'] = utils.unscale_df(testing_data['max_u_regressor_sparse'][model]['real'],\\\n",
    "                                                                        data_max_u_sparse['scaler']['y'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression focused\n"
     ]
    }
   ],
   "source": [
    "# max_u regression focused\n",
    "if 'max_u_regressor_focused.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression focused')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_focused = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_gradient_boost_regression_focused_max_u.csv'])\n",
    "    regressor_max_u_focused.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_xgboost_regression_focused_max_u.csv']) \n",
    "    regressor_max_u_focused.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_support_vector_regression_focused_max_u.csv'])\n",
    "    regressor_max_u_focused.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_focused['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_focused['y_train'].shape[1]\n",
    "    regressor_max_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_focused', regressor_max_u_focused)\n",
    "else: \n",
    "    print('Loading max_u regression focused')\n",
    "    regressor_max_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\\\max_u_regressor_focused')\n",
    "\n",
    "testing_data['max_u_regressor_focused'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_focused.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_sparse['y_test'].columns)\n",
    "    testing_data['max_u_regressor_focused'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_focused'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_focused'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['max_u_regressor_focused'][model]['predicted'] = utils.unscale_df(testing_data['max_u_regressor_focused'][model]['predicted'],\\\n",
    "                                                                                data_max_u_sparse['scaler']['y'])\n",
    "    testing_data['max_u_regressor_focused'][model]['real'] = utils.unscale_df(testing_data['max_u_regressor_focused'][model]['real'],\\\n",
    "                                                                        data_max_u_sparse['scaler']['y'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u filtered regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n"
     ]
    }
   ],
   "source": [
    "# max_u regression filtered\n",
    "if 'max_u_filtered_regressor.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression filtered')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_filtered = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_gradient_boost_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_xgboost_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_support_vector_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_mlp_regression_filtered_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_filtered['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_filtered['y_train'].shape[1]\n",
    "    regressor_max_u_filtered.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_filtered_regressor', regressor_max_u_filtered)\n",
    "else: \n",
    "    print('Loading max_u filtered regression')\n",
    "    regressor_max_u_filtered = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_filtered_regressor')\n",
    "\n",
    "testing_data['max_u_filtered_regressor'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_filtered.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_filtered['y_test'].columns)\n",
    "    testing_data['max_u_filtered_regressor'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_filtered_regressor'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_filtered_regressor'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['max_u_filtered_regressor'][model]['predicted'] = inverse_transform_filtered(testing_data['max_u_filtered_regressor'][model]['predicted'],\\\n",
    "                                                                                data_max_u_sparse['scaler']['y'])\n",
    "    testing_data['max_u_filtered_regressor'][model]['real'] = inverse_transform_filtered(testing_data['max_u_filtered_regressor'][model]['real'][utils.cols_with_positive_values(prediction)],\\\n",
    "                                                                        data_max_u_sparse['scaler']['y'])\n",
    "    # Filter real data\n",
    "    #testing_data['max_u_filtered_regressor'][model]['real'] = deepcopy(data_max_u_sparse['y_test'][utils.cols_with_positive_values(prediction)])\n",
    "# print(utils.count_positives_reg(testing_data['max_u_filtered_regressor']['mlp']['real'], max_u_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression balanced\n"
     ]
    }
   ],
   "source": [
    "# max u regression balanced\n",
    "if 'max_u_regressor_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression balanced')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_balanced = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_gradient_boost_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_xgboost_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_support_vector_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_mlp_regression_balanced_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_balanced['y_train'].shape[1]\n",
    "    regressor_max_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_balanced', regressor_max_u_balanced)\n",
    "else: \n",
    "    print('Loading max_u regression balanced')\n",
    "    regressor_max_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_regressor_balanced')\n",
    "\n",
    "testing_data['max_u_regressor_balanced'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_balanced['y_test'].columns)\n",
    "    testing_data['max_u_regressor_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_balanced'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['max_u_regressor_balanced'][model]['predicted'] = utils.unscale_df(testing_data['max_u_regressor_balanced'][model]['predicted'],\\\n",
    "                                                                                data_max_u_balanced['scaler']['y'])\n",
    "    testing_data['max_u_regressor_balanced'][model]['real'] = utils.unscale_df(testing_data['max_u_regressor_balanced'][model]['real'],\\\n",
    "                                                                        data_max_u_sparse['scaler']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u classification\n"
     ]
    }
   ],
   "source": [
    "# max_u classification\n",
    "if 'max_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u classification')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_gradient_boost_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_xgboost_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_support_vector_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_mlp_sparse_classifier_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_bool['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_bool['y_train'].shape[1]\n",
    "    classifier_max_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_classifier', classifier_max_u)\n",
    "else: \n",
    "    print('Loading max_u classification')\n",
    "    classifier_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_classifier')\n",
    "\n",
    "testing_data['max_u_classifier'] = {}\n",
    "for model, strategy in zip(class_models, classifier_max_u.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_bool['y_test'].columns)\n",
    "    testing_data['max_u_classifier'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_classifier'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_classifier'][model]['real'] = deepcopy(data_max_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testing_data['max_u_classifier']['xgb']['predicted']['bus_15'].plot()\n",
    "testing_data['max_u_classifier']['xgb']['real']['bus_15'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u classification balanced\n"
     ]
    }
   ],
   "source": [
    "# max_u classification balanced\n",
    "if 'max_u_classifier_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u classification balanced')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_gradient_boost_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_xgboost_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_support_vector_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_mlp_balanced_classifier_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_bool_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_bool_balanced['y_train'].shape[1]\n",
    "    classifier_max_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_classifier_balanced', classifier_max_u_balanced)\n",
    "else: \n",
    "    print('Loading max_u classification balanced')\n",
    "    classifier_max_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_classifier_balanced')\n",
    "\n",
    "testing_data['max_u_classifier_balanced'] = {}\n",
    "for model, strategy in zip(class_models, classifier_max_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_bool_balanced['y_test'].columns)\n",
    "    testing_data['max_u_classifier_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_classifier_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_classifier_balanced'][model]['real'] = deepcopy(data_max_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<thesis_package.aimodels.GradientBoostClassifierStrategy at 0x1e4ccd0a190>,\n",
       " <thesis_package.aimodels.XGBoostClassifierStrategy at 0x1e4d0c41ca0>,\n",
       " <thesis_package.aimodels.SupportVectorClassifierStrategy at 0x1e4cccfdf70>,\n",
       " <thesis_package.aimodels.MultilayerPerceptronStrategy at 0x1e4cf5c2b20>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_max_u_balanced.strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min u regression training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression sparse\n"
     ]
    }
   ],
   "source": [
    "# min_u regression sparse\n",
    "if 'min_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression sparse')\n",
    "    # Linear Regression\n",
    "    regressor_min_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_xgboost_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_support_vector_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_mlp_regression_sparse_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_sparse['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_sparse['y_train'].shape[1]\n",
    "    regressor_min_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse', regressor_min_u)\n",
    "else:\n",
    "    print('Loading min_u regression sparse')\n",
    "    regressor_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse')\n",
    "\n",
    "testing_data['min_u_regressor_sparse'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_sparse['y_test'].columns)\n",
    "    testing_data['min_u_regressor_sparse'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_sparse'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_sparse'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['min_u_regressor_sparse'][model]['predicted'] = utils.unscale_df(testing_data['min_u_regressor_sparse'][model]['predicted'],\\\n",
    "                                                                                data_min_u_sparse['scaler']['y'])\n",
    "    testing_data['min_u_regressor_sparse'][model]['real'] = utils.unscale_df(testing_data['min_u_regressor_sparse'][model]['real'],\\\n",
    "                                                                        data_min_u_sparse['scaler']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression focused\n"
     ]
    }
   ],
   "source": [
    "# min_u regression focused\n",
    "if 'min_u_regressor_focused.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression focused')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_focused = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_gradient_boost_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_xgboost_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_support_vector_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_focused['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_focused['y_train'].shape[1]\n",
    "    regressor_min_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_focused', regressor_min_u_focused)\n",
    "else:\n",
    "    print('Loading min_u regression focused')\n",
    "    regressor_min_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_focused')\n",
    "\n",
    "testing_data['min_u_regressor_focused'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_focused.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_focused['y_test'].columns)\n",
    "    testing_data['min_u_regressor_focused'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_focused'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_focused'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['min_u_regressor_focused'][model]['predicted'] = utils.unscale_df(testing_data['min_u_regressor_focused'][model]['predicted'],\\\n",
    "                                                                                data_min_u_focused['scaler']['y'])\n",
    "    testing_data['min_u_regressor_focused'][model]['real'] = utils.unscale_df(testing_data['min_u_regressor_focused'][model]['real'],\\\n",
    "                                                                        data_min_u_sparse['scaler']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u filtered regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n"
     ]
    }
   ],
   "source": [
    "# min u regression filtered\n",
    "if 'min_u_filtered_regressor.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression filtered')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_filtered = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_gradient_boost_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_xgboost_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_support_vector_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_mlp_regression_filtered_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_filtered['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_filtered['y_train'].shape[1]\n",
    "    regressor_min_u_filtered.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_filtered_regressor', regressor_min_u_filtered)\n",
    "else: \n",
    "    print('Loading min_u filtered regression')\n",
    "    regressor_min_u_filtered = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_filtered_regressor')\n",
    "\n",
    "testing_data['min_u_filtered_regressor'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_filtered.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_filtered['y_test'].columns)\n",
    "    testing_data['min_u_filtered_regressor'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_filtered_regressor'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_filtered_regressor'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['min_u_filtered_regressor'][model]['predicted'] = inverse_transform_filtered(testing_data['min_u_filtered_regressor'][model]['predicted'],\\\n",
    "                                                                                data_min_u_sparse['scaler']['y'])\n",
    "    testing_data['min_u_filtered_regressor'][model]['real'] = inverse_transform_filtered(testing_data['min_u_filtered_regressor'][model]['real'][utils.cols_with_positive_values(prediction)],\\\n",
    "                                                                        data_min_u_sparse['scaler']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression balanced\n"
     ]
    }
   ],
   "source": [
    "# min u regression balanced\n",
    "if 'min_u_regressor_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression balanced')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_balanced = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_gradient_boost_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_xgboost_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_support_vector_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_mlp_regression_balanced_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_balanced['y_train'].shape[1]\n",
    "    regressor_min_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_balanced', regressor_min_u_balanced)\n",
    "else: \n",
    "    print('Loading min_u regression balanced')\n",
    "    regressor_min_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_balanced')\n",
    "\n",
    "testing_data['min_u_regressor_balanced'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_balanced['y_test'].columns)\n",
    "    testing_data['min_u_regressor_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_balanced'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['min_u_regressor_balanced'][model]['predicted'] = utils.unscale_df(testing_data['min_u_regressor_balanced'][model]['predicted'],\\\n",
    "                                                                                data_min_u_balanced['scaler']['y'])\n",
    "    testing_data['min_u_regressor_balanced'][model]['real'] = utils.unscale_df(testing_data['min_u_regressor_balanced'][model]['real'],\\\n",
    "                                                                        data_min_u_sparse['scaler']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u classification\n"
     ]
    }
   ],
   "source": [
    "# min_u classification\n",
    "if 'min_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u classification')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_gradient_boost_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_xgboost_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_support_vector_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_mlp_sparse_classifier_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_bool['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_bool['y_train'].shape[1]\n",
    "    classifier_min_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_classifier', classifier_min_u)\n",
    "else: \n",
    "    print('Loading min_u classification')\n",
    "    classifier_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_classifier')\n",
    "\n",
    "testing_data['min_u_classifier'] = {}\n",
    "for model, strategy in zip(class_models, classifier_min_u.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_bool['y_test'].columns)\n",
    "    testing_data['min_u_classifier'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_classifier'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_classifier'][model]['real'] = deepcopy(data_min_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<thesis_package.aimodels.GradientBoostClassifierStrategy at 0x1e4d4ef8280>,\n",
       " <thesis_package.aimodels.XGBoostClassifierStrategy at 0x1e4d4fefdf0>,\n",
       " <thesis_package.aimodels.SupportVectorClassifierStrategy at 0x1e4d4fddf10>,\n",
       " <thesis_package.aimodels.MultilayerPerceptronStrategy at 0x1e4de822b80>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_min_u.strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u classification balanced\n"
     ]
    }
   ],
   "source": [
    "# min u classification balanced\n",
    "if 'min_u_classifier_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u classification balanced')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_gradient_boost_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_xgboost_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_support_vector_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_mlp_balanced_classifier_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_bool_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_bool_balanced['y_train'].shape[1]\n",
    "    classifier_min_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_classifier_balanced', classifier_min_u_balanced)\n",
    "else: \n",
    "    print('Loading min_u classification balanced')\n",
    "    classifier_min_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_classifier_balanced')\n",
    "\n",
    "testing_data['min_u_classifier_balanced'] = {}\n",
    "for model, strategy in zip(class_models, classifier_min_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_bool_balanced['y_test'].columns)\n",
    "    testing_data['min_u_classifier_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_classifier_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_classifier_balanced'][model]['real'] = deepcopy(data_min_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "In this section the results of the training and testing are presented and compared. The main objectives of this experience is to compare the performance of the regression models in terms of the hybrid metrics confusion matrix and the hybrid metrics rmse. The comparisons will be the following:\n",
    "- Compare the confusion matrices of the classification models and the regression models evaluate with the hybrid metrics.\n",
    "- Compare the error results of the regression models trained with the focused dataset and the sparse dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_u_regressor_sparse :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_regressor_focused :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_filtered_regressor :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_regressor_balanced :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_classifier :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_classifier_balanced :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_regressor_sparse :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_regressor_focused :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_filtered_regressor :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_regressor_balanced :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_classifier :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_classifier_balanced :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n"
     ]
    }
   ],
   "source": [
    "for experience in testing_data.keys():\n",
    "    print(experience,': ', testing_data[experience].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3990 + 1046 = 5036 = 5036.0 possible positive values.\n",
      "59437 + 25967 = 85404 = 85404.0 possible negative values.\n"
     ]
    }
   ],
   "source": [
    "# Testing all models: Function that receives a dict with the real and predicted values, and outputs a dataframe with the results of the metrics.\n",
    "# Accumulate all the classifications for each bus.\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for bus in testing_data['max_u_classifier']['mlp']['predicted'].columns:\n",
    "    # Compute tp, tn, fp, fn\n",
    "    tp += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 1) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 1))\n",
    "    tn += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 0) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 0))\n",
    "    fp += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 1) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 0))\n",
    "    fn += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 0) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 1))\n",
    "print('{} + {} = {} = {} possible positive values.'.format(tp, fn, tp+fn, testing_data['max_u_classifier']['mlp']['real'].sum().sum()))\n",
    "print('{} + {} = {} = {} possible negative values.'.format(tn, fp, tn+fp, testing_data['max_u_classifier']['mlp']['real'].shape[0]*testing_data['max_u_classifier']['mlp']['real'].shape[1] - testing_data['max_u_classifier']['mlp']['real'].sum().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    df.to_csv('dataset_benchmark.csv')\n",
    "except:\n",
    "    from numpy import sqrt \n",
    "    # Build a multi-index dataframe with the results of the metrics. The first index is the testing_data.keys(), the second index are the tp, tn, fp, fn, and the columns are the models.\n",
    "    columns = ['tp', 'tn', 'fp', 'fn', '(hybrid)accuracy', '(hybrid)precision', '(hybrid)recall', '(hybrid)f1']\n",
    "    index = pd.MultiIndex.from_product([testing_data.keys(), ['lr', 'gb', 'xgb', 'svr', 'mlp']], names=['experiment', 'class'])\n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "    classifier_experiments =[experiment for experiment in testing_data.keys() if 'classifier' in experiment.split('_')] # TODO confirm this\n",
    "    regressor_experiments = [experiment for experiment in testing_data.keys() if 'regressor' in experiment.split('_')]\n",
    "    # Classifier experiments\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for experiment in classifier_experiments:\n",
    "        for model in testing_data[experiment].keys():\n",
    "            for bus in testing_data[experiment][model]['predicted'].columns:\n",
    "                try:\n",
    "                    tp += sum((testing_data[experiment][model]['predicted'][bus] == 1) & (testing_data[experiment][model]['real'][bus] == 1))\n",
    "                    tn += sum((testing_data[experiment][model]['predicted'][bus] == 0) & (testing_data[experiment][model]['real'][bus] == 0))\n",
    "                    fp += sum((testing_data[experiment][model]['predicted'][bus] == 1) & (testing_data[experiment][model]['real'][bus] == 0))\n",
    "                    fn += sum((testing_data[experiment][model]['predicted'][bus] == 0) & (testing_data[experiment][model]['real'][bus] == 1))\n",
    "                except: \n",
    "                    print('In the experiment ', experiment, ' and model ', model, ' there was a problem with bus: ', bus)\n",
    "                    if not testing_data[experiment][model]['real'][bus].any():\n",
    "                        print('Bus {} has no positive data points. Just ignore the little shit.'.format(bus))    \n",
    "            df.loc[(experiment, model), 'tp'] = tp\n",
    "            df.loc[(experiment, model), 'tn'] = tn\n",
    "            df.loc[(experiment, model), 'fp'] = fp\n",
    "            df.loc[(experiment, model), 'fn'] = fn\n",
    "            #print('Experiment: {}, model: {}, tp: {}, tn: {}, fp: {}, fn: {}'.format(experiment, model, tp, tn, fp, fn))\n",
    "            if (tp + tn + fp + fn) != 0:\n",
    "                accuracy = (tp + tn ) / (tp + tn + fp + fn)\n",
    "            else: \n",
    "                accuracy = 0\n",
    "            if (tp + fp) != 0:\n",
    "                precision = tp / (tp + fp)\n",
    "            else:\n",
    "                precision = 0\n",
    "            if (tp + fn) != 0:\n",
    "                recall = tp / (tp + fn)\n",
    "            else:\n",
    "                recall = 0\n",
    "            if (precision + recall) != 0:\n",
    "                f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            else:\n",
    "                f1 = 0\n",
    "            if (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) > 0:\n",
    "                mcc = (tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "            df.loc[(experiment, model), '(hybrid)accuracy'] = accuracy\n",
    "            df.loc[(experiment, model), '(hybrid)precision'] = precision\n",
    "            df.loc[(experiment, model), '(hybrid)recall'] = recall\n",
    "            df.loc[(experiment, model), '(hybrid)f1'] = f1\n",
    "            df.loc[(experiment, model), '(hybrid)mcc'] = mcc\n",
    "            # print('Experiment: {}, model: {}, accuracy: {}, precision: {}, recall: {}, f1: {}'.format(experiment, model, accuracy, precision, recall, f1))\n",
    "            tp = 0\n",
    "            tn = 0\n",
    "            fp = 0\n",
    "            fn = 0 \n",
    "    # Regressor experiments\n",
    "    _threshold = lambda experiment: max_u_threshold if 'max_u' in experiment else min_u_threshold\n",
    "    for experiment in regressor_experiments:\n",
    "        for model in testing_data[experiment].keys():\n",
    "            try:\n",
    "                threshold = _threshold(experiment)\n",
    "                print('Experiment: {}, model: {}, threshold: {}'.format(experiment, model, threshold))\n",
    "                hybrid_metrics = metrics.Metrics()\n",
    "                hybrid_metrics.get_prediction_scores(testing_data[experiment][model]['predicted'], testing_data[experiment][model]['real'], threshold=threshold)\n",
    "                df.loc[(experiment, model), 'tp'] = hybrid_metrics.true_positives_ctr\n",
    "                df.loc[(experiment, model), 'tn'] = hybrid_metrics.true_negatives_ctr\n",
    "                df.loc[(experiment, model), 'fp'] = hybrid_metrics.false_positives_ctr\n",
    "                df.loc[(experiment, model), 'fn'] = hybrid_metrics.false_negatives_ctr\n",
    "                df.loc[(experiment, model), '(hybrid)accuracy'] = hybrid_metrics.hybrid_accuracy\n",
    "                df.loc[(experiment, model), '(hybrid)precision'] = hybrid_metrics.hybrid_precision\n",
    "                df.loc[(experiment, model), '(hybrid)recall'] = hybrid_metrics.hybrid_recall\n",
    "                df.loc[(experiment, model), '(hybrid)f1'] = hybrid_metrics.hybrid_f1\n",
    "                df.loc[(experiment, model), '(hybrid)mcc'] = hybrid_metrics.hybrid_mcc\n",
    "                # print('Experiment: {}, model: {}, tp: {}, tn: {}, fp: {}, fn: {}'.format(experiment, model, hybrid_metrics.true_positives_ctr, hybrid_metrics.true_negatives_ctr, hybrid_metrics.false_positives_ctr, hybrid_metrics.false_negatives_ctr))\n",
    "                # print('Experiment: {}, model: {}, accuracy: {}, precision: {}, recall: {}, f1: {}'.format(experiment, model, hybrid_metrics.hybrid_accuracy, hybrid_metrics.hybrid_precision, hybrid_metrics.hybrid_recall, hybrid_metrics.hybrid_f1))\n",
    "            except(Exception) as e:\n",
    "                print('In the experiment ', experiment, ' and model ', model, ' there was a problem')\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: max_u_regressor_sparse, model: lr, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_sparse, model: gb, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_sparse, model: xgb, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_sparse, model: svr, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_sparse, model: mlp, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_focused, model: lr, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_focused, model: gb, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_focused, model: xgb, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_focused, model: svr, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_focused, model: mlp, threshold: 0.001591058368850724\n",
      "Experiment: max_u_filtered_regressor, model: lr, threshold: 0.001591058368850724\n",
      "Experiment: max_u_filtered_regressor, model: gb, threshold: 0.001591058368850724\n",
      "Experiment: max_u_filtered_regressor, model: xgb, threshold: 0.001591058368850724\n",
      "Experiment: max_u_filtered_regressor, model: svr, threshold: 0.001591058368850724\n",
      "Experiment: max_u_filtered_regressor, model: mlp, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_balanced, model: lr, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_balanced, model: gb, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_balanced, model: xgb, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_balanced, model: svr, threshold: 0.001591058368850724\n",
      "Experiment: max_u_regressor_balanced, model: mlp, threshold: 0.001591058368850724\n",
      "Experiment: min_u_regressor_sparse, model: lr, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_sparse, model: gb, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_sparse, model: xgb, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_sparse, model: svr, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_sparse, model: mlp, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_focused, model: lr, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_focused, model: gb, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_focused, model: xgb, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_focused, model: svr, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_focused, model: mlp, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_filtered_regressor, model: lr, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_filtered_regressor, model: gb, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_filtered_regressor, model: xgb, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_filtered_regressor, model: svr, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_filtered_regressor, model: mlp, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_balanced, model: lr, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_balanced, model: gb, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_balanced, model: xgb, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_balanced, model: svr, threshold: 0.0020242378560612192\n",
      "Experiment: min_u_regressor_balanced, model: mlp, threshold: 0.0020242378560612192\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>298021</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.979047</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.479834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3361</td>\n",
       "      <td>299916</td>\n",
       "      <td>2544</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>0.568695</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.613786</td>\n",
       "      <td>0.608879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3983</td>\n",
       "      <td>97550</td>\n",
       "      <td>204910</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.338473</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.790252</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.032998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3885</td>\n",
       "      <td>290445</td>\n",
       "      <td>12015</td>\n",
       "      <td>1151</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>0.257948</td>\n",
       "      <td>0.770778</td>\n",
       "      <td>0.386537</td>\n",
       "      <td>0.431802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>211703</td>\n",
       "      <td>90757</td>\n",
       "      <td>0</td>\n",
       "      <td>1.466242</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099287</td>\n",
       "      <td>-0.275648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>4089</td>\n",
       "      <td>298466</td>\n",
       "      <td>3994</td>\n",
       "      <td>947</td>\n",
       "      <td>0.983766</td>\n",
       "      <td>0.505694</td>\n",
       "      <td>0.81232</td>\n",
       "      <td>0.62334</td>\n",
       "      <td>0.633630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4416</td>\n",
       "      <td>252698</td>\n",
       "      <td>49762</td>\n",
       "      <td>620</td>\n",
       "      <td>0.836891</td>\n",
       "      <td>0.081427</td>\n",
       "      <td>0.876656</td>\n",
       "      <td>0.149013</td>\n",
       "      <td>0.237300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4700</td>\n",
       "      <td>71618</td>\n",
       "      <td>230842</td>\n",
       "      <td>336</td>\n",
       "      <td>0.256491</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.933189</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>0.053516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4300</td>\n",
       "      <td>275628</td>\n",
       "      <td>26832</td>\n",
       "      <td>736</td>\n",
       "      <td>0.91493</td>\n",
       "      <td>0.144597</td>\n",
       "      <td>0.85364</td>\n",
       "      <td>0.247304</td>\n",
       "      <td>0.330513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3247</td>\n",
       "      <td>194204</td>\n",
       "      <td>108256</td>\n",
       "      <td>1789</td>\n",
       "      <td>0.631347</td>\n",
       "      <td>0.030686</td>\n",
       "      <td>0.645414</td>\n",
       "      <td>0.058587</td>\n",
       "      <td>0.075512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>80965</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.453823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3260</td>\n",
       "      <td>83067</td>\n",
       "      <td>2337</td>\n",
       "      <td>1776</td>\n",
       "      <td>0.954708</td>\n",
       "      <td>0.581068</td>\n",
       "      <td>0.646568</td>\n",
       "      <td>0.612071</td>\n",
       "      <td>0.589039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4315</td>\n",
       "      <td>71932</td>\n",
       "      <td>13472</td>\n",
       "      <td>721</td>\n",
       "      <td>0.843229</td>\n",
       "      <td>0.241617</td>\n",
       "      <td>0.85642</td>\n",
       "      <td>0.376901</td>\n",
       "      <td>0.402428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3613</td>\n",
       "      <td>75525</td>\n",
       "      <td>9879</td>\n",
       "      <td>1423</td>\n",
       "      <td>0.875203</td>\n",
       "      <td>0.266493</td>\n",
       "      <td>0.716784</td>\n",
       "      <td>0.388533</td>\n",
       "      <td>0.386221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>22</td>\n",
       "      <td>85382</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.05415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.004214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4513</td>\n",
       "      <td>282576</td>\n",
       "      <td>19884</td>\n",
       "      <td>523</td>\n",
       "      <td>0.933826</td>\n",
       "      <td>0.184598</td>\n",
       "      <td>0.895985</td>\n",
       "      <td>0.306125</td>\n",
       "      <td>0.389575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4109</td>\n",
       "      <td>297630</td>\n",
       "      <td>4830</td>\n",
       "      <td>927</td>\n",
       "      <td>0.981642</td>\n",
       "      <td>0.463762</td>\n",
       "      <td>0.815659</td>\n",
       "      <td>0.591317</td>\n",
       "      <td>0.607091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4242</td>\n",
       "      <td>87755</td>\n",
       "      <td>214705</td>\n",
       "      <td>794</td>\n",
       "      <td>0.304284</td>\n",
       "      <td>0.019722</td>\n",
       "      <td>0.842058</td>\n",
       "      <td>0.038541</td>\n",
       "      <td>0.038495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3974</td>\n",
       "      <td>290777</td>\n",
       "      <td>11683</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.959239</td>\n",
       "      <td>0.258391</td>\n",
       "      <td>0.789028</td>\n",
       "      <td>0.389296</td>\n",
       "      <td>0.437418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5008</td>\n",
       "      <td>226465</td>\n",
       "      <td>75995</td>\n",
       "      <td>28</td>\n",
       "      <td>1.008493</td>\n",
       "      <td>0.061813</td>\n",
       "      <td>0.994026</td>\n",
       "      <td>0.116388</td>\n",
       "      <td>-0.248941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>2598</td>\n",
       "      <td>83124</td>\n",
       "      <td>2280</td>\n",
       "      <td>2438</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>0.532595</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.524107</td>\n",
       "      <td>0.496589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1867</td>\n",
       "      <td>84013</td>\n",
       "      <td>1391</td>\n",
       "      <td>3169</td>\n",
       "      <td>0.94958</td>\n",
       "      <td>0.573051</td>\n",
       "      <td>0.370731</td>\n",
       "      <td>0.450205</td>\n",
       "      <td>0.436154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>0</td>\n",
       "      <td>85404</td>\n",
       "      <td>0</td>\n",
       "      <td>5036</td>\n",
       "      <td>0.944317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3990</td>\n",
       "      <td>59437</td>\n",
       "      <td>25967</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.701316</td>\n",
       "      <td>0.133191</td>\n",
       "      <td>0.792295</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.237879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3546</td>\n",
       "      <td>81683</td>\n",
       "      <td>3721</td>\n",
       "      <td>1490</td>\n",
       "      <td>0.942382</td>\n",
       "      <td>0.487959</td>\n",
       "      <td>0.70413</td>\n",
       "      <td>0.576445</td>\n",
       "      <td>0.557219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3180</td>\n",
       "      <td>83080</td>\n",
       "      <td>2324</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.953782</td>\n",
       "      <td>0.577762</td>\n",
       "      <td>0.631454</td>\n",
       "      <td>0.603416</td>\n",
       "      <td>0.579572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4029</td>\n",
       "      <td>80498</td>\n",
       "      <td>4906</td>\n",
       "      <td>1007</td>\n",
       "      <td>0.93462</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>0.80004</td>\n",
       "      <td>0.576766</td>\n",
       "      <td>0.570683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3967</td>\n",
       "      <td>49322</td>\n",
       "      <td>36082</td>\n",
       "      <td>1069</td>\n",
       "      <td>0.589219</td>\n",
       "      <td>0.099054</td>\n",
       "      <td>0.787728</td>\n",
       "      <td>0.175979</td>\n",
       "      <td>0.168613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>297640</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.976169</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.391930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5039</td>\n",
       "      <td>294700</td>\n",
       "      <td>6778</td>\n",
       "      <td>979</td>\n",
       "      <td>0.974861</td>\n",
       "      <td>0.426091</td>\n",
       "      <td>0.837325</td>\n",
       "      <td>0.564781</td>\n",
       "      <td>0.586979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4781</td>\n",
       "      <td>293809</td>\n",
       "      <td>7669</td>\n",
       "      <td>1237</td>\n",
       "      <td>0.971135</td>\n",
       "      <td>0.383609</td>\n",
       "      <td>0.794475</td>\n",
       "      <td>0.517396</td>\n",
       "      <td>0.540216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5208</td>\n",
       "      <td>288486</td>\n",
       "      <td>12992</td>\n",
       "      <td>810</td>\n",
       "      <td>0.955228</td>\n",
       "      <td>0.285838</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>0.42972</td>\n",
       "      <td>0.482436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6011</td>\n",
       "      <td>229483</td>\n",
       "      <td>71995</td>\n",
       "      <td>7</td>\n",
       "      <td>1.023041</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>-0.280415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>5929</td>\n",
       "      <td>242474</td>\n",
       "      <td>59004</td>\n",
       "      <td>89</td>\n",
       "      <td>0.808951</td>\n",
       "      <td>0.091561</td>\n",
       "      <td>0.985199</td>\n",
       "      <td>0.167551</td>\n",
       "      <td>0.268520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5657</td>\n",
       "      <td>257440</td>\n",
       "      <td>44038</td>\n",
       "      <td>361</td>\n",
       "      <td>0.856304</td>\n",
       "      <td>0.114014</td>\n",
       "      <td>0.939976</td>\n",
       "      <td>0.203362</td>\n",
       "      <td>0.299146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5721</td>\n",
       "      <td>213689</td>\n",
       "      <td>87789</td>\n",
       "      <td>297</td>\n",
       "      <td>0.714145</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>0.95063</td>\n",
       "      <td>0.115166</td>\n",
       "      <td>0.198854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5779</td>\n",
       "      <td>261411</td>\n",
       "      <td>40067</td>\n",
       "      <td>239</td>\n",
       "      <td>0.869434</td>\n",
       "      <td>0.126096</td>\n",
       "      <td>0.960264</td>\n",
       "      <td>0.222919</td>\n",
       "      <td>0.321932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>2425</td>\n",
       "      <td>203328</td>\n",
       "      <td>98150</td>\n",
       "      <td>3593</td>\n",
       "      <td>0.660005</td>\n",
       "      <td>0.025915</td>\n",
       "      <td>0.40367</td>\n",
       "      <td>0.048704</td>\n",
       "      <td>0.021320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>80584</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.918923</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.360620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4997</td>\n",
       "      <td>77824</td>\n",
       "      <td>6598</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.430647</td>\n",
       "      <td>0.830352</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>0.560655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4950</td>\n",
       "      <td>78375</td>\n",
       "      <td>6047</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.921556</td>\n",
       "      <td>0.449796</td>\n",
       "      <td>0.822501</td>\n",
       "      <td>0.581558</td>\n",
       "      <td>0.572522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5100</td>\n",
       "      <td>72437</td>\n",
       "      <td>11985</td>\n",
       "      <td>918</td>\n",
       "      <td>0.857577</td>\n",
       "      <td>0.298173</td>\n",
       "      <td>0.847308</td>\n",
       "      <td>0.441115</td>\n",
       "      <td>0.448986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5751</td>\n",
       "      <td>30442</td>\n",
       "      <td>53980</td>\n",
       "      <td>267</td>\n",
       "      <td>0.406414</td>\n",
       "      <td>0.098079</td>\n",
       "      <td>0.954641</td>\n",
       "      <td>0.177883</td>\n",
       "      <td>0.169389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4817</td>\n",
       "      <td>285372</td>\n",
       "      <td>16106</td>\n",
       "      <td>1201</td>\n",
       "      <td>0.943881</td>\n",
       "      <td>0.229869</td>\n",
       "      <td>0.800296</td>\n",
       "      <td>0.357153</td>\n",
       "      <td>0.410611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5032</td>\n",
       "      <td>291795</td>\n",
       "      <td>9683</td>\n",
       "      <td>986</td>\n",
       "      <td>0.965425</td>\n",
       "      <td>0.341713</td>\n",
       "      <td>0.836223</td>\n",
       "      <td>0.485168</td>\n",
       "      <td>0.521648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5270</td>\n",
       "      <td>82675</td>\n",
       "      <td>218803</td>\n",
       "      <td>748</td>\n",
       "      <td>0.289631</td>\n",
       "      <td>0.023833</td>\n",
       "      <td>0.875709</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.047840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5168</td>\n",
       "      <td>288985</td>\n",
       "      <td>12493</td>\n",
       "      <td>850</td>\n",
       "      <td>0.956723</td>\n",
       "      <td>0.292504</td>\n",
       "      <td>0.858663</td>\n",
       "      <td>0.436361</td>\n",
       "      <td>0.486502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6015</td>\n",
       "      <td>222433</td>\n",
       "      <td>79045</td>\n",
       "      <td>3</td>\n",
       "      <td>1.399868</td>\n",
       "      <td>0.068243</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>0.127762</td>\n",
       "      <td>-0.307781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4149</td>\n",
       "      <td>80988</td>\n",
       "      <td>3434</td>\n",
       "      <td>1869</td>\n",
       "      <td>0.941364</td>\n",
       "      <td>0.547145</td>\n",
       "      <td>0.689432</td>\n",
       "      <td>0.610102</td>\n",
       "      <td>0.583377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4092</td>\n",
       "      <td>80554</td>\n",
       "      <td>3868</td>\n",
       "      <td>1926</td>\n",
       "      <td>0.935935</td>\n",
       "      <td>0.51407</td>\n",
       "      <td>0.67996</td>\n",
       "      <td>0.585491</td>\n",
       "      <td>0.557840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>2976</td>\n",
       "      <td>82972</td>\n",
       "      <td>1450</td>\n",
       "      <td>3042</td>\n",
       "      <td>0.950332</td>\n",
       "      <td>0.67239</td>\n",
       "      <td>0.494516</td>\n",
       "      <td>0.569897</td>\n",
       "      <td>0.551432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5323</td>\n",
       "      <td>47507</td>\n",
       "      <td>36915</td>\n",
       "      <td>695</td>\n",
       "      <td>0.584144</td>\n",
       "      <td>0.126024</td>\n",
       "      <td>0.884513</td>\n",
       "      <td>0.220615</td>\n",
       "      <td>0.223417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4478</td>\n",
       "      <td>77802</td>\n",
       "      <td>6620</td>\n",
       "      <td>1540</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.403496</td>\n",
       "      <td>0.744101</td>\n",
       "      <td>0.523253</td>\n",
       "      <td>0.505649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4267</td>\n",
       "      <td>78129</td>\n",
       "      <td>6293</td>\n",
       "      <td>1751</td>\n",
       "      <td>0.911057</td>\n",
       "      <td>0.404072</td>\n",
       "      <td>0.70904</td>\n",
       "      <td>0.514779</td>\n",
       "      <td>0.492417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4934</td>\n",
       "      <td>79490</td>\n",
       "      <td>4932</td>\n",
       "      <td>1084</td>\n",
       "      <td>0.933481</td>\n",
       "      <td>0.500101</td>\n",
       "      <td>0.819874</td>\n",
       "      <td>0.621254</td>\n",
       "      <td>0.608736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5425</td>\n",
       "      <td>46582</td>\n",
       "      <td>37840</td>\n",
       "      <td>593</td>\n",
       "      <td>0.575044</td>\n",
       "      <td>0.12539</td>\n",
       "      <td>0.901462</td>\n",
       "      <td>0.220157</td>\n",
       "      <td>0.226129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tp      tn      fp    fn (hybrid)accuracy  \\\n",
       "experiment                class                                                \n",
       "max_u_regressor_sparse    lr     3011  298021    4439  2025         0.979047   \n",
       "                          gb     3361  299916    2544  1675         0.986368   \n",
       "                          xgb    3983   97550  204910  1053         0.338473   \n",
       "                          svr    3885  290445   12015  1151         0.960099   \n",
       "                          mlp    5036  211703   90757     0         1.466242   \n",
       "max_u_regressor_focused   lr     4089  298466    3994   947         0.983766   \n",
       "                          gb     4416  252698   49762   620         0.836891   \n",
       "                          xgb    4700   71618  230842   336         0.256491   \n",
       "                          svr    4300  275628   26832   736          0.91493   \n",
       "                          mlp    3247  194204  108256  1789         0.631347   \n",
       "max_u_filtered_regressor  lr     3011   80965    4439  2025         0.928714   \n",
       "                          gb     3260   83067    2337  1776         0.954708   \n",
       "                          xgb    4315   71932   13472   721         0.843229   \n",
       "                          svr    3613   75525    9879  1423         0.875203   \n",
       "                          mlp    5036      22   85382     0         0.054443   \n",
       "max_u_regressor_balanced  lr     4513  282576   19884   523         0.933826   \n",
       "                          gb     4109  297630    4830   927         0.981642   \n",
       "                          xgb    4242   87755  214705   794         0.304284   \n",
       "                          svr    3974  290777   11683  1062         0.959239   \n",
       "                          mlp    5008  226465   75995    28         1.008493   \n",
       "max_u_classifier          lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     2598   83124    2280  2438         0.947833   \n",
       "                          xgb    1867   84013    1391  3169          0.94958   \n",
       "                          svr       0   85404       0  5036         0.944317   \n",
       "                          mlp    3990   59437   25967  1046         0.701316   \n",
       "max_u_classifier_balanced lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     3546   81683    3721  1490         0.942382   \n",
       "                          xgb    3180   83080    2324  1856         0.953782   \n",
       "                          svr    4029   80498    4906  1007          0.93462   \n",
       "                          mlp    3967   49322   36082  1069         0.589219   \n",
       "min_u_regressor_sparse    lr     2502  297640    3838  3516         0.976169   \n",
       "                          gb     5039  294700    6778   979         0.974861   \n",
       "                          xgb    4781  293809    7669  1237         0.971135   \n",
       "                          svr    5208  288486   12992   810         0.955228   \n",
       "                          mlp    6011  229483   71995     7         1.023041   \n",
       "min_u_regressor_focused   lr     5929  242474   59004    89         0.808951   \n",
       "                          gb     5657  257440   44038   361         0.856304   \n",
       "                          xgb    5721  213689   87789   297         0.714145   \n",
       "                          svr    5779  261411   40067   239         0.869434   \n",
       "                          mlp    2425  203328   98150  3593         0.660005   \n",
       "min_u_filtered_regressor  lr     2502   80584    3838  3516         0.918923   \n",
       "                          gb     4997   77824    6598  1021         0.916002   \n",
       "                          xgb    4950   78375    6047  1068         0.921556   \n",
       "                          svr    5100   72437   11985   918         0.857577   \n",
       "                          mlp    5751   30442   53980   267         0.406414   \n",
       "min_u_regressor_balanced  lr     4817  285372   16106  1201         0.943881   \n",
       "                          gb     5032  291795    9683   986         0.965425   \n",
       "                          xgb    5270   82675  218803   748         0.289631   \n",
       "                          svr    5168  288985   12493   850         0.956723   \n",
       "                          mlp    6015  222433   79045     3         1.399868   \n",
       "min_u_classifier          lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     4149   80988    3434  1869         0.941364   \n",
       "                          xgb    4092   80554    3868  1926         0.935935   \n",
       "                          svr    2976   82972    1450  3042         0.950332   \n",
       "                          mlp    5323   47507   36915   695         0.584144   \n",
       "min_u_classifier_balanced lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     4478   77802    6620  1540         0.909774   \n",
       "                          xgb    4267   78129    6293  1751         0.911057   \n",
       "                          svr    4934   79490    4932  1084         0.933481   \n",
       "                          mlp    5425   46582   37840   593         0.575044   \n",
       "\n",
       "                                (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment                class                                               \n",
       "max_u_regressor_sparse    lr             0.402003       0.597092   0.480501   \n",
       "                          gb             0.568695       0.666644   0.613786   \n",
       "                          xgb             0.01963       0.790252   0.038308   \n",
       "                          svr            0.257948       0.770778   0.386537   \n",
       "                          mlp            0.052237            1.0   0.099287   \n",
       "max_u_regressor_focused   lr             0.505694        0.81232    0.62334   \n",
       "                          gb             0.081427       0.876656   0.149013   \n",
       "                          xgb            0.020734       0.933189   0.040567   \n",
       "                          svr            0.144597        0.85364   0.247304   \n",
       "                          mlp            0.030686       0.645414   0.058587   \n",
       "max_u_filtered_regressor  lr             0.402003       0.597092   0.480501   \n",
       "                          gb             0.581068       0.646568   0.612071   \n",
       "                          xgb            0.241617        0.85642   0.376901   \n",
       "                          svr            0.266493       0.716784   0.388533   \n",
       "                          mlp             0.05415            1.0   0.102736   \n",
       "max_u_regressor_balanced  lr             0.184598       0.895985   0.306125   \n",
       "                          gb             0.463762       0.815659   0.591317   \n",
       "                          xgb            0.019722       0.842058   0.038541   \n",
       "                          svr            0.258391       0.789028   0.389296   \n",
       "                          mlp            0.061813       0.994026   0.116388   \n",
       "max_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.532595       0.515886   0.524107   \n",
       "                          xgb            0.573051       0.370731   0.450205   \n",
       "                          svr                   0            0.0          0   \n",
       "                          mlp            0.133191       0.792295   0.228046   \n",
       "max_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.487959        0.70413   0.576445   \n",
       "                          xgb            0.577762       0.631454   0.603416   \n",
       "                          svr            0.450923        0.80004   0.576766   \n",
       "                          mlp            0.099054       0.787728   0.175979   \n",
       "min_u_regressor_sparse    lr             0.393263       0.415191   0.403929   \n",
       "                          gb             0.426091       0.837325   0.564781   \n",
       "                          xgb            0.383609       0.794475   0.517396   \n",
       "                          svr            0.285838       0.865268    0.42972   \n",
       "                          mlp            0.076957       0.998749   0.142903   \n",
       "min_u_regressor_focused   lr             0.091561       0.985199   0.167551   \n",
       "                          gb             0.114014       0.939976   0.203362   \n",
       "                          xgb            0.061296        0.95063   0.115166   \n",
       "                          svr            0.126096       0.960264   0.222919   \n",
       "                          mlp            0.025915        0.40367   0.048704   \n",
       "min_u_filtered_regressor  lr             0.393263       0.415191   0.403929   \n",
       "                          gb             0.430647       0.830352   0.567151   \n",
       "                          xgb            0.449796       0.822501   0.581558   \n",
       "                          svr            0.298173       0.847308   0.441115   \n",
       "                          mlp            0.098079       0.954641   0.177883   \n",
       "min_u_regressor_balanced  lr             0.229869       0.800296   0.357153   \n",
       "                          gb             0.341713       0.836223   0.485168   \n",
       "                          xgb            0.023833       0.875709   0.046404   \n",
       "                          svr            0.292504       0.858663   0.436361   \n",
       "                          mlp            0.068243       0.999402   0.127762   \n",
       "min_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.547145       0.689432   0.610102   \n",
       "                          xgb             0.51407        0.67996   0.585491   \n",
       "                          svr             0.67239       0.494516   0.569897   \n",
       "                          mlp            0.126024       0.884513   0.220615   \n",
       "min_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.403496       0.744101   0.523253   \n",
       "                          xgb            0.404072        0.70904   0.514779   \n",
       "                          svr            0.500101       0.819874   0.621254   \n",
       "                          mlp             0.12539       0.901462   0.220157   \n",
       "\n",
       "                                 (hybrid)mcc  \n",
       "experiment                class               \n",
       "max_u_regressor_sparse    lr        0.479834  \n",
       "                          gb        0.608879  \n",
       "                          xgb       0.032998  \n",
       "                          svr       0.431802  \n",
       "                          mlp      -0.275648  \n",
       "max_u_regressor_focused   lr        0.633630  \n",
       "                          gb        0.237300  \n",
       "                          xgb       0.053516  \n",
       "                          svr       0.330513  \n",
       "                          mlp       0.075512  \n",
       "max_u_filtered_regressor  lr        0.453823  \n",
       "                          gb        0.589039  \n",
       "                          xgb       0.402428  \n",
       "                          svr       0.386221  \n",
       "                          mlp       0.004214  \n",
       "max_u_regressor_balanced  lr        0.389575  \n",
       "                          gb        0.607091  \n",
       "                          xgb       0.038495  \n",
       "                          svr       0.437418  \n",
       "                          mlp      -0.248941  \n",
       "max_u_classifier          lr             NaN  \n",
       "                          gb        0.496589  \n",
       "                          xgb       0.436154  \n",
       "                          svr       0.436154  \n",
       "                          mlp       0.237879  \n",
       "max_u_classifier_balanced lr             NaN  \n",
       "                          gb        0.557219  \n",
       "                          xgb       0.579572  \n",
       "                          svr       0.570683  \n",
       "                          mlp       0.168613  \n",
       "min_u_regressor_sparse    lr        0.391930  \n",
       "                          gb        0.586979  \n",
       "                          xgb       0.540216  \n",
       "                          svr       0.482436  \n",
       "                          mlp      -0.280415  \n",
       "min_u_regressor_focused   lr        0.268520  \n",
       "                          gb        0.299146  \n",
       "                          xgb       0.198854  \n",
       "                          svr       0.321932  \n",
       "                          mlp       0.021320  \n",
       "min_u_filtered_regressor  lr        0.360620  \n",
       "                          gb        0.560655  \n",
       "                          xgb       0.572522  \n",
       "                          svr       0.448986  \n",
       "                          mlp       0.169389  \n",
       "min_u_regressor_balanced  lr        0.410611  \n",
       "                          gb        0.521648  \n",
       "                          xgb       0.047840  \n",
       "                          svr       0.486502  \n",
       "                          mlp      -0.307781  \n",
       "min_u_classifier          lr             NaN  \n",
       "                          gb        0.583377  \n",
       "                          xgb       0.557840  \n",
       "                          svr       0.551432  \n",
       "                          mlp       0.223417  \n",
       "min_u_classifier_balanced lr             NaN  \n",
       "                          gb        0.505649  \n",
       "                          xgb       0.492417  \n",
       "                          svr       0.608736  \n",
       "                          mlp       0.226129  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>possible_positives</th>\n",
       "      <th>possible_negatives</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                possible_positives possible_negatives\n",
       "experiment                class                                      \n",
       "max_u_regressor_sparse    lr                  5036             302460\n",
       "                          gb                  5036             302460\n",
       "                          xgb                 5036             302460\n",
       "                          svr                 5036             302460\n",
       "                          mlp                 5036             302460\n",
       "max_u_regressor_focused   lr                  5036             302460\n",
       "                          gb                  5036             302460\n",
       "                          xgb                 5036             302460\n",
       "                          svr                 5036             302460\n",
       "                          mlp                 5036             302460\n",
       "max_u_filtered_regressor  lr                  5036              85404\n",
       "                          gb                  5036              85404\n",
       "                          xgb                 5036              85404\n",
       "                          svr                 5036              85404\n",
       "                          mlp                 5036              85404\n",
       "max_u_regressor_balanced  lr                  5036             302460\n",
       "                          gb                  5036             302460\n",
       "                          xgb                 5036             302460\n",
       "                          svr                 5036             302460\n",
       "                          mlp                 5036             302460\n",
       "max_u_classifier          lr                   NaN                NaN\n",
       "                          gb                  5036              85404\n",
       "                          xgb                 5036              85404\n",
       "                          svr                 5036              85404\n",
       "                          mlp                 5036              85404\n",
       "max_u_classifier_balanced lr                   NaN                NaN\n",
       "                          gb                  5036              85404\n",
       "                          xgb                 5036              85404\n",
       "                          svr                 5036              85404\n",
       "                          mlp                 5036              85404\n",
       "min_u_regressor_sparse    lr                  6018             301478\n",
       "                          gb                  6018             301478\n",
       "                          xgb                 6018             301478\n",
       "                          svr                 6018             301478\n",
       "                          mlp                 6018             301478\n",
       "min_u_regressor_focused   lr                  6018             301478\n",
       "                          gb                  6018             301478\n",
       "                          xgb                 6018             301478\n",
       "                          svr                 6018             301478\n",
       "                          mlp                 6018             301478\n",
       "min_u_filtered_regressor  lr                  6018              84422\n",
       "                          gb                  6018              84422\n",
       "                          xgb                 6018              84422\n",
       "                          svr                 6018              84422\n",
       "                          mlp                 6018              84422\n",
       "min_u_regressor_balanced  lr                  6018             301478\n",
       "                          gb                  6018             301478\n",
       "                          xgb                 6018             301478\n",
       "                          svr                 6018             301478\n",
       "                          mlp                 6018             301478\n",
       "min_u_classifier          lr                   NaN                NaN\n",
       "                          gb                  6018              84422\n",
       "                          xgb                 6018              84422\n",
       "                          svr                 6018              84422\n",
       "                          mlp                 6018              84422\n",
       "min_u_classifier_balanced lr                   NaN                NaN\n",
       "                          gb                  6018              84422\n",
       "                          xgb                 6018              84422\n",
       "                          svr                 6018              84422\n",
       "                          mlp                 6018              84422"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmation_df = pd.DataFrame()\n",
    "confirmation_df['possible_positives'] = df['tp'] + df['fn']\n",
    "confirmation_df['possible_negatives'] = df['fp'] + df['tn']\n",
    "confirmation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to evaluate all the results obtained above. This benchmarking has the objective of obtaining the answer following questions:\n",
    "- What is the optimum number of rows for the training data set? What is the respective model?\n",
    "    - sparse reg. vs balanced reg. vs focused reg.\n",
    "- What is the optimum number present busses in regresison?\n",
    "    - sparse reg. vs filtered reg.\n",
    "- Regression vs Classification\n",
    "    - filtered reg. vs sparse class.\n",
    "- What is the optimum number of rows in class?\n",
    "    - sparse class. vs balanced class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimum proportion of P/N rows for the training data set? What is the respective model?\n",
    "In order to understand the optinum number of rows for the training set of the regression data set the data sets used will be:\n",
    "\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Maximum Voltage Constraints|Balanced Regression|6971|34|\n",
    "|Maximum Voltage Constraints|Focused Regression|3486|34|\n",
    "|Minimum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Minimum Voltage Constraints|Balanced Regression|13917|34|\n",
    "|Minimum Voltage Constraints|Focused Regression|6958|34|\n",
    "\n",
    "The **Sparse Regression data set** is generated directly from the power flow results. The important moments are those where the constraints are violated, so the output feature contains null values for when there is no constraint, and positive value for when there is a constraint. The positive values represent the amplitude of the constraint violation. It can be expressed as follows:\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\text{Target} &= \\begin{cases}\n",
    "            0 & \\text{if} \\; \\text{constraint} \\; \\text{is not violated} \\\\\n",
    "            \\text{amplitude of constraint} & \\text{if} \\; \\text{constraint} \\; \\text{is violated} \\\\\n",
    "        \\end{cases}\n",
    "    \\end{align}\n",
    "$$\n",
    "In our case, the constraints are being considered as the following:\n",
    "- Minimal voltage on bus: $v_bus < 0.95 \\text{ [pu]}$ (constraint is violated if the voltage is below $0.95 \\text{ [pu]} $)\n",
    "- Maximal voltage on bus: $v_bus > 1.05 \\text{ [pu]}$ (constraint is violated if the voltage is above $1.05 \\text{ [pu]} $)\n",
    "- Maximal current on line: $i_{line} > 1 \\text{ [kA]}$ (constraint is violated if the current is above $1 \\text{ [kA]} $)\n",
    "\n",
    "The **Balanced Regression** data set is created from the **Sparse Regression data set**. It is created by taking all the rows that containt at least one constraint violation and then taking the same number of rows that do not contain any constraint violation. Finally, the **Focused Regression data set** is created by taking all the rows that contain at least one constraint violation.\n",
    "\n",
    "Since these data sets have the same number of possible negative and possible positives, all the metrics can be used to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>298021</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.979047</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.479834</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3361</td>\n",
       "      <td>299916</td>\n",
       "      <td>2544</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>0.568695</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.613786</td>\n",
       "      <td>0.608879</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3983</td>\n",
       "      <td>97550</td>\n",
       "      <td>204910</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.338473</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.790252</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3885</td>\n",
       "      <td>290445</td>\n",
       "      <td>12015</td>\n",
       "      <td>1151</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>0.257948</td>\n",
       "      <td>0.770778</td>\n",
       "      <td>0.386537</td>\n",
       "      <td>0.431802</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>211703</td>\n",
       "      <td>90757</td>\n",
       "      <td>0</td>\n",
       "      <td>1.466242</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099287</td>\n",
       "      <td>-0.275648</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4513</td>\n",
       "      <td>282576</td>\n",
       "      <td>19884</td>\n",
       "      <td>523</td>\n",
       "      <td>0.933826</td>\n",
       "      <td>0.184598</td>\n",
       "      <td>0.895985</td>\n",
       "      <td>0.306125</td>\n",
       "      <td>0.389575</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4109</td>\n",
       "      <td>297630</td>\n",
       "      <td>4830</td>\n",
       "      <td>927</td>\n",
       "      <td>0.981642</td>\n",
       "      <td>0.463762</td>\n",
       "      <td>0.815659</td>\n",
       "      <td>0.591317</td>\n",
       "      <td>0.607091</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4242</td>\n",
       "      <td>87755</td>\n",
       "      <td>214705</td>\n",
       "      <td>794</td>\n",
       "      <td>0.304284</td>\n",
       "      <td>0.019722</td>\n",
       "      <td>0.842058</td>\n",
       "      <td>0.038541</td>\n",
       "      <td>0.038495</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3974</td>\n",
       "      <td>290777</td>\n",
       "      <td>11683</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.959239</td>\n",
       "      <td>0.258391</td>\n",
       "      <td>0.789028</td>\n",
       "      <td>0.389296</td>\n",
       "      <td>0.437418</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5008</td>\n",
       "      <td>226465</td>\n",
       "      <td>75995</td>\n",
       "      <td>28</td>\n",
       "      <td>1.008493</td>\n",
       "      <td>0.061813</td>\n",
       "      <td>0.994026</td>\n",
       "      <td>0.116388</td>\n",
       "      <td>-0.248941</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>4089</td>\n",
       "      <td>298466</td>\n",
       "      <td>3994</td>\n",
       "      <td>947</td>\n",
       "      <td>0.983766</td>\n",
       "      <td>0.505694</td>\n",
       "      <td>0.81232</td>\n",
       "      <td>0.62334</td>\n",
       "      <td>0.633630</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4416</td>\n",
       "      <td>252698</td>\n",
       "      <td>49762</td>\n",
       "      <td>620</td>\n",
       "      <td>0.836891</td>\n",
       "      <td>0.081427</td>\n",
       "      <td>0.876656</td>\n",
       "      <td>0.149013</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4700</td>\n",
       "      <td>71618</td>\n",
       "      <td>230842</td>\n",
       "      <td>336</td>\n",
       "      <td>0.256491</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.933189</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>0.053516</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4300</td>\n",
       "      <td>275628</td>\n",
       "      <td>26832</td>\n",
       "      <td>736</td>\n",
       "      <td>0.91493</td>\n",
       "      <td>0.144597</td>\n",
       "      <td>0.85364</td>\n",
       "      <td>0.247304</td>\n",
       "      <td>0.330513</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3247</td>\n",
       "      <td>194204</td>\n",
       "      <td>108256</td>\n",
       "      <td>1789</td>\n",
       "      <td>0.631347</td>\n",
       "      <td>0.030686</td>\n",
       "      <td>0.645414</td>\n",
       "      <td>0.058587</td>\n",
       "      <td>0.075512</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp      tn      fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                                \n",
       "max_u_regressor_sparse   lr     3011  298021    4439  2025         0.979047   \n",
       "                         gb     3361  299916    2544  1675         0.986368   \n",
       "                         xgb    3983   97550  204910  1053         0.338473   \n",
       "                         svr    3885  290445   12015  1151         0.960099   \n",
       "                         mlp    5036  211703   90757     0         1.466242   \n",
       "max_u_regressor_balanced lr     4513  282576   19884   523         0.933826   \n",
       "                         gb     4109  297630    4830   927         0.981642   \n",
       "                         xgb    4242   87755  214705   794         0.304284   \n",
       "                         svr    3974  290777   11683  1062         0.959239   \n",
       "                         mlp    5008  226465   75995    28         1.008493   \n",
       "max_u_regressor_focused  lr     4089  298466    3994   947         0.983766   \n",
       "                         gb     4416  252698   49762   620         0.836891   \n",
       "                         xgb    4700   71618  230842   336         0.256491   \n",
       "                         svr    4300  275628   26832   736          0.91493   \n",
       "                         mlp    3247  194204  108256  1789         0.631347   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "max_u_regressor_sparse   lr             0.402003       0.597092   0.480501   \n",
       "                         gb             0.568695       0.666644   0.613786   \n",
       "                         xgb             0.01963       0.790252   0.038308   \n",
       "                         svr            0.257948       0.770778   0.386537   \n",
       "                         mlp            0.052237            1.0   0.099287   \n",
       "max_u_regressor_balanced lr             0.184598       0.895985   0.306125   \n",
       "                         gb             0.463762       0.815659   0.591317   \n",
       "                         xgb            0.019722       0.842058   0.038541   \n",
       "                         svr            0.258391       0.789028   0.389296   \n",
       "                         mlp            0.061813       0.994026   0.116388   \n",
       "max_u_regressor_focused  lr             0.505694        0.81232    0.62334   \n",
       "                         gb             0.081427       0.876656   0.149013   \n",
       "                         xgb            0.020734       0.933189   0.040567   \n",
       "                         svr            0.144597        0.85364   0.247304   \n",
       "                         mlp            0.030686       0.645414   0.058587   \n",
       "\n",
       "                                (hybrid)mcc         q   f1_coin  \n",
       "experiment               class                                   \n",
       "max_u_regressor_sparse   lr        0.479834  0.016377  0.032227  \n",
       "                         gb        0.608879  0.016377  0.032227  \n",
       "                         xgb       0.032998  0.016377  0.032227  \n",
       "                         svr       0.431802  0.016377  0.032227  \n",
       "                         mlp      -0.275648  0.016377  0.032227  \n",
       "max_u_regressor_balanced lr        0.389575  0.016377  0.032227  \n",
       "                         gb        0.607091  0.016377  0.032227  \n",
       "                         xgb       0.038495  0.016377  0.032227  \n",
       "                         svr       0.437418  0.016377  0.032227  \n",
       "                         mlp      -0.248941  0.016377  0.032227  \n",
       "max_u_regressor_focused  lr        0.633630  0.016377  0.032227  \n",
       "                         gb        0.237300  0.016377  0.032227  \n",
       "                         xgb       0.053516  0.016377  0.032227  \n",
       "                         svr       0.330513  0.016377  0.032227  \n",
       "                         mlp       0.075512  0.016377  0.032227  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['max_u_regressor_sparse', 'max_u_regressor_balanced', 'max_u_regressor_focused']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best alternative is to use the focused data set with a the linear regression model, because it presents a the best values for F1 and MCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>297640</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.976169</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.391930</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5039</td>\n",
       "      <td>294700</td>\n",
       "      <td>6778</td>\n",
       "      <td>979</td>\n",
       "      <td>0.974861</td>\n",
       "      <td>0.426091</td>\n",
       "      <td>0.837325</td>\n",
       "      <td>0.564781</td>\n",
       "      <td>0.586979</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4781</td>\n",
       "      <td>293809</td>\n",
       "      <td>7669</td>\n",
       "      <td>1237</td>\n",
       "      <td>0.971135</td>\n",
       "      <td>0.383609</td>\n",
       "      <td>0.794475</td>\n",
       "      <td>0.517396</td>\n",
       "      <td>0.540216</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5208</td>\n",
       "      <td>288486</td>\n",
       "      <td>12992</td>\n",
       "      <td>810</td>\n",
       "      <td>0.955228</td>\n",
       "      <td>0.285838</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>0.42972</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6011</td>\n",
       "      <td>229483</td>\n",
       "      <td>71995</td>\n",
       "      <td>7</td>\n",
       "      <td>1.023041</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>-0.280415</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4817</td>\n",
       "      <td>285372</td>\n",
       "      <td>16106</td>\n",
       "      <td>1201</td>\n",
       "      <td>0.943881</td>\n",
       "      <td>0.229869</td>\n",
       "      <td>0.800296</td>\n",
       "      <td>0.357153</td>\n",
       "      <td>0.410611</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5032</td>\n",
       "      <td>291795</td>\n",
       "      <td>9683</td>\n",
       "      <td>986</td>\n",
       "      <td>0.965425</td>\n",
       "      <td>0.341713</td>\n",
       "      <td>0.836223</td>\n",
       "      <td>0.485168</td>\n",
       "      <td>0.521648</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5270</td>\n",
       "      <td>82675</td>\n",
       "      <td>218803</td>\n",
       "      <td>748</td>\n",
       "      <td>0.289631</td>\n",
       "      <td>0.023833</td>\n",
       "      <td>0.875709</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.047840</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5168</td>\n",
       "      <td>288985</td>\n",
       "      <td>12493</td>\n",
       "      <td>850</td>\n",
       "      <td>0.956723</td>\n",
       "      <td>0.292504</td>\n",
       "      <td>0.858663</td>\n",
       "      <td>0.436361</td>\n",
       "      <td>0.486502</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6015</td>\n",
       "      <td>222433</td>\n",
       "      <td>79045</td>\n",
       "      <td>3</td>\n",
       "      <td>1.399868</td>\n",
       "      <td>0.068243</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>0.127762</td>\n",
       "      <td>-0.307781</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>5929</td>\n",
       "      <td>242474</td>\n",
       "      <td>59004</td>\n",
       "      <td>89</td>\n",
       "      <td>0.808951</td>\n",
       "      <td>0.091561</td>\n",
       "      <td>0.985199</td>\n",
       "      <td>0.167551</td>\n",
       "      <td>0.268520</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5657</td>\n",
       "      <td>257440</td>\n",
       "      <td>44038</td>\n",
       "      <td>361</td>\n",
       "      <td>0.856304</td>\n",
       "      <td>0.114014</td>\n",
       "      <td>0.939976</td>\n",
       "      <td>0.203362</td>\n",
       "      <td>0.299146</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5721</td>\n",
       "      <td>213689</td>\n",
       "      <td>87789</td>\n",
       "      <td>297</td>\n",
       "      <td>0.714145</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>0.95063</td>\n",
       "      <td>0.115166</td>\n",
       "      <td>0.198854</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5779</td>\n",
       "      <td>261411</td>\n",
       "      <td>40067</td>\n",
       "      <td>239</td>\n",
       "      <td>0.869434</td>\n",
       "      <td>0.126096</td>\n",
       "      <td>0.960264</td>\n",
       "      <td>0.222919</td>\n",
       "      <td>0.321932</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>2425</td>\n",
       "      <td>203328</td>\n",
       "      <td>98150</td>\n",
       "      <td>3593</td>\n",
       "      <td>0.660005</td>\n",
       "      <td>0.025915</td>\n",
       "      <td>0.40367</td>\n",
       "      <td>0.048704</td>\n",
       "      <td>0.021320</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp      tn      fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                                \n",
       "min_u_regressor_sparse   lr     2502  297640    3838  3516         0.976169   \n",
       "                         gb     5039  294700    6778   979         0.974861   \n",
       "                         xgb    4781  293809    7669  1237         0.971135   \n",
       "                         svr    5208  288486   12992   810         0.955228   \n",
       "                         mlp    6011  229483   71995     7         1.023041   \n",
       "min_u_regressor_balanced lr     4817  285372   16106  1201         0.943881   \n",
       "                         gb     5032  291795    9683   986         0.965425   \n",
       "                         xgb    5270   82675  218803   748         0.289631   \n",
       "                         svr    5168  288985   12493   850         0.956723   \n",
       "                         mlp    6015  222433   79045     3         1.399868   \n",
       "min_u_regressor_focused  lr     5929  242474   59004    89         0.808951   \n",
       "                         gb     5657  257440   44038   361         0.856304   \n",
       "                         xgb    5721  213689   87789   297         0.714145   \n",
       "                         svr    5779  261411   40067   239         0.869434   \n",
       "                         mlp    2425  203328   98150  3593         0.660005   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "min_u_regressor_sparse   lr             0.393263       0.415191   0.403929   \n",
       "                         gb             0.426091       0.837325   0.564781   \n",
       "                         xgb            0.383609       0.794475   0.517396   \n",
       "                         svr            0.285838       0.865268    0.42972   \n",
       "                         mlp            0.076957       0.998749   0.142903   \n",
       "min_u_regressor_balanced lr             0.229869       0.800296   0.357153   \n",
       "                         gb             0.341713       0.836223   0.485168   \n",
       "                         xgb            0.023833       0.875709   0.046404   \n",
       "                         svr            0.292504       0.858663   0.436361   \n",
       "                         mlp            0.068243       0.999402   0.127762   \n",
       "min_u_regressor_focused  lr             0.091561       0.985199   0.167551   \n",
       "                         gb             0.114014       0.939976   0.203362   \n",
       "                         xgb            0.061296        0.95063   0.115166   \n",
       "                         svr            0.126096       0.960264   0.222919   \n",
       "                         mlp            0.025915        0.40367   0.048704   \n",
       "\n",
       "                                (hybrid)mcc         q   f1_coin  \n",
       "experiment               class                                   \n",
       "min_u_regressor_sparse   lr        0.391930  0.019571  0.038391  \n",
       "                         gb        0.586979  0.019571  0.038391  \n",
       "                         xgb       0.540216  0.019571  0.038391  \n",
       "                         svr       0.482436  0.019571  0.038391  \n",
       "                         mlp      -0.280415  0.019571  0.038391  \n",
       "min_u_regressor_balanced lr        0.410611  0.019571  0.038391  \n",
       "                         gb        0.521648  0.019571  0.038391  \n",
       "                         xgb       0.047840  0.019571  0.038391  \n",
       "                         svr       0.486502  0.019571  0.038391  \n",
       "                         mlp      -0.307781  0.019571  0.038391  \n",
       "min_u_regressor_focused  lr        0.268520  0.019571  0.038391  \n",
       "                         gb        0.299146  0.019571  0.038391  \n",
       "                         xgb       0.198854  0.019571  0.038391  \n",
       "                         svr       0.321932  0.019571  0.038391  \n",
       "                         mlp       0.021320  0.019571  0.038391  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['min_u_regressor_sparse', 'min_u_regressor_balanced', 'min_u_regressor_focused']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best sparse, with gradient boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimum number present busses in regresison?\n",
    "In order to understand the optinum number of busses for the training set of the regression data set the data sets used will be:\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Maximum Voltage Constraints|Filtered Regression|45216|10|\n",
    "|Minimum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Minimum Voltage Constraints|Filtered Regression|45216|10|\n",
    "\n",
    "The **Filtered Regression data set** is created from the Sparse Regression data set, but only keeping the columns that contain at least one time step with a constraint violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>298021</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.979047</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.479834</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3361</td>\n",
       "      <td>299916</td>\n",
       "      <td>2544</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>0.568695</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.613786</td>\n",
       "      <td>0.608879</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3983</td>\n",
       "      <td>97550</td>\n",
       "      <td>204910</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.338473</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.790252</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3885</td>\n",
       "      <td>290445</td>\n",
       "      <td>12015</td>\n",
       "      <td>1151</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>0.257948</td>\n",
       "      <td>0.770778</td>\n",
       "      <td>0.386537</td>\n",
       "      <td>0.431802</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>211703</td>\n",
       "      <td>90757</td>\n",
       "      <td>0</td>\n",
       "      <td>1.466242</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099287</td>\n",
       "      <td>-0.275648</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>80965</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.453823</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3260</td>\n",
       "      <td>83067</td>\n",
       "      <td>2337</td>\n",
       "      <td>1776</td>\n",
       "      <td>0.954708</td>\n",
       "      <td>0.581068</td>\n",
       "      <td>0.646568</td>\n",
       "      <td>0.612071</td>\n",
       "      <td>0.589039</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4315</td>\n",
       "      <td>71932</td>\n",
       "      <td>13472</td>\n",
       "      <td>721</td>\n",
       "      <td>0.843229</td>\n",
       "      <td>0.241617</td>\n",
       "      <td>0.85642</td>\n",
       "      <td>0.376901</td>\n",
       "      <td>0.402428</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3613</td>\n",
       "      <td>75525</td>\n",
       "      <td>9879</td>\n",
       "      <td>1423</td>\n",
       "      <td>0.875203</td>\n",
       "      <td>0.266493</td>\n",
       "      <td>0.716784</td>\n",
       "      <td>0.388533</td>\n",
       "      <td>0.386221</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>22</td>\n",
       "      <td>85382</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.05415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp      tn      fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                                \n",
       "max_u_regressor_sparse   lr     3011  298021    4439  2025         0.979047   \n",
       "                         gb     3361  299916    2544  1675         0.986368   \n",
       "                         xgb    3983   97550  204910  1053         0.338473   \n",
       "                         svr    3885  290445   12015  1151         0.960099   \n",
       "                         mlp    5036  211703   90757     0         1.466242   \n",
       "max_u_filtered_regressor lr     3011   80965    4439  2025         0.928714   \n",
       "                         gb     3260   83067    2337  1776         0.954708   \n",
       "                         xgb    4315   71932   13472   721         0.843229   \n",
       "                         svr    3613   75525    9879  1423         0.875203   \n",
       "                         mlp    5036      22   85382     0         0.054443   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "max_u_regressor_sparse   lr             0.402003       0.597092   0.480501   \n",
       "                         gb             0.568695       0.666644   0.613786   \n",
       "                         xgb             0.01963       0.790252   0.038308   \n",
       "                         svr            0.257948       0.770778   0.386537   \n",
       "                         mlp            0.052237            1.0   0.099287   \n",
       "max_u_filtered_regressor lr             0.402003       0.597092   0.480501   \n",
       "                         gb             0.581068       0.646568   0.612071   \n",
       "                         xgb            0.241617        0.85642   0.376901   \n",
       "                         svr            0.266493       0.716784   0.388533   \n",
       "                         mlp             0.05415            1.0   0.102736   \n",
       "\n",
       "                                (hybrid)mcc         q   f1_coin  \n",
       "experiment               class                                   \n",
       "max_u_regressor_sparse   lr        0.479834  0.016377  0.032227  \n",
       "                         gb        0.608879  0.016377  0.032227  \n",
       "                         xgb       0.032998  0.016377  0.032227  \n",
       "                         svr       0.431802  0.016377  0.032227  \n",
       "                         mlp      -0.275648  0.016377  0.032227  \n",
       "max_u_filtered_regressor lr        0.453823  0.055683  0.105492  \n",
       "                         gb        0.589039  0.055683  0.105492  \n",
       "                         xgb       0.402428  0.055683  0.105492  \n",
       "                         svr       0.386221  0.055683  0.105492  \n",
       "                         mlp       0.004214  0.055683  0.105492  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['max_u_regressor_sparse', 'max_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse, with GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>297640</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.976169</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.391930</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5039</td>\n",
       "      <td>294700</td>\n",
       "      <td>6778</td>\n",
       "      <td>979</td>\n",
       "      <td>0.974861</td>\n",
       "      <td>0.426091</td>\n",
       "      <td>0.837325</td>\n",
       "      <td>0.564781</td>\n",
       "      <td>0.586979</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4781</td>\n",
       "      <td>293809</td>\n",
       "      <td>7669</td>\n",
       "      <td>1237</td>\n",
       "      <td>0.971135</td>\n",
       "      <td>0.383609</td>\n",
       "      <td>0.794475</td>\n",
       "      <td>0.517396</td>\n",
       "      <td>0.540216</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5208</td>\n",
       "      <td>288486</td>\n",
       "      <td>12992</td>\n",
       "      <td>810</td>\n",
       "      <td>0.955228</td>\n",
       "      <td>0.285838</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>0.42972</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6011</td>\n",
       "      <td>229483</td>\n",
       "      <td>71995</td>\n",
       "      <td>7</td>\n",
       "      <td>1.023041</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>-0.280415</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>80584</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.918923</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.360620</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4997</td>\n",
       "      <td>77824</td>\n",
       "      <td>6598</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.430647</td>\n",
       "      <td>0.830352</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4950</td>\n",
       "      <td>78375</td>\n",
       "      <td>6047</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.921556</td>\n",
       "      <td>0.449796</td>\n",
       "      <td>0.822501</td>\n",
       "      <td>0.581558</td>\n",
       "      <td>0.572522</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5100</td>\n",
       "      <td>72437</td>\n",
       "      <td>11985</td>\n",
       "      <td>918</td>\n",
       "      <td>0.857577</td>\n",
       "      <td>0.298173</td>\n",
       "      <td>0.847308</td>\n",
       "      <td>0.441115</td>\n",
       "      <td>0.448986</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5751</td>\n",
       "      <td>30442</td>\n",
       "      <td>53980</td>\n",
       "      <td>267</td>\n",
       "      <td>0.406414</td>\n",
       "      <td>0.098079</td>\n",
       "      <td>0.954641</td>\n",
       "      <td>0.177883</td>\n",
       "      <td>0.169389</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp      tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                               \n",
       "min_u_regressor_sparse   lr     2502  297640   3838  3516         0.976169   \n",
       "                         gb     5039  294700   6778   979         0.974861   \n",
       "                         xgb    4781  293809   7669  1237         0.971135   \n",
       "                         svr    5208  288486  12992   810         0.955228   \n",
       "                         mlp    6011  229483  71995     7         1.023041   \n",
       "min_u_filtered_regressor lr     2502   80584   3838  3516         0.918923   \n",
       "                         gb     4997   77824   6598  1021         0.916002   \n",
       "                         xgb    4950   78375   6047  1068         0.921556   \n",
       "                         svr    5100   72437  11985   918         0.857577   \n",
       "                         mlp    5751   30442  53980   267         0.406414   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "min_u_regressor_sparse   lr             0.393263       0.415191   0.403929   \n",
       "                         gb             0.426091       0.837325   0.564781   \n",
       "                         xgb            0.383609       0.794475   0.517396   \n",
       "                         svr            0.285838       0.865268    0.42972   \n",
       "                         mlp            0.076957       0.998749   0.142903   \n",
       "min_u_filtered_regressor lr             0.393263       0.415191   0.403929   \n",
       "                         gb             0.430647       0.830352   0.567151   \n",
       "                         xgb            0.449796       0.822501   0.581558   \n",
       "                         svr            0.298173       0.847308   0.441115   \n",
       "                         mlp            0.098079       0.954641   0.177883   \n",
       "\n",
       "                                (hybrid)mcc         q   f1_coin  \n",
       "experiment               class                                   \n",
       "min_u_regressor_sparse   lr        0.391930  0.019571  0.038391  \n",
       "                         gb        0.586979  0.019571  0.038391  \n",
       "                         xgb       0.540216  0.019571  0.038391  \n",
       "                         svr       0.482436  0.019571  0.038391  \n",
       "                         mlp      -0.280415  0.019571  0.038391  \n",
       "min_u_filtered_regressor lr        0.360620  0.066541   0.12478  \n",
       "                         gb        0.560655  0.066541   0.12478  \n",
       "                         xgb       0.572522  0.066541   0.12478  \n",
       "                         svr       0.448986  0.066541   0.12478  \n",
       "                         mlp       0.169389  0.066541   0.12478  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['min_u_regressor_sparse', 'min_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse, with GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression vs Classification\n",
    "In order to understand the optinum number of busses for the training set of the regression data set the data sets used will be:\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Filtered Regression|45216|10|\n",
    "|Maximum Voltage Constraints|Sparse Classification|45216|10|\n",
    "|Minimum Voltage Constraints|Filtered Regression|45216|10|\n",
    "|Minimum Voltage Constraints|Sparse Classification|45216|10|\n",
    "\n",
    "The **Sparse Classification data set** is created from the Sparse Regression data set, but instead of having the target feature as the amplitude of the constraint violation, it is a binary feature that indicates if there is a constraint violation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>2598</td>\n",
       "      <td>83124</td>\n",
       "      <td>2280</td>\n",
       "      <td>2438</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>0.532595</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.524107</td>\n",
       "      <td>0.496589</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1867</td>\n",
       "      <td>84013</td>\n",
       "      <td>1391</td>\n",
       "      <td>3169</td>\n",
       "      <td>0.94958</td>\n",
       "      <td>0.573051</td>\n",
       "      <td>0.370731</td>\n",
       "      <td>0.450205</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>0</td>\n",
       "      <td>85404</td>\n",
       "      <td>0</td>\n",
       "      <td>5036</td>\n",
       "      <td>0.944317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3990</td>\n",
       "      <td>59437</td>\n",
       "      <td>25967</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.701316</td>\n",
       "      <td>0.133191</td>\n",
       "      <td>0.792295</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.237879</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>80965</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.453823</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3260</td>\n",
       "      <td>83067</td>\n",
       "      <td>2337</td>\n",
       "      <td>1776</td>\n",
       "      <td>0.954708</td>\n",
       "      <td>0.581068</td>\n",
       "      <td>0.646568</td>\n",
       "      <td>0.612071</td>\n",
       "      <td>0.589039</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4315</td>\n",
       "      <td>71932</td>\n",
       "      <td>13472</td>\n",
       "      <td>721</td>\n",
       "      <td>0.843229</td>\n",
       "      <td>0.241617</td>\n",
       "      <td>0.85642</td>\n",
       "      <td>0.376901</td>\n",
       "      <td>0.402428</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3613</td>\n",
       "      <td>75525</td>\n",
       "      <td>9879</td>\n",
       "      <td>1423</td>\n",
       "      <td>0.875203</td>\n",
       "      <td>0.266493</td>\n",
       "      <td>0.716784</td>\n",
       "      <td>0.388533</td>\n",
       "      <td>0.386221</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>22</td>\n",
       "      <td>85382</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.05415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp     tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                              \n",
       "max_u_classifier         lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                         gb     2598  83124   2280  2438         0.947833   \n",
       "                         xgb    1867  84013   1391  3169          0.94958   \n",
       "                         svr       0  85404      0  5036         0.944317   \n",
       "                         mlp    3990  59437  25967  1046         0.701316   \n",
       "max_u_filtered_regressor lr     3011  80965   4439  2025         0.928714   \n",
       "                         gb     3260  83067   2337  1776         0.954708   \n",
       "                         xgb    4315  71932  13472   721         0.843229   \n",
       "                         svr    3613  75525   9879  1423         0.875203   \n",
       "                         mlp    5036     22  85382     0         0.054443   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "max_u_classifier         lr                  NaN            NaN        NaN   \n",
       "                         gb             0.532595       0.515886   0.524107   \n",
       "                         xgb            0.573051       0.370731   0.450205   \n",
       "                         svr                   0            0.0          0   \n",
       "                         mlp            0.133191       0.792295   0.228046   \n",
       "max_u_filtered_regressor lr             0.402003       0.597092   0.480501   \n",
       "                         gb             0.581068       0.646568   0.612071   \n",
       "                         xgb            0.241617        0.85642   0.376901   \n",
       "                         svr            0.266493       0.716784   0.388533   \n",
       "                         mlp             0.05415            1.0   0.102736   \n",
       "\n",
       "                                (hybrid)mcc         q   f1_coin  \n",
       "experiment               class                                   \n",
       "max_u_classifier         lr             NaN       NaN       NaN  \n",
       "                         gb        0.496589  0.055683  0.105492  \n",
       "                         xgb       0.436154  0.055683  0.105492  \n",
       "                         svr       0.436154  0.055683  0.105492  \n",
       "                         mlp       0.237879  0.055683  0.105492  \n",
       "max_u_filtered_regressor lr        0.453823  0.055683  0.105492  \n",
       "                         gb        0.589039  0.055683  0.105492  \n",
       "                         xgb       0.402428  0.055683  0.105492  \n",
       "                         svr       0.386221  0.055683  0.105492  \n",
       "                         mlp       0.004214  0.055683  0.105492  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['max_u_classifier', 'max_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_u_filtered regressor with the gradient boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4149</td>\n",
       "      <td>80988</td>\n",
       "      <td>3434</td>\n",
       "      <td>1869</td>\n",
       "      <td>0.941364</td>\n",
       "      <td>0.547145</td>\n",
       "      <td>0.689432</td>\n",
       "      <td>0.610102</td>\n",
       "      <td>0.583377</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4092</td>\n",
       "      <td>80554</td>\n",
       "      <td>3868</td>\n",
       "      <td>1926</td>\n",
       "      <td>0.935935</td>\n",
       "      <td>0.51407</td>\n",
       "      <td>0.67996</td>\n",
       "      <td>0.585491</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>2976</td>\n",
       "      <td>82972</td>\n",
       "      <td>1450</td>\n",
       "      <td>3042</td>\n",
       "      <td>0.950332</td>\n",
       "      <td>0.67239</td>\n",
       "      <td>0.494516</td>\n",
       "      <td>0.569897</td>\n",
       "      <td>0.551432</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5323</td>\n",
       "      <td>47507</td>\n",
       "      <td>36915</td>\n",
       "      <td>695</td>\n",
       "      <td>0.584144</td>\n",
       "      <td>0.126024</td>\n",
       "      <td>0.884513</td>\n",
       "      <td>0.220615</td>\n",
       "      <td>0.223417</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>80584</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.918923</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.360620</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4997</td>\n",
       "      <td>77824</td>\n",
       "      <td>6598</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.430647</td>\n",
       "      <td>0.830352</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4950</td>\n",
       "      <td>78375</td>\n",
       "      <td>6047</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.921556</td>\n",
       "      <td>0.449796</td>\n",
       "      <td>0.822501</td>\n",
       "      <td>0.581558</td>\n",
       "      <td>0.572522</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5100</td>\n",
       "      <td>72437</td>\n",
       "      <td>11985</td>\n",
       "      <td>918</td>\n",
       "      <td>0.857577</td>\n",
       "      <td>0.298173</td>\n",
       "      <td>0.847308</td>\n",
       "      <td>0.441115</td>\n",
       "      <td>0.448986</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5751</td>\n",
       "      <td>30442</td>\n",
       "      <td>53980</td>\n",
       "      <td>267</td>\n",
       "      <td>0.406414</td>\n",
       "      <td>0.098079</td>\n",
       "      <td>0.954641</td>\n",
       "      <td>0.177883</td>\n",
       "      <td>0.169389</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp     tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                              \n",
       "min_u_classifier         lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                         gb     4149  80988   3434  1869         0.941364   \n",
       "                         xgb    4092  80554   3868  1926         0.935935   \n",
       "                         svr    2976  82972   1450  3042         0.950332   \n",
       "                         mlp    5323  47507  36915   695         0.584144   \n",
       "min_u_filtered_regressor lr     2502  80584   3838  3516         0.918923   \n",
       "                         gb     4997  77824   6598  1021         0.916002   \n",
       "                         xgb    4950  78375   6047  1068         0.921556   \n",
       "                         svr    5100  72437  11985   918         0.857577   \n",
       "                         mlp    5751  30442  53980   267         0.406414   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "min_u_classifier         lr                  NaN            NaN        NaN   \n",
       "                         gb             0.547145       0.689432   0.610102   \n",
       "                         xgb             0.51407        0.67996   0.585491   \n",
       "                         svr             0.67239       0.494516   0.569897   \n",
       "                         mlp            0.126024       0.884513   0.220615   \n",
       "min_u_filtered_regressor lr             0.393263       0.415191   0.403929   \n",
       "                         gb             0.430647       0.830352   0.567151   \n",
       "                         xgb            0.449796       0.822501   0.581558   \n",
       "                         svr            0.298173       0.847308   0.441115   \n",
       "                         mlp            0.098079       0.954641   0.177883   \n",
       "\n",
       "                                (hybrid)mcc         q  f1_coin  \n",
       "experiment               class                                  \n",
       "min_u_classifier         lr             NaN       NaN      NaN  \n",
       "                         gb        0.583377  0.066541  0.12478  \n",
       "                         xgb       0.557840  0.066541  0.12478  \n",
       "                         svr       0.551432  0.066541  0.12478  \n",
       "                         mlp       0.223417  0.066541  0.12478  \n",
       "min_u_filtered_regressor lr        0.360620  0.066541  0.12478  \n",
       "                         gb        0.560655  0.066541  0.12478  \n",
       "                         xgb       0.572522  0.066541  0.12478  \n",
       "                         svr       0.448986  0.066541  0.12478  \n",
       "                         mlp       0.169389  0.066541  0.12478  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['min_u_classifier', 'min_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_u_classifier with the gradient boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimum number of rows in class?\n",
    "In order to understand the optinum number of rows for the training set of the classification data set the data sets used will be:\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Sparse Classification|45216|10|\n",
    "|Maximum Voltage Constraints|Balanced Classification|6971|10|\n",
    "|Minimum Voltage Constraints|Sparse Classification|45216|10|\n",
    "|Minimum Voltage Constraints|Balanced Classification|13917|10|\n",
    "\n",
    "The **Balanced Classification data set** is created from the Sparse Classification data set. It is created by taking all the rows that containt at least one constraint violation and then taking the same number of rows that do not contain any constraint violation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>2598</td>\n",
       "      <td>83124</td>\n",
       "      <td>2280</td>\n",
       "      <td>2438</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>0.532595</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.524107</td>\n",
       "      <td>0.496589</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1867</td>\n",
       "      <td>84013</td>\n",
       "      <td>1391</td>\n",
       "      <td>3169</td>\n",
       "      <td>0.94958</td>\n",
       "      <td>0.573051</td>\n",
       "      <td>0.370731</td>\n",
       "      <td>0.450205</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>0</td>\n",
       "      <td>85404</td>\n",
       "      <td>0</td>\n",
       "      <td>5036</td>\n",
       "      <td>0.944317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3990</td>\n",
       "      <td>59437</td>\n",
       "      <td>25967</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.701316</td>\n",
       "      <td>0.133191</td>\n",
       "      <td>0.792295</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.237879</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3546</td>\n",
       "      <td>81683</td>\n",
       "      <td>3721</td>\n",
       "      <td>1490</td>\n",
       "      <td>0.942382</td>\n",
       "      <td>0.487959</td>\n",
       "      <td>0.70413</td>\n",
       "      <td>0.576445</td>\n",
       "      <td>0.557219</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3180</td>\n",
       "      <td>83080</td>\n",
       "      <td>2324</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.953782</td>\n",
       "      <td>0.577762</td>\n",
       "      <td>0.631454</td>\n",
       "      <td>0.603416</td>\n",
       "      <td>0.579572</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4029</td>\n",
       "      <td>80498</td>\n",
       "      <td>4906</td>\n",
       "      <td>1007</td>\n",
       "      <td>0.93462</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>0.80004</td>\n",
       "      <td>0.576766</td>\n",
       "      <td>0.570683</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3967</td>\n",
       "      <td>49322</td>\n",
       "      <td>36082</td>\n",
       "      <td>1069</td>\n",
       "      <td>0.589219</td>\n",
       "      <td>0.099054</td>\n",
       "      <td>0.787728</td>\n",
       "      <td>0.175979</td>\n",
       "      <td>0.168613</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tp     tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment                class                                              \n",
       "max_u_classifier          lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb     2598  83124   2280  2438         0.947833   \n",
       "                          xgb    1867  84013   1391  3169          0.94958   \n",
       "                          svr       0  85404      0  5036         0.944317   \n",
       "                          mlp    3990  59437  25967  1046         0.701316   \n",
       "max_u_classifier_balanced lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb     3546  81683   3721  1490         0.942382   \n",
       "                          xgb    3180  83080   2324  1856         0.953782   \n",
       "                          svr    4029  80498   4906  1007          0.93462   \n",
       "                          mlp    3967  49322  36082  1069         0.589219   \n",
       "\n",
       "                                (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment                class                                               \n",
       "max_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.532595       0.515886   0.524107   \n",
       "                          xgb            0.573051       0.370731   0.450205   \n",
       "                          svr                   0            0.0          0   \n",
       "                          mlp            0.133191       0.792295   0.228046   \n",
       "max_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.487959        0.70413   0.576445   \n",
       "                          xgb            0.577762       0.631454   0.603416   \n",
       "                          svr            0.450923        0.80004   0.576766   \n",
       "                          mlp            0.099054       0.787728   0.175979   \n",
       "\n",
       "                                 (hybrid)mcc         q   f1_coin  \n",
       "experiment                class                                   \n",
       "max_u_classifier          lr             NaN       NaN       NaN  \n",
       "                          gb        0.496589  0.055683  0.105492  \n",
       "                          xgb       0.436154  0.055683  0.105492  \n",
       "                          svr       0.436154  0.055683  0.105492  \n",
       "                          mlp       0.237879  0.055683  0.105492  \n",
       "max_u_classifier_balanced lr             NaN       NaN       NaN  \n",
       "                          gb        0.557219  0.055683  0.105492  \n",
       "                          xgb       0.579572  0.055683  0.105492  \n",
       "                          svr       0.570683  0.055683  0.105492  \n",
       "                          mlp       0.168613  0.055683  0.105492  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['max_u_classifier', 'max_u_classifier_balanced']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier balanced with xgb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4149</td>\n",
       "      <td>80988</td>\n",
       "      <td>3434</td>\n",
       "      <td>1869</td>\n",
       "      <td>0.941364</td>\n",
       "      <td>0.547145</td>\n",
       "      <td>0.689432</td>\n",
       "      <td>0.610102</td>\n",
       "      <td>0.583377</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4092</td>\n",
       "      <td>80554</td>\n",
       "      <td>3868</td>\n",
       "      <td>1926</td>\n",
       "      <td>0.935935</td>\n",
       "      <td>0.51407</td>\n",
       "      <td>0.67996</td>\n",
       "      <td>0.585491</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>2976</td>\n",
       "      <td>82972</td>\n",
       "      <td>1450</td>\n",
       "      <td>3042</td>\n",
       "      <td>0.950332</td>\n",
       "      <td>0.67239</td>\n",
       "      <td>0.494516</td>\n",
       "      <td>0.569897</td>\n",
       "      <td>0.551432</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5323</td>\n",
       "      <td>47507</td>\n",
       "      <td>36915</td>\n",
       "      <td>695</td>\n",
       "      <td>0.584144</td>\n",
       "      <td>0.126024</td>\n",
       "      <td>0.884513</td>\n",
       "      <td>0.220615</td>\n",
       "      <td>0.223417</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4478</td>\n",
       "      <td>77802</td>\n",
       "      <td>6620</td>\n",
       "      <td>1540</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.403496</td>\n",
       "      <td>0.744101</td>\n",
       "      <td>0.523253</td>\n",
       "      <td>0.505649</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4267</td>\n",
       "      <td>78129</td>\n",
       "      <td>6293</td>\n",
       "      <td>1751</td>\n",
       "      <td>0.911057</td>\n",
       "      <td>0.404072</td>\n",
       "      <td>0.70904</td>\n",
       "      <td>0.514779</td>\n",
       "      <td>0.492417</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4934</td>\n",
       "      <td>79490</td>\n",
       "      <td>4932</td>\n",
       "      <td>1084</td>\n",
       "      <td>0.933481</td>\n",
       "      <td>0.500101</td>\n",
       "      <td>0.819874</td>\n",
       "      <td>0.621254</td>\n",
       "      <td>0.608736</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5425</td>\n",
       "      <td>46582</td>\n",
       "      <td>37840</td>\n",
       "      <td>593</td>\n",
       "      <td>0.575044</td>\n",
       "      <td>0.12539</td>\n",
       "      <td>0.901462</td>\n",
       "      <td>0.220157</td>\n",
       "      <td>0.226129</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tp     tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment                class                                              \n",
       "min_u_classifier          lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb     4149  80988   3434  1869         0.941364   \n",
       "                          xgb    4092  80554   3868  1926         0.935935   \n",
       "                          svr    2976  82972   1450  3042         0.950332   \n",
       "                          mlp    5323  47507  36915   695         0.584144   \n",
       "min_u_classifier_balanced lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb     4478  77802   6620  1540         0.909774   \n",
       "                          xgb    4267  78129   6293  1751         0.911057   \n",
       "                          svr    4934  79490   4932  1084         0.933481   \n",
       "                          mlp    5425  46582  37840   593         0.575044   \n",
       "\n",
       "                                (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment                class                                               \n",
       "min_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.547145       0.689432   0.610102   \n",
       "                          xgb             0.51407        0.67996   0.585491   \n",
       "                          svr             0.67239       0.494516   0.569897   \n",
       "                          mlp            0.126024       0.884513   0.220615   \n",
       "min_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.403496       0.744101   0.523253   \n",
       "                          xgb            0.404072        0.70904   0.514779   \n",
       "                          svr            0.500101       0.819874   0.621254   \n",
       "                          mlp             0.12539       0.901462   0.220157   \n",
       "\n",
       "                                 (hybrid)mcc         q  f1_coin  \n",
       "experiment                class                                  \n",
       "min_u_classifier          lr             NaN       NaN      NaN  \n",
       "                          gb        0.583377  0.066541  0.12478  \n",
       "                          xgb       0.557840  0.066541  0.12478  \n",
       "                          svr       0.551432  0.066541  0.12478  \n",
       "                          mlp       0.223417  0.066541  0.12478  \n",
       "min_u_classifier_balanced lr             NaN       NaN      NaN  \n",
       "                          gb        0.505649  0.066541  0.12478  \n",
       "                          xgb       0.492417  0.066541  0.12478  \n",
       "                          svr       0.608736  0.066541  0.12478  \n",
       "                          mlp       0.226129  0.066541  0.12478  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['min_u_classifier', 'min_u_classifier_balanced']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier balanced with svr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "In order to understand how good our predictions really are, we can compare it to a random classifier. [source](https://inside.getyourguide.com/blog/2020/9/30/what-makes-a-good-f1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>298021</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.979047</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.479834</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3361</td>\n",
       "      <td>299916</td>\n",
       "      <td>2544</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>0.568695</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.613786</td>\n",
       "      <td>0.608879</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3983</td>\n",
       "      <td>97550</td>\n",
       "      <td>204910</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.338473</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.790252</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3885</td>\n",
       "      <td>290445</td>\n",
       "      <td>12015</td>\n",
       "      <td>1151</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>0.257948</td>\n",
       "      <td>0.770778</td>\n",
       "      <td>0.386537</td>\n",
       "      <td>0.431802</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>211703</td>\n",
       "      <td>90757</td>\n",
       "      <td>0</td>\n",
       "      <td>1.466242</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099287</td>\n",
       "      <td>-0.275648</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>4089</td>\n",
       "      <td>298466</td>\n",
       "      <td>3994</td>\n",
       "      <td>947</td>\n",
       "      <td>0.983766</td>\n",
       "      <td>0.505694</td>\n",
       "      <td>0.81232</td>\n",
       "      <td>0.62334</td>\n",
       "      <td>0.633630</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4416</td>\n",
       "      <td>252698</td>\n",
       "      <td>49762</td>\n",
       "      <td>620</td>\n",
       "      <td>0.836891</td>\n",
       "      <td>0.081427</td>\n",
       "      <td>0.876656</td>\n",
       "      <td>0.149013</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4700</td>\n",
       "      <td>71618</td>\n",
       "      <td>230842</td>\n",
       "      <td>336</td>\n",
       "      <td>0.256491</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.933189</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>0.053516</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4300</td>\n",
       "      <td>275628</td>\n",
       "      <td>26832</td>\n",
       "      <td>736</td>\n",
       "      <td>0.91493</td>\n",
       "      <td>0.144597</td>\n",
       "      <td>0.85364</td>\n",
       "      <td>0.247304</td>\n",
       "      <td>0.330513</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3247</td>\n",
       "      <td>194204</td>\n",
       "      <td>108256</td>\n",
       "      <td>1789</td>\n",
       "      <td>0.631347</td>\n",
       "      <td>0.030686</td>\n",
       "      <td>0.645414</td>\n",
       "      <td>0.058587</td>\n",
       "      <td>0.075512</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>80965</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.453823</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3260</td>\n",
       "      <td>83067</td>\n",
       "      <td>2337</td>\n",
       "      <td>1776</td>\n",
       "      <td>0.954708</td>\n",
       "      <td>0.581068</td>\n",
       "      <td>0.646568</td>\n",
       "      <td>0.612071</td>\n",
       "      <td>0.589039</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4315</td>\n",
       "      <td>71932</td>\n",
       "      <td>13472</td>\n",
       "      <td>721</td>\n",
       "      <td>0.843229</td>\n",
       "      <td>0.241617</td>\n",
       "      <td>0.85642</td>\n",
       "      <td>0.376901</td>\n",
       "      <td>0.402428</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3613</td>\n",
       "      <td>75525</td>\n",
       "      <td>9879</td>\n",
       "      <td>1423</td>\n",
       "      <td>0.875203</td>\n",
       "      <td>0.266493</td>\n",
       "      <td>0.716784</td>\n",
       "      <td>0.388533</td>\n",
       "      <td>0.386221</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>22</td>\n",
       "      <td>85382</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.05415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4513</td>\n",
       "      <td>282576</td>\n",
       "      <td>19884</td>\n",
       "      <td>523</td>\n",
       "      <td>0.933826</td>\n",
       "      <td>0.184598</td>\n",
       "      <td>0.895985</td>\n",
       "      <td>0.306125</td>\n",
       "      <td>0.389575</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4109</td>\n",
       "      <td>297630</td>\n",
       "      <td>4830</td>\n",
       "      <td>927</td>\n",
       "      <td>0.981642</td>\n",
       "      <td>0.463762</td>\n",
       "      <td>0.815659</td>\n",
       "      <td>0.591317</td>\n",
       "      <td>0.607091</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4242</td>\n",
       "      <td>87755</td>\n",
       "      <td>214705</td>\n",
       "      <td>794</td>\n",
       "      <td>0.304284</td>\n",
       "      <td>0.019722</td>\n",
       "      <td>0.842058</td>\n",
       "      <td>0.038541</td>\n",
       "      <td>0.038495</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3974</td>\n",
       "      <td>290777</td>\n",
       "      <td>11683</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.959239</td>\n",
       "      <td>0.258391</td>\n",
       "      <td>0.789028</td>\n",
       "      <td>0.389296</td>\n",
       "      <td>0.437418</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5008</td>\n",
       "      <td>226465</td>\n",
       "      <td>75995</td>\n",
       "      <td>28</td>\n",
       "      <td>1.008493</td>\n",
       "      <td>0.061813</td>\n",
       "      <td>0.994026</td>\n",
       "      <td>0.116388</td>\n",
       "      <td>-0.248941</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>2598</td>\n",
       "      <td>83124</td>\n",
       "      <td>2280</td>\n",
       "      <td>2438</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>0.532595</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.524107</td>\n",
       "      <td>0.496589</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1867</td>\n",
       "      <td>84013</td>\n",
       "      <td>1391</td>\n",
       "      <td>3169</td>\n",
       "      <td>0.94958</td>\n",
       "      <td>0.573051</td>\n",
       "      <td>0.370731</td>\n",
       "      <td>0.450205</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>0</td>\n",
       "      <td>85404</td>\n",
       "      <td>0</td>\n",
       "      <td>5036</td>\n",
       "      <td>0.944317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3990</td>\n",
       "      <td>59437</td>\n",
       "      <td>25967</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.701316</td>\n",
       "      <td>0.133191</td>\n",
       "      <td>0.792295</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.237879</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3546</td>\n",
       "      <td>81683</td>\n",
       "      <td>3721</td>\n",
       "      <td>1490</td>\n",
       "      <td>0.942382</td>\n",
       "      <td>0.487959</td>\n",
       "      <td>0.70413</td>\n",
       "      <td>0.576445</td>\n",
       "      <td>0.557219</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3180</td>\n",
       "      <td>83080</td>\n",
       "      <td>2324</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.953782</td>\n",
       "      <td>0.577762</td>\n",
       "      <td>0.631454</td>\n",
       "      <td>0.603416</td>\n",
       "      <td>0.579572</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4029</td>\n",
       "      <td>80498</td>\n",
       "      <td>4906</td>\n",
       "      <td>1007</td>\n",
       "      <td>0.93462</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>0.80004</td>\n",
       "      <td>0.576766</td>\n",
       "      <td>0.570683</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3967</td>\n",
       "      <td>49322</td>\n",
       "      <td>36082</td>\n",
       "      <td>1069</td>\n",
       "      <td>0.589219</td>\n",
       "      <td>0.099054</td>\n",
       "      <td>0.787728</td>\n",
       "      <td>0.175979</td>\n",
       "      <td>0.168613</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>297640</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.976169</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.391930</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5039</td>\n",
       "      <td>294700</td>\n",
       "      <td>6778</td>\n",
       "      <td>979</td>\n",
       "      <td>0.974861</td>\n",
       "      <td>0.426091</td>\n",
       "      <td>0.837325</td>\n",
       "      <td>0.564781</td>\n",
       "      <td>0.586979</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4781</td>\n",
       "      <td>293809</td>\n",
       "      <td>7669</td>\n",
       "      <td>1237</td>\n",
       "      <td>0.971135</td>\n",
       "      <td>0.383609</td>\n",
       "      <td>0.794475</td>\n",
       "      <td>0.517396</td>\n",
       "      <td>0.540216</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5208</td>\n",
       "      <td>288486</td>\n",
       "      <td>12992</td>\n",
       "      <td>810</td>\n",
       "      <td>0.955228</td>\n",
       "      <td>0.285838</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>0.42972</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6011</td>\n",
       "      <td>229483</td>\n",
       "      <td>71995</td>\n",
       "      <td>7</td>\n",
       "      <td>1.023041</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>-0.280415</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>5929</td>\n",
       "      <td>242474</td>\n",
       "      <td>59004</td>\n",
       "      <td>89</td>\n",
       "      <td>0.808951</td>\n",
       "      <td>0.091561</td>\n",
       "      <td>0.985199</td>\n",
       "      <td>0.167551</td>\n",
       "      <td>0.268520</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5657</td>\n",
       "      <td>257440</td>\n",
       "      <td>44038</td>\n",
       "      <td>361</td>\n",
       "      <td>0.856304</td>\n",
       "      <td>0.114014</td>\n",
       "      <td>0.939976</td>\n",
       "      <td>0.203362</td>\n",
       "      <td>0.299146</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5721</td>\n",
       "      <td>213689</td>\n",
       "      <td>87789</td>\n",
       "      <td>297</td>\n",
       "      <td>0.714145</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>0.95063</td>\n",
       "      <td>0.115166</td>\n",
       "      <td>0.198854</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5779</td>\n",
       "      <td>261411</td>\n",
       "      <td>40067</td>\n",
       "      <td>239</td>\n",
       "      <td>0.869434</td>\n",
       "      <td>0.126096</td>\n",
       "      <td>0.960264</td>\n",
       "      <td>0.222919</td>\n",
       "      <td>0.321932</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>2425</td>\n",
       "      <td>203328</td>\n",
       "      <td>98150</td>\n",
       "      <td>3593</td>\n",
       "      <td>0.660005</td>\n",
       "      <td>0.025915</td>\n",
       "      <td>0.40367</td>\n",
       "      <td>0.048704</td>\n",
       "      <td>0.021320</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>80584</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.918923</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.360620</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4997</td>\n",
       "      <td>77824</td>\n",
       "      <td>6598</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.430647</td>\n",
       "      <td>0.830352</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4950</td>\n",
       "      <td>78375</td>\n",
       "      <td>6047</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.921556</td>\n",
       "      <td>0.449796</td>\n",
       "      <td>0.822501</td>\n",
       "      <td>0.581558</td>\n",
       "      <td>0.572522</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5100</td>\n",
       "      <td>72437</td>\n",
       "      <td>11985</td>\n",
       "      <td>918</td>\n",
       "      <td>0.857577</td>\n",
       "      <td>0.298173</td>\n",
       "      <td>0.847308</td>\n",
       "      <td>0.441115</td>\n",
       "      <td>0.448986</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5751</td>\n",
       "      <td>30442</td>\n",
       "      <td>53980</td>\n",
       "      <td>267</td>\n",
       "      <td>0.406414</td>\n",
       "      <td>0.098079</td>\n",
       "      <td>0.954641</td>\n",
       "      <td>0.177883</td>\n",
       "      <td>0.169389</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4817</td>\n",
       "      <td>285372</td>\n",
       "      <td>16106</td>\n",
       "      <td>1201</td>\n",
       "      <td>0.943881</td>\n",
       "      <td>0.229869</td>\n",
       "      <td>0.800296</td>\n",
       "      <td>0.357153</td>\n",
       "      <td>0.410611</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5032</td>\n",
       "      <td>291795</td>\n",
       "      <td>9683</td>\n",
       "      <td>986</td>\n",
       "      <td>0.965425</td>\n",
       "      <td>0.341713</td>\n",
       "      <td>0.836223</td>\n",
       "      <td>0.485168</td>\n",
       "      <td>0.521648</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5270</td>\n",
       "      <td>82675</td>\n",
       "      <td>218803</td>\n",
       "      <td>748</td>\n",
       "      <td>0.289631</td>\n",
       "      <td>0.023833</td>\n",
       "      <td>0.875709</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.047840</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5168</td>\n",
       "      <td>288985</td>\n",
       "      <td>12493</td>\n",
       "      <td>850</td>\n",
       "      <td>0.956723</td>\n",
       "      <td>0.292504</td>\n",
       "      <td>0.858663</td>\n",
       "      <td>0.436361</td>\n",
       "      <td>0.486502</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6015</td>\n",
       "      <td>222433</td>\n",
       "      <td>79045</td>\n",
       "      <td>3</td>\n",
       "      <td>1.399868</td>\n",
       "      <td>0.068243</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>0.127762</td>\n",
       "      <td>-0.307781</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4149</td>\n",
       "      <td>80988</td>\n",
       "      <td>3434</td>\n",
       "      <td>1869</td>\n",
       "      <td>0.941364</td>\n",
       "      <td>0.547145</td>\n",
       "      <td>0.689432</td>\n",
       "      <td>0.610102</td>\n",
       "      <td>0.583377</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4092</td>\n",
       "      <td>80554</td>\n",
       "      <td>3868</td>\n",
       "      <td>1926</td>\n",
       "      <td>0.935935</td>\n",
       "      <td>0.51407</td>\n",
       "      <td>0.67996</td>\n",
       "      <td>0.585491</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>2976</td>\n",
       "      <td>82972</td>\n",
       "      <td>1450</td>\n",
       "      <td>3042</td>\n",
       "      <td>0.950332</td>\n",
       "      <td>0.67239</td>\n",
       "      <td>0.494516</td>\n",
       "      <td>0.569897</td>\n",
       "      <td>0.551432</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5323</td>\n",
       "      <td>47507</td>\n",
       "      <td>36915</td>\n",
       "      <td>695</td>\n",
       "      <td>0.584144</td>\n",
       "      <td>0.126024</td>\n",
       "      <td>0.884513</td>\n",
       "      <td>0.220615</td>\n",
       "      <td>0.223417</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4478</td>\n",
       "      <td>77802</td>\n",
       "      <td>6620</td>\n",
       "      <td>1540</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.403496</td>\n",
       "      <td>0.744101</td>\n",
       "      <td>0.523253</td>\n",
       "      <td>0.505649</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4267</td>\n",
       "      <td>78129</td>\n",
       "      <td>6293</td>\n",
       "      <td>1751</td>\n",
       "      <td>0.911057</td>\n",
       "      <td>0.404072</td>\n",
       "      <td>0.70904</td>\n",
       "      <td>0.514779</td>\n",
       "      <td>0.492417</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4934</td>\n",
       "      <td>79490</td>\n",
       "      <td>4932</td>\n",
       "      <td>1084</td>\n",
       "      <td>0.933481</td>\n",
       "      <td>0.500101</td>\n",
       "      <td>0.819874</td>\n",
       "      <td>0.621254</td>\n",
       "      <td>0.608736</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5425</td>\n",
       "      <td>46582</td>\n",
       "      <td>37840</td>\n",
       "      <td>593</td>\n",
       "      <td>0.575044</td>\n",
       "      <td>0.12539</td>\n",
       "      <td>0.901462</td>\n",
       "      <td>0.220157</td>\n",
       "      <td>0.226129</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tp      tn      fp    fn (hybrid)accuracy  \\\n",
       "experiment                class                                                \n",
       "max_u_regressor_sparse    lr     3011  298021    4439  2025         0.979047   \n",
       "                          gb     3361  299916    2544  1675         0.986368   \n",
       "                          xgb    3983   97550  204910  1053         0.338473   \n",
       "                          svr    3885  290445   12015  1151         0.960099   \n",
       "                          mlp    5036  211703   90757     0         1.466242   \n",
       "max_u_regressor_focused   lr     4089  298466    3994   947         0.983766   \n",
       "                          gb     4416  252698   49762   620         0.836891   \n",
       "                          xgb    4700   71618  230842   336         0.256491   \n",
       "                          svr    4300  275628   26832   736          0.91493   \n",
       "                          mlp    3247  194204  108256  1789         0.631347   \n",
       "max_u_filtered_regressor  lr     3011   80965    4439  2025         0.928714   \n",
       "                          gb     3260   83067    2337  1776         0.954708   \n",
       "                          xgb    4315   71932   13472   721         0.843229   \n",
       "                          svr    3613   75525    9879  1423         0.875203   \n",
       "                          mlp    5036      22   85382     0         0.054443   \n",
       "max_u_regressor_balanced  lr     4513  282576   19884   523         0.933826   \n",
       "                          gb     4109  297630    4830   927         0.981642   \n",
       "                          xgb    4242   87755  214705   794         0.304284   \n",
       "                          svr    3974  290777   11683  1062         0.959239   \n",
       "                          mlp    5008  226465   75995    28         1.008493   \n",
       "max_u_classifier          lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     2598   83124    2280  2438         0.947833   \n",
       "                          xgb    1867   84013    1391  3169          0.94958   \n",
       "                          svr       0   85404       0  5036         0.944317   \n",
       "                          mlp    3990   59437   25967  1046         0.701316   \n",
       "max_u_classifier_balanced lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     3546   81683    3721  1490         0.942382   \n",
       "                          xgb    3180   83080    2324  1856         0.953782   \n",
       "                          svr    4029   80498    4906  1007          0.93462   \n",
       "                          mlp    3967   49322   36082  1069         0.589219   \n",
       "min_u_regressor_sparse    lr     2502  297640    3838  3516         0.976169   \n",
       "                          gb     5039  294700    6778   979         0.974861   \n",
       "                          xgb    4781  293809    7669  1237         0.971135   \n",
       "                          svr    5208  288486   12992   810         0.955228   \n",
       "                          mlp    6011  229483   71995     7         1.023041   \n",
       "min_u_regressor_focused   lr     5929  242474   59004    89         0.808951   \n",
       "                          gb     5657  257440   44038   361         0.856304   \n",
       "                          xgb    5721  213689   87789   297         0.714145   \n",
       "                          svr    5779  261411   40067   239         0.869434   \n",
       "                          mlp    2425  203328   98150  3593         0.660005   \n",
       "min_u_filtered_regressor  lr     2502   80584    3838  3516         0.918923   \n",
       "                          gb     4997   77824    6598  1021         0.916002   \n",
       "                          xgb    4950   78375    6047  1068         0.921556   \n",
       "                          svr    5100   72437   11985   918         0.857577   \n",
       "                          mlp    5751   30442   53980   267         0.406414   \n",
       "min_u_regressor_balanced  lr     4817  285372   16106  1201         0.943881   \n",
       "                          gb     5032  291795    9683   986         0.965425   \n",
       "                          xgb    5270   82675  218803   748         0.289631   \n",
       "                          svr    5168  288985   12493   850         0.956723   \n",
       "                          mlp    6015  222433   79045     3         1.399868   \n",
       "min_u_classifier          lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     4149   80988    3434  1869         0.941364   \n",
       "                          xgb    4092   80554    3868  1926         0.935935   \n",
       "                          svr    2976   82972    1450  3042         0.950332   \n",
       "                          mlp    5323   47507   36915   695         0.584144   \n",
       "min_u_classifier_balanced lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     4478   77802    6620  1540         0.909774   \n",
       "                          xgb    4267   78129    6293  1751         0.911057   \n",
       "                          svr    4934   79490    4932  1084         0.933481   \n",
       "                          mlp    5425   46582   37840   593         0.575044   \n",
       "\n",
       "                                (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment                class                                               \n",
       "max_u_regressor_sparse    lr             0.402003       0.597092   0.480501   \n",
       "                          gb             0.568695       0.666644   0.613786   \n",
       "                          xgb             0.01963       0.790252   0.038308   \n",
       "                          svr            0.257948       0.770778   0.386537   \n",
       "                          mlp            0.052237            1.0   0.099287   \n",
       "max_u_regressor_focused   lr             0.505694        0.81232    0.62334   \n",
       "                          gb             0.081427       0.876656   0.149013   \n",
       "                          xgb            0.020734       0.933189   0.040567   \n",
       "                          svr            0.144597        0.85364   0.247304   \n",
       "                          mlp            0.030686       0.645414   0.058587   \n",
       "max_u_filtered_regressor  lr             0.402003       0.597092   0.480501   \n",
       "                          gb             0.581068       0.646568   0.612071   \n",
       "                          xgb            0.241617        0.85642   0.376901   \n",
       "                          svr            0.266493       0.716784   0.388533   \n",
       "                          mlp             0.05415            1.0   0.102736   \n",
       "max_u_regressor_balanced  lr             0.184598       0.895985   0.306125   \n",
       "                          gb             0.463762       0.815659   0.591317   \n",
       "                          xgb            0.019722       0.842058   0.038541   \n",
       "                          svr            0.258391       0.789028   0.389296   \n",
       "                          mlp            0.061813       0.994026   0.116388   \n",
       "max_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.532595       0.515886   0.524107   \n",
       "                          xgb            0.573051       0.370731   0.450205   \n",
       "                          svr                   0            0.0          0   \n",
       "                          mlp            0.133191       0.792295   0.228046   \n",
       "max_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.487959        0.70413   0.576445   \n",
       "                          xgb            0.577762       0.631454   0.603416   \n",
       "                          svr            0.450923        0.80004   0.576766   \n",
       "                          mlp            0.099054       0.787728   0.175979   \n",
       "min_u_regressor_sparse    lr             0.393263       0.415191   0.403929   \n",
       "                          gb             0.426091       0.837325   0.564781   \n",
       "                          xgb            0.383609       0.794475   0.517396   \n",
       "                          svr            0.285838       0.865268    0.42972   \n",
       "                          mlp            0.076957       0.998749   0.142903   \n",
       "min_u_regressor_focused   lr             0.091561       0.985199   0.167551   \n",
       "                          gb             0.114014       0.939976   0.203362   \n",
       "                          xgb            0.061296        0.95063   0.115166   \n",
       "                          svr            0.126096       0.960264   0.222919   \n",
       "                          mlp            0.025915        0.40367   0.048704   \n",
       "min_u_filtered_regressor  lr             0.393263       0.415191   0.403929   \n",
       "                          gb             0.430647       0.830352   0.567151   \n",
       "                          xgb            0.449796       0.822501   0.581558   \n",
       "                          svr            0.298173       0.847308   0.441115   \n",
       "                          mlp            0.098079       0.954641   0.177883   \n",
       "min_u_regressor_balanced  lr             0.229869       0.800296   0.357153   \n",
       "                          gb             0.341713       0.836223   0.485168   \n",
       "                          xgb            0.023833       0.875709   0.046404   \n",
       "                          svr            0.292504       0.858663   0.436361   \n",
       "                          mlp            0.068243       0.999402   0.127762   \n",
       "min_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.547145       0.689432   0.610102   \n",
       "                          xgb             0.51407        0.67996   0.585491   \n",
       "                          svr             0.67239       0.494516   0.569897   \n",
       "                          mlp            0.126024       0.884513   0.220615   \n",
       "min_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.403496       0.744101   0.523253   \n",
       "                          xgb            0.404072        0.70904   0.514779   \n",
       "                          svr            0.500101       0.819874   0.621254   \n",
       "                          mlp             0.12539       0.901462   0.220157   \n",
       "\n",
       "                                 (hybrid)mcc         q   f1_coin  \n",
       "experiment                class                                   \n",
       "max_u_regressor_sparse    lr        0.479834  0.016377  0.032227  \n",
       "                          gb        0.608879  0.016377  0.032227  \n",
       "                          xgb       0.032998  0.016377  0.032227  \n",
       "                          svr       0.431802  0.016377  0.032227  \n",
       "                          mlp      -0.275648  0.016377  0.032227  \n",
       "max_u_regressor_focused   lr        0.633630  0.016377  0.032227  \n",
       "                          gb        0.237300  0.016377  0.032227  \n",
       "                          xgb       0.053516  0.016377  0.032227  \n",
       "                          svr       0.330513  0.016377  0.032227  \n",
       "                          mlp       0.075512  0.016377  0.032227  \n",
       "max_u_filtered_regressor  lr        0.453823  0.055683  0.105492  \n",
       "                          gb        0.589039  0.055683  0.105492  \n",
       "                          xgb       0.402428  0.055683  0.105492  \n",
       "                          svr       0.386221  0.055683  0.105492  \n",
       "                          mlp       0.004214  0.055683  0.105492  \n",
       "max_u_regressor_balanced  lr        0.389575  0.016377  0.032227  \n",
       "                          gb        0.607091  0.016377  0.032227  \n",
       "                          xgb       0.038495  0.016377  0.032227  \n",
       "                          svr       0.437418  0.016377  0.032227  \n",
       "                          mlp      -0.248941  0.016377  0.032227  \n",
       "max_u_classifier          lr             NaN       NaN       NaN  \n",
       "                          gb        0.496589  0.055683  0.105492  \n",
       "                          xgb       0.436154  0.055683  0.105492  \n",
       "                          svr       0.436154  0.055683  0.105492  \n",
       "                          mlp       0.237879  0.055683  0.105492  \n",
       "max_u_classifier_balanced lr             NaN       NaN       NaN  \n",
       "                          gb        0.557219  0.055683  0.105492  \n",
       "                          xgb       0.579572  0.055683  0.105492  \n",
       "                          svr       0.570683  0.055683  0.105492  \n",
       "                          mlp       0.168613  0.055683  0.105492  \n",
       "min_u_regressor_sparse    lr        0.391930  0.019571  0.038391  \n",
       "                          gb        0.586979  0.019571  0.038391  \n",
       "                          xgb       0.540216  0.019571  0.038391  \n",
       "                          svr       0.482436  0.019571  0.038391  \n",
       "                          mlp      -0.280415  0.019571  0.038391  \n",
       "min_u_regressor_focused   lr        0.268520  0.019571  0.038391  \n",
       "                          gb        0.299146  0.019571  0.038391  \n",
       "                          xgb       0.198854  0.019571  0.038391  \n",
       "                          svr       0.321932  0.019571  0.038391  \n",
       "                          mlp       0.021320  0.019571  0.038391  \n",
       "min_u_filtered_regressor  lr        0.360620  0.066541   0.12478  \n",
       "                          gb        0.560655  0.066541   0.12478  \n",
       "                          xgb       0.572522  0.066541   0.12478  \n",
       "                          svr       0.448986  0.066541   0.12478  \n",
       "                          mlp       0.169389  0.066541   0.12478  \n",
       "min_u_regressor_balanced  lr        0.410611  0.019571  0.038391  \n",
       "                          gb        0.521648  0.019571  0.038391  \n",
       "                          xgb       0.047840  0.019571  0.038391  \n",
       "                          svr       0.486502  0.019571  0.038391  \n",
       "                          mlp      -0.307781  0.019571  0.038391  \n",
       "min_u_classifier          lr             NaN       NaN       NaN  \n",
       "                          gb        0.583377  0.066541   0.12478  \n",
       "                          xgb       0.557840  0.066541   0.12478  \n",
       "                          svr       0.551432  0.066541   0.12478  \n",
       "                          mlp       0.223417  0.066541   0.12478  \n",
       "min_u_classifier_balanced lr             NaN       NaN       NaN  \n",
       "                          gb        0.505649  0.066541   0.12478  \n",
       "                          xgb       0.492417  0.066541   0.12478  \n",
       "                          svr       0.608736  0.066541   0.12478  \n",
       "                          mlp       0.226129  0.066541   0.12478  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['q'] =  (df['tp'] + df['fn']) / (df['fp'] + df['tn'] + df['tp'] + df['fn'])\n",
    "df['f1_coin'] = (2*df['q'])/(df['q']+1)\n",
    "# write df to csv in this directory, with the name dataset_benchmark.csv\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5425"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[('min_u_classifier_balanced', 'mlp'), 'tp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: max_u_regressor_sparse, model: mlp, threshold: 0.001591058368850724\n",
      "hybrid_metrics.true_positives_ctr:  5036\n",
      "hybrid_metrics.true_negatives_ctr:  211703\n",
      "hybrid_metrics.false_positives_ctr:  90757\n",
      "hybrid_metrics.false_negatives_ctr:  0\n",
      "\n",
      "\n",
      "true_positives_hybrid_error 4405.631148287974\n",
      "true_negatives_hybrid_error -255782.01855689107\n",
      "false_positives_hybrid_error 79933.69811142237\n",
      "false_negatives_hybrid_error 0\n",
      "\n",
      "\n",
      "true_positives_rmse 0.12517252813979865\n",
      "true_negatives_rmse 2.2082115915074\n",
      "false_positives_rmse 0.11925583578762657\n",
      "false_negatives_rmse 0\n",
      "\n",
      "\n",
      "hybrid_metrics.hybrid_accuracy:  1.4662415086878646\n",
      "hybrid_metrics.hybrid_precision:  0.052236971611684176\n",
      "hybrid_metrics.hybrid_recall:  1.0\n",
      "hybrid_metrics.hybrid_f1:  0.09928746664674623\n",
      "hybrid_metrics.hybrid_mcc:  -0.27564802710445296\n"
     ]
    }
   ],
   "source": [
    "threshold = _threshold(experiment)\n",
    "experiment = 'max_u_regressor_sparse'\n",
    "model = 'mlp' \n",
    "threshold = _threshold(experiment)\n",
    "print('Experiment: {}, model: {}, threshold: {}'.format(experiment, model, threshold))\n",
    "hybrid_metrics = metrics.Metrics()\n",
    "hybrid_metrics.get_prediction_scores(testing_data[experiment][model]['predicted'], testing_data[experiment][model]['real'], threshold=threshold)\n",
    "print('hybrid_metrics.true_positives_ctr: ', hybrid_metrics.true_positives_ctr)\n",
    "print('hybrid_metrics.true_negatives_ctr: ', hybrid_metrics.true_negatives_ctr)\n",
    "print('hybrid_metrics.false_positives_ctr: ', hybrid_metrics.false_positives_ctr)\n",
    "print('hybrid_metrics.false_negatives_ctr: ', hybrid_metrics.false_negatives_ctr)\n",
    "print('\\n')\n",
    "print('true_positives_hybrid_error', hybrid_metrics.true_positives_hybrid_error)\n",
    "print('true_negatives_hybrid_error', hybrid_metrics.true_negatives_hybrid_error)\n",
    "print('false_positives_hybrid_error', hybrid_metrics.false_positives_hybrid_error)\n",
    "print('false_negatives_hybrid_error', hybrid_metrics.false_negatives_hybrid_error)\n",
    "print('\\n')\n",
    "print('true_positives_rmse', hybrid_metrics.true_positives_rmse)\n",
    "print('true_negatives_rmse', hybrid_metrics.true_negatives_rmse)\n",
    "print('false_positives_rmse', hybrid_metrics.false_positives_rmse)\n",
    "print('false_negatives_rmse', hybrid_metrics.false_negatives_rmse)\n",
    "print('\\n')\n",
    "print('hybrid_metrics.hybrid_accuracy: ', hybrid_metrics.hybrid_accuracy)\n",
    "print('hybrid_metrics.hybrid_precision: ', hybrid_metrics.hybrid_precision)\n",
    "print('hybrid_metrics.hybrid_recall: ', hybrid_metrics.hybrid_recall)\n",
    "print('hybrid_metrics.hybrid_f1: ', hybrid_metrics.hybrid_f1)\n",
    "print('hybrid_metrics.hybrid_mcc: ', hybrid_metrics.hybrid_mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e511966070>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# figure size of the plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 8))\n",
    "testing_data['min_u_regressor_sparse']['gb']['predicted']['bus_15'].plot()\n",
    "testing_data['min_u_regressor_sparse']['gb']['real']['bus_15'].plot()\n",
    "# plot a line with the threshold\n",
    "plt.axhline(y=threshold, color='r', linestyle='-')\n",
    "# Add legend\n",
    "plt.legend(['predicted', 'real', 'threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>R</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>season</th>\n",
       "      <th>weekday</th>\n",
       "      <th>last_hour_mean_wind_speed</th>\n",
       "      <th>last_day_mean_wind_direction</th>\n",
       "      <th>last_hour_mean_temperature</th>\n",
       "      <th>cos_hour_day</th>\n",
       "      <th>last_hour_mean_irradiance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.938805</td>\n",
       "      <td>0.455304</td>\n",
       "      <td>0.160055</td>\n",
       "      <td>0.101979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.153689</td>\n",
       "      <td>0.096501</td>\n",
       "      <td>0.939932</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.456003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.938404</td>\n",
       "      <td>0.455304</td>\n",
       "      <td>0.165477</td>\n",
       "      <td>0.103931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.159173</td>\n",
       "      <td>0.098412</td>\n",
       "      <td>0.939531</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.456003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.938003</td>\n",
       "      <td>0.455304</td>\n",
       "      <td>0.170898</td>\n",
       "      <td>0.105884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.164657</td>\n",
       "      <td>0.100345</td>\n",
       "      <td>0.939129</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.456003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.937602</td>\n",
       "      <td>0.455304</td>\n",
       "      <td>0.176319</td>\n",
       "      <td>0.107836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.170142</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.938728</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.456003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.937201</td>\n",
       "      <td>0.455304</td>\n",
       "      <td>0.181740</td>\n",
       "      <td>0.109788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.175626</td>\n",
       "      <td>0.104277</td>\n",
       "      <td>0.938327</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.456003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9039</th>\n",
       "      <td>0.965973</td>\n",
       "      <td>0.377550</td>\n",
       "      <td>0.259132</td>\n",
       "      <td>0.225232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.273230</td>\n",
       "      <td>0.181410</td>\n",
       "      <td>0.966734</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.411494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9040</th>\n",
       "      <td>0.965825</td>\n",
       "      <td>0.355341</td>\n",
       "      <td>0.251827</td>\n",
       "      <td>0.238410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.265839</td>\n",
       "      <td>0.194757</td>\n",
       "      <td>0.966587</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.389251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9041</th>\n",
       "      <td>0.965678</td>\n",
       "      <td>0.333132</td>\n",
       "      <td>0.244521</td>\n",
       "      <td>0.251588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.258448</td>\n",
       "      <td>0.208104</td>\n",
       "      <td>0.966439</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.367008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9042</th>\n",
       "      <td>0.965531</td>\n",
       "      <td>0.310923</td>\n",
       "      <td>0.237215</td>\n",
       "      <td>0.264766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.251058</td>\n",
       "      <td>0.221451</td>\n",
       "      <td>0.966292</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.344765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9043</th>\n",
       "      <td>0.965383</td>\n",
       "      <td>0.288715</td>\n",
       "      <td>0.229909</td>\n",
       "      <td>0.277943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.243667</td>\n",
       "      <td>0.234798</td>\n",
       "      <td>0.966144</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.322522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9044 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             T         R  wind_speed  wind_direction  season   weekday  \\\n",
       "0     0.938805  0.455304    0.160055        0.101979     1.0  0.142857   \n",
       "1     0.938404  0.455304    0.165477        0.103931     1.0  0.142857   \n",
       "2     0.938003  0.455304    0.170898        0.105884     1.0  0.142857   \n",
       "3     0.937602  0.455304    0.176319        0.107836     1.0  0.142857   \n",
       "4     0.937201  0.455304    0.181740        0.109788     1.0  0.142857   \n",
       "...        ...       ...         ...             ...     ...       ...   \n",
       "9039  0.965973  0.377550    0.259132        0.225232     0.0  0.571429   \n",
       "9040  0.965825  0.355341    0.251827        0.238410     0.0  0.571429   \n",
       "9041  0.965678  0.333132    0.244521        0.251588     0.0  0.571429   \n",
       "9042  0.965531  0.310923    0.237215        0.264766     0.0  0.571429   \n",
       "9043  0.965383  0.288715    0.229909        0.277943     0.0  0.571429   \n",
       "\n",
       "      last_hour_mean_wind_speed  last_day_mean_wind_direction  \\\n",
       "0                      0.153689                      0.096501   \n",
       "1                      0.159173                      0.098412   \n",
       "2                      0.164657                      0.100345   \n",
       "3                      0.170142                      0.102300   \n",
       "4                      0.175626                      0.104277   \n",
       "...                         ...                           ...   \n",
       "9039                   0.273230                      0.181410   \n",
       "9040                   0.265839                      0.194757   \n",
       "9041                   0.258448                      0.208104   \n",
       "9042                   0.251058                      0.221451   \n",
       "9043                   0.243667                      0.234798   \n",
       "\n",
       "      last_hour_mean_temperature  cos_hour_day  last_hour_mean_irradiance  \n",
       "0                       0.939932      0.258819                   0.456003  \n",
       "1                       0.939531      0.258819                   0.456003  \n",
       "2                       0.939129      0.258819                   0.456003  \n",
       "3                       0.938728      0.258819                   0.456003  \n",
       "4                       0.938327      0.500000                   0.456003  \n",
       "...                          ...           ...                        ...  \n",
       "9039                    0.966734      0.866025                   0.411494  \n",
       "9040                    0.966587      0.965926                   0.389251  \n",
       "9041                    0.966439      0.965926                   0.367008  \n",
       "9042                    0.966292      0.965926                   0.344765  \n",
       "9043                    0.966144      0.965926                   0.322522  \n",
       "\n",
       "[9044 rows x 11 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_max_u_sparse['X_test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
