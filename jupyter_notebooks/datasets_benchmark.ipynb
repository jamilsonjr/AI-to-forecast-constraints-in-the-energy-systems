{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of this Article** \n",
    "- Loading best hyperparameters for each model\n",
    "- Model training\n",
    "- Results discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading best hyperparameters for each model\n",
    "\n",
    "TODO... explain this model bench mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hyperparameters dataset.\n",
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse hyperparameters: 8/8\n",
      "Focused hyperparameters: 8/8\n",
      "Balanced hyperparameters: 8/8\n",
      "Filtered hyperparameters: 8/8\n",
      "Sparse classifier hyperparameters: 8/8\n",
      "Balanced classifier hyperparameters: 8/8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparse_hyper_params = {}\n",
    "focused_hyper_params = {}\n",
    "balanced_hyper_params = {}\n",
    "filtered_hyper_params = {}\n",
    "sparse_class_hyper_params = {}\n",
    "balanced_class_hyper_params = {}\n",
    "for file in os.listdir('hyper_params_results'):\n",
    "    if file.endswith('.csv') and 'regression_sparse' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'regression_focused' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        focused_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'regression_balanced' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        balanced_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'filtered' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        filtered_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'sparse_classifier' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_class_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'balanced_classifier' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        balanced_class_hyper_params[file] = df\n",
    "print('Sparse hyperparameters: {}/8'.format(len(sparse_hyper_params)))\n",
    "print('Focused hyperparameters: {}/8'.format(len(focused_hyper_params)))\n",
    "print('Balanced hyperparameters: {}/8'.format(len(balanced_hyper_params)))\n",
    "print('Filtered hyperparameters: {}/8'.format(len(filtered_hyper_params)))\n",
    "print('Sparse classifier hyperparameters: {}/8'.format(len(sparse_class_hyper_params)))\n",
    "print('Balanced classifier hyperparameters: {}/8'.format(len(balanced_class_hyper_params)))\n",
    "print('\\n')\n",
    "# print('Sparse hyper params:\\n')\n",
    "# for key in sparse_hyper_params.keys():\n",
    "#     print(key, ':\\n ',sparse_hyper_params[key])\n",
    "# print('Focused hyper params:\\n')\n",
    "# for key in focused_hyper_params.keys():\n",
    "#     print(key, ':\\n',focused_hyper_params[key])\n",
    "# print('Boolean hyper params:\\n')\n",
    "# for key in sparse_class_hyper_params.keys():\n",
    "#     print(key, ':\\n',sparse_class_hyper_params[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_size': 34,\n",
       " 'n_layers': 3,\n",
       " 'dropout': 0.0030412321477918842,\n",
       " 'activation': 'relu',\n",
       " 'optimizer': 'sgd',\n",
       " 'lr': 9.741292351005151e-05,\n",
       " 'epochs': 55,\n",
       " 'batch_size': 8,\n",
       " 'classifier': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "def get_hyper_params_from_df(df):\n",
    "    output = {}\n",
    "    for row in df.iterrows():\n",
    "        if row[1]['params'] != 'value':\n",
    "            try:\n",
    "                output[row[1]['params']] = ast.literal_eval(row[1]['value'])\n",
    "            except :\n",
    "                output[row[1]['params']] = row[1]['value']\n",
    "    return output\n",
    "get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_max_u.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from thesis_package import aimodels as my_ai, utils, metrics\n",
    "from copy import deepcopy\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data sparse\n",
    "y_max_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse, test_size=0.2, scaling=True)\n",
    "data_max_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse, test_size=0.2, scaling=True)\n",
    "data_min_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data sparse\n",
    "y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_max_u_bool = y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)]\n",
    "y_min_u_bool = y_min_u_bool[utils.cols_with_positive_values(y_min_u_bool)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_bool, test_size=0.2, scaling=True)\n",
    "data_max_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_bool, test_size=0.2, scaling=True)\n",
    "data_min_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered data\n",
    "y_max_u_filtered = deepcopy(y_max_u_sparse[utils.cols_with_positive_values(y_max_u_bool)])\n",
    "y_min_u_filtered = deepcopy(y_min_u_sparse[utils.cols_with_positive_values(y_min_u_bool)])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_filtered, test_size=0.2, scaling=True)\n",
    "data_max_u_filtered = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_filtered, test_size=0.2, scaling=True)\n",
    "data_min_u_filtered = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification data size:  (9044, 10)\n",
      "Regression data size:  (9044, 10)\n",
      "Positive in classification data:  5036.0\n",
      "Positive in regression data:  5036\n",
      "Theshhold:  0.001591058368850724\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the classiciation testing data and the filtered testing data\n",
    "print('Classification data size: ', data_max_u_bool['y_test'].shape)\n",
    "print('Regression data size: ', data_max_u_filtered['y_test'].shape)\n",
    "print('Positive in classification data: ', utils.count_positives_class(data_max_u_bool['y_test']))\n",
    "#unscaled_y_test = pd.DataFrame(data_max_u_filtered['scaler']['y'].inverse_transform(data_max_u_filtered['y_test']), columns=data_max_u_filtered['y_test'].columns)\n",
    "unscaled_y_test = utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y'])\n",
    "print('Positive in regression data: ', utils.count_positives_reg(unscaled_y_test, utils.compute_threshold(y_max_u_sparse)))\n",
    "print('Theshhold: ', utils.compute_threshold(y_max_u_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data focused\n",
    "y_max_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_focused_constr.csv')\n",
    "exogenous_data_focused_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_focused.csv').drop(columns=['date'])\n",
    "y_min_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_focused_constr.csv')\n",
    "exogenous_data_focused_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_focused.csv').drop(columns=['date'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_focused_max_u, y_max_u_focused, test_size=0.2, scaling=True)\n",
    "data_max_u_focused = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_focused_min_u, y_min_u_focused, test_size=0.2, scaling=True)\n",
    "data_min_u_focused = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data balanced\n",
    "y_max_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced, test_size=0.2, scaling=True)\n",
    "data_max_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced, test_size=0.2, scaling=True)\n",
    "data_min_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data balanced\n",
    "y_max_u_balanced_class = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_bool_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced_class = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_bool_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "y_max_u_balanced_class = y_max_u_balanced_class[utils.cols_with_positive_values(y_max_u_balanced_class)]\n",
    "y_min_u_balanced_class = y_min_u_balanced_class[utils.cols_with_positive_values(y_min_u_balanced_class)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced_class, test_size=0.2, scaling=True)\n",
    "data_max_u_bool_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced_class, test_size=0.2, scaling=True)\n",
    "data_min_u_bool_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count in classification data max_u : 5036.0\n",
      "Positive count in regression data max_u with threshold 0.001591058368850724 : 5036\n",
      "\n",
      "\n",
      "Positive count in classification data min_u : 6018.0\n",
      "Positive count in regression data min_u with threshold 0.0020242378560612192 : 6018\n",
      "\n",
      "\n",
      "Negative count in classification data max_u : 85404.0\n",
      "Negative count in regression data max_u with threshold 0.001591058368850724 : 85404\n",
      "\n",
      "\n",
      "Negative count in classification data min_u : 84422.0\n",
      "Negative count in regression data min_u with threshold 0.0020242378560612192 : 84422\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.check_positive_count(utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y']), data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')\n",
    "utils.check_positive_count(utils.unscale_df(data_min_u_filtered['y_test'], data_min_u_filtered['scaler']['y']), data_min_u_bool['y_test'], utils.compute_threshold(y_min_u_sparse), experiment='min_u')\n",
    "utils.check_negative_count(utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y']), data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')\n",
    "utils.check_negative_count(utils.unscale_df(data_min_u_filtered['y_test'], data_min_u_filtered['scaler']['y']), data_min_u_bool['y_test'], utils.compute_threshold(y_min_u_sparse), experiment='min_u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "In this section the models will be trained with the hyperparameters loaded above. All the models will be stored in the same `Context` object for later evaluation. The `Context` object is a class that stores all the models and their respective hyperparameters. The `Context` object is defined in the `aimodels.py` file. The `Context` object is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_models = ['lr', 'gb', 'xgb', 'svr', 'mlp']\n",
    "class_models =  ['gb', 'xgb', 'svr', 'mlp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['params_gradient_boost_regression_sparse_max_u.csv', 'params_gradient_boost_regression_sparse_min_u.csv', 'params_mlp_regression_sparse_max_u.csv', 'params_mlp_regression_sparse_min_u.csv', 'params_support_vector_regression_sparse_max_u.csv', 'params_support_vector_regression_sparse_min_u.csv', 'params_xgboost_regression_sparse_max_u.csv', 'params_xgboost_regression_sparse_min_u.csv'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_hyper_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression sparse\n"
     ]
    }
   ],
   "source": [
    "# max_u regression sparse\n",
    "if 'max_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression sparse')\n",
    "    # Linear Regression\n",
    "    regressor_max_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_max_u.csv'])\n",
    "    regressor_max_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_xgboost_regression_sparse_max_u.csv']) \n",
    "    regressor_max_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_support_vector_regression_sparse_max_u.csv'])\n",
    "    regressor_max_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_mlp_regression_sparse_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_sparse['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_sparse['y_train'].shape[1]\n",
    "    regressor_max_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_sparse', regressor_max_u)\n",
    "else:\n",
    "    print('Loading max_u regression sparse') \n",
    "    regressor_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_regressor_sparse')\n",
    "\n",
    "testing_data = {'max_u_regressor_sparse': {}}\n",
    "for model, strategy in zip(reg_models, regressor_max_u.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_sparse['y_test'].columns)\n",
    "    testing_data['max_u_regressor_sparse'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_sparse'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_sparse'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression focused\n"
     ]
    }
   ],
   "source": [
    "# max_u regression focused\n",
    "if 'max_u_regressor_focused.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression focused')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_focused = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_gradient_boost_regression_focused_max_u.csv'])\n",
    "    regressor_max_u_focused.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_xgboost_regression_focused_max_u.csv']) \n",
    "    regressor_max_u_focused.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_support_vector_regression_focused_max_u.csv'])\n",
    "    regressor_max_u_focused.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_focused['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_focused['y_train'].shape[1]\n",
    "    regressor_max_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_focused', regressor_max_u_focused)\n",
    "else: \n",
    "    print('Loading max_u regression focused')\n",
    "    regressor_max_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\\\max_u_regressor_focused')\n",
    "\n",
    "testing_data['max_u_regressor_focused'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_focused.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_sparse['y_test'].columns)\n",
    "    testing_data['max_u_regressor_focused'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_focused'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_focused'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u filtered regression\n"
     ]
    }
   ],
   "source": [
    "# max_u regression filtered\n",
    "if 'max_u_filtered_regressor.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression filtered')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_filtered = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_gradient_boost_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_xgboost_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_support_vector_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_mlp_regression_filtered_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_filtered['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_filtered['y_train'].shape[1]\n",
    "    regressor_max_u_filtered.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_filtered_regressor', regressor_max_u_filtered)\n",
    "else: \n",
    "    print('Loading max_u filtered regression')\n",
    "    regressor_max_u_filtered = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_filtered_regressor')\n",
    "\n",
    "testing_data['max_u_filtered_regressor'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_filtered.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_filtered['y_test'].columns)\n",
    "    testing_data['max_u_filtered_regressor'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_filtered_regressor'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_filtered_regressor'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression balanced\n"
     ]
    }
   ],
   "source": [
    "# max u regression balanced\n",
    "if 'max_u_regressor_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression balanced')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_balanced = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_gradient_boost_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_xgboost_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_support_vector_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_mlp_regression_balanced_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_balanced['y_train'].shape[1]\n",
    "    regressor_max_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_balanced', regressor_max_u_balanced)\n",
    "else: \n",
    "    print('Loading max_u regression balanced')\n",
    "    regressor_max_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_regressor_balanced')\n",
    "\n",
    "testing_data['max_u_regressor_balanced'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_balanced['y_test'].columns)\n",
    "    testing_data['max_u_regressor_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_balanced'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u classification\n"
     ]
    }
   ],
   "source": [
    "# max_u classification\n",
    "if 'max_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u classification')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_gradient_boost_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_xgboost_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_support_vector_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_mlp_sparse_classifier_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_bool['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_bool['y_train'].shape[1]\n",
    "    classifier_max_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_classifier', classifier_max_u)\n",
    "else: \n",
    "    print('Loading max_u classification')\n",
    "    classifier_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_classifier')\n",
    "\n",
    "testing_data['max_u_classifier'] = {}\n",
    "for model, strategy in zip(class_models, classifier_max_u.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_bool['y_test'].columns)\n",
    "    testing_data['max_u_classifier'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_classifier'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_classifier'][model]['real'] = deepcopy(data_max_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u classification balanced\n"
     ]
    }
   ],
   "source": [
    "# max_u classification balanced\n",
    "if 'max_u_classifier_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u classification balanced')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_gradient_boost_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_xgboost_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_support_vector_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_mlp_balanced_classifier_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_bool_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_bool_balanced['y_train'].shape[1]\n",
    "    classifier_max_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_classifier_balanced', classifier_max_u_balanced)\n",
    "else: \n",
    "    print('Loading max_u classification balanced')\n",
    "    classifier_max_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_classifier_balanced')\n",
    "\n",
    "testing_data['max_u_classifier_balanced'] = {}\n",
    "for model, strategy in zip(class_models, classifier_max_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_bool_balanced['y_test'].columns)\n",
    "    testing_data['max_u_classifier_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_classifier_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_classifier_balanced'][model]['real'] = deepcopy(data_max_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<thesis_package.aimodels.GradientBoostClassifierStrategy at 0x1d57501b250>,\n",
       " <thesis_package.aimodels.XGBoostClassifierStrategy at 0x1d57694c9a0>,\n",
       " <thesis_package.aimodels.SupportVectorClassifierStrategy at 0x1d57694cb80>,\n",
       " <thesis_package.aimodels.MultilayerPerceptronStrategy at 0x1d576905160>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_max_u_balanced.strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min u regression training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression sparse\n"
     ]
    }
   ],
   "source": [
    "# min_u regression sparse\n",
    "if 'min_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression sparse')\n",
    "    # Linear Regression\n",
    "    regressor_min_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_xgboost_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_support_vector_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_mlp_regression_sparse_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_sparse['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_sparse['y_train'].shape[1]\n",
    "    regressor_min_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse', regressor_min_u)\n",
    "else:\n",
    "    print('Loading min_u regression sparse')\n",
    "    regressor_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse')\n",
    "\n",
    "testing_data['min_u_regressor_sparse'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_sparse['y_test'].columns)\n",
    "    testing_data['min_u_regressor_sparse'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_sparse'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_sparse'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression focused\n"
     ]
    }
   ],
   "source": [
    "# min_u regression focused\n",
    "if 'min_u_regressor_focused.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression focused')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_focused = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_gradient_boost_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_xgboost_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_support_vector_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_focused['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_focused['y_train'].shape[1]\n",
    "    regressor_min_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_focused', regressor_min_u_focused)\n",
    "else:\n",
    "    print('Loading min_u regression focused')\n",
    "    regressor_min_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_focused')\n",
    "\n",
    "testing_data['min_u_regressor_focused'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_focused.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_focused['y_test'].columns)\n",
    "    testing_data['min_u_regressor_focused'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_focused'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_focused'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u filtered regression\n"
     ]
    }
   ],
   "source": [
    "# min u regression filtered\n",
    "if 'min_u_filtered_regressor.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression filtered')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_filtered = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_gradient_boost_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_xgboost_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_support_vector_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_mlp_regression_filtered_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_filtered['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_filtered['y_train'].shape[1]\n",
    "    regressor_min_u_filtered.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_filtered_regressor', regressor_min_u_filtered)\n",
    "else: \n",
    "    print('Loading min_u filtered regression')\n",
    "    regressor_min_u_filtered = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_filtered_regressor')\n",
    "\n",
    "testing_data['min_u_filtered_regressor'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_filtered.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_filtered['y_test'].columns)\n",
    "    testing_data['min_u_filtered_regressor'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_filtered_regressor'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_filtered_regressor'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression balanced\n"
     ]
    }
   ],
   "source": [
    "# min u regression balanced\n",
    "if 'min_u_regressor_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression balanced')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_balanced = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_gradient_boost_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_xgboost_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_support_vector_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_mlp_regression_balanced_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_balanced['y_train'].shape[1]\n",
    "    regressor_min_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_balanced', regressor_min_u_balanced)\n",
    "else: \n",
    "    print('Loading min_u regression balanced')\n",
    "    regressor_min_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_balanced')\n",
    "\n",
    "testing_data['min_u_regressor_balanced'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_balanced['y_test'].columns)\n",
    "    testing_data['min_u_regressor_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_balanced'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training min_u classification\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# min_u classification\n",
    "if 'min_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u classification')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_gradient_boost_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_xgboost_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_support_vector_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_classifier', classifier_min_u)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_mlp_sparse_classifier_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_bool['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_bool['y_train'].shape[1]\n",
    "    classifier_min_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "else: \n",
    "    print('Loading min_u classification')\n",
    "    classifier_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_classifier')\n",
    "\n",
    "testing_data['min_u_classifier'] = {}\n",
    "for model, strategy in zip(class_models, classifier_min_u.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_bool['y_test'].columns)\n",
    "    testing_data['min_u_classifier'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_classifier'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_classifier'][model]['real'] = deepcopy(data_min_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u classification balanced\n"
     ]
    }
   ],
   "source": [
    "# min u classification balanced\n",
    "if 'min_u_classifier_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u classification balanced')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_gradient_boost_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_xgboost_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_support_vector_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_mlp_balanced_classifier_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_bool_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_bool_balanced['y_train'].shape[1]\n",
    "    classifier_min_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_classifier_balanced', classifier_min_u_balanced)\n",
    "else: \n",
    "    print('Loading min_u classification balanced')\n",
    "    classifier_min_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_classifier_balanced')\n",
    "\n",
    "testing_data['min_u_classifier_balanced'] = {}\n",
    "for model, strategy in zip(class_models, classifier_min_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_bool_balanced['y_test'].columns)\n",
    "    testing_data['min_u_classifier_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_classifier_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_classifier_balanced'][model]['real'] = deepcopy(data_min_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "In this section the results of the training and testing are presented and compared. The main objectives of this experience is to compare the performance of the regression models in terms of the hybrid metrics confusion matrix and the hybrid metrics rmse. The comparisons will be the following:\n",
    "- Compare the confusion matrices of the classification models and the regression models evaluate with the hybrid metrics.\n",
    "- Compare the error results of the regression models trained with the focused dataset and the sparse dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_u_regressor_sparse :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_regressor_focused :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_filtered_regressor :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_regressor_balanced :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_classifier :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_classifier_balanced :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_regressor_sparse :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_regressor_focused :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_filtered_regressor :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_regressor_balanced :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_classifier :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_classifier_balanced :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n"
     ]
    }
   ],
   "source": [
    "for experience in testing_data.keys():\n",
    "    print(experience,': ', testing_data[experience].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3965 + 1071 = 5036 = 5036.0 possible positive values.\n",
      "59393 + 26011 = 85404 = 85404.0 possible negative values.\n"
     ]
    }
   ],
   "source": [
    "# Testing all models: Function that receives a dict with the real and predicted values, and outputs a dataframe with the results of the metrics.\n",
    "# Accumulate all the classifications for each bus.\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for bus in testing_data['max_u_classifier']['mlp']['predicted'].columns:\n",
    "    # Compute tp, tn, fp, fn\n",
    "    tp += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 1) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 1))\n",
    "    tn += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 0) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 0))\n",
    "    fp += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 1) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 0))\n",
    "    fn += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 0) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 1))\n",
    "print('{} + {} = {} = {} possible positive values.'.format(tp, fn, tp+fn, testing_data['max_u_classifier']['mlp']['real'].sum().sum()))\n",
    "print('{} + {} = {} = {} possible negative values.'.format(tn, fp, tn+fp, testing_data['max_u_classifier']['mlp']['real'].shape[0]*testing_data['max_u_classifier']['mlp']['real'].shape[1] - testing_data['max_u_classifier']['mlp']['real'].sum().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a multi-index dataframe with the results of the metrics. The first index is the testing_data.keys(), the second index are the tp, tn, fp, fn, and the columns are the models.\n",
    "columns = ['tp', 'tn', 'fp', 'fn', '(hybrid)accuracy', '(hybrid)precision', '(hybrid)recall', '(hybrid)f1']\n",
    "index = pd.MultiIndex.from_product([testing_data.keys(), ['lr', 'gb', 'xgb', 'svr', 'mlp']], names=['experiment', 'class'])\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "classifier_experiments =[experiment for experiment in testing_data.keys() if 'classifier' in experiment.split('_')] # TODO confirm this\n",
    "regressor_experiments = [experiment for experiment in testing_data.keys() if 'regressor' in experiment.split('_')]\n",
    "# Classifier experiments\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for experiment in classifier_experiments:\n",
    "    for model in testing_data[experiment].keys():\n",
    "        for bus in testing_data[experiment][model]['predicted'].columns:\n",
    "            try:\n",
    "                tp += sum((testing_data[experiment][model]['predicted'][bus] == 1) & (testing_data[experiment][model]['real'][bus] == 1))\n",
    "                tn += sum((testing_data[experiment][model]['predicted'][bus] == 0) & (testing_data[experiment][model]['real'][bus] == 0))\n",
    "                fp += sum((testing_data[experiment][model]['predicted'][bus] == 1) & (testing_data[experiment][model]['real'][bus] == 0))\n",
    "                fn += sum((testing_data[experiment][model]['predicted'][bus] == 0) & (testing_data[experiment][model]['real'][bus] == 1))\n",
    "            except: \n",
    "                print('In the experiment ', experiment, ' and model ', model, ' there was a problem with bus: ', bus)\n",
    "                if not testing_data[experiment][model]['real'][bus].any():\n",
    "                    print('Bus {} has no positive data points. Just ignore the little shit.'.format(bus))    \n",
    "        df.loc[(experiment, model), 'tp'] = tp\n",
    "        df.loc[(experiment, model), 'tn'] = tn\n",
    "        df.loc[(experiment, model), 'fp'] = fp\n",
    "        df.loc[(experiment, model), 'fn'] = fn\n",
    "        #print('Experiment: {}, model: {}, tp: {}, tn: {}, fp: {}, fn: {}'.format(experiment, model, tp, tn, fp, fn))\n",
    "        if (tp + tn + fp + fn) != 0:\n",
    "            accuracy = (tp + tn ) / (tp + tn + fp + fn)\n",
    "        else: \n",
    "            accuracy = 0\n",
    "        if (tp + fp) != 0:\n",
    "            precision = tp / (tp + fp)\n",
    "        else:\n",
    "            precision = 0\n",
    "        if (tp + fn) != 0:\n",
    "            recall = tp / (tp + fn)\n",
    "        else:\n",
    "            recall = 0\n",
    "        if (precision + recall) != 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        df.loc[(experiment, model), '(hybrid)accuracy'] = accuracy\n",
    "        df.loc[(experiment, model), '(hybrid)precision'] = precision\n",
    "        df.loc[(experiment, model), '(hybrid)recall'] = recall\n",
    "        df.loc[(experiment, model), '(hybrid)f1'] = f1\n",
    "        # print('Experiment: {}, model: {}, accuracy: {}, precision: {}, recall: {}, f1: {}'.format(experiment, model, accuracy, precision, recall, f1))\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>2598</td>\n",
       "      <td>83124</td>\n",
       "      <td>2280</td>\n",
       "      <td>2438</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>0.532595</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.524107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1867</td>\n",
       "      <td>84013</td>\n",
       "      <td>1391</td>\n",
       "      <td>3169</td>\n",
       "      <td>0.94958</td>\n",
       "      <td>0.573051</td>\n",
       "      <td>0.370731</td>\n",
       "      <td>0.450205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>0</td>\n",
       "      <td>85404</td>\n",
       "      <td>0</td>\n",
       "      <td>5036</td>\n",
       "      <td>0.944317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3965</td>\n",
       "      <td>59393</td>\n",
       "      <td>26011</td>\n",
       "      <td>1071</td>\n",
       "      <td>0.700553</td>\n",
       "      <td>0.132272</td>\n",
       "      <td>0.787331</td>\n",
       "      <td>0.226494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3546</td>\n",
       "      <td>81683</td>\n",
       "      <td>3721</td>\n",
       "      <td>1490</td>\n",
       "      <td>0.942382</td>\n",
       "      <td>0.487959</td>\n",
       "      <td>0.70413</td>\n",
       "      <td>0.576445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3180</td>\n",
       "      <td>83080</td>\n",
       "      <td>2324</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.953782</td>\n",
       "      <td>0.577762</td>\n",
       "      <td>0.631454</td>\n",
       "      <td>0.603416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4029</td>\n",
       "      <td>80498</td>\n",
       "      <td>4906</td>\n",
       "      <td>1007</td>\n",
       "      <td>0.93462</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>0.80004</td>\n",
       "      <td>0.576766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3948</td>\n",
       "      <td>49342</td>\n",
       "      <td>36062</td>\n",
       "      <td>1088</td>\n",
       "      <td>0.58923</td>\n",
       "      <td>0.098675</td>\n",
       "      <td>0.783956</td>\n",
       "      <td>0.175287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tp     tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment                class                                              \n",
       "max_u_regressor_sparse    lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb      NaN    NaN    NaN   NaN              NaN   \n",
       "                          xgb     NaN    NaN    NaN   NaN              NaN   \n",
       "                          svr     NaN    NaN    NaN   NaN              NaN   \n",
       "                          mlp     NaN    NaN    NaN   NaN              NaN   \n",
       "max_u_regressor_focused   lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb      NaN    NaN    NaN   NaN              NaN   \n",
       "                          xgb     NaN    NaN    NaN   NaN              NaN   \n",
       "                          svr     NaN    NaN    NaN   NaN              NaN   \n",
       "                          mlp     NaN    NaN    NaN   NaN              NaN   \n",
       "max_u_filtered_regressor  lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb      NaN    NaN    NaN   NaN              NaN   \n",
       "                          xgb     NaN    NaN    NaN   NaN              NaN   \n",
       "                          svr     NaN    NaN    NaN   NaN              NaN   \n",
       "                          mlp     NaN    NaN    NaN   NaN              NaN   \n",
       "max_u_regressor_balanced  lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb      NaN    NaN    NaN   NaN              NaN   \n",
       "                          xgb     NaN    NaN    NaN   NaN              NaN   \n",
       "                          svr     NaN    NaN    NaN   NaN              NaN   \n",
       "                          mlp     NaN    NaN    NaN   NaN              NaN   \n",
       "max_u_classifier          lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb     2598  83124   2280  2438         0.947833   \n",
       "                          xgb    1867  84013   1391  3169          0.94958   \n",
       "                          svr       0  85404      0  5036         0.944317   \n",
       "                          mlp    3965  59393  26011  1071         0.700553   \n",
       "max_u_classifier_balanced lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb     3546  81683   3721  1490         0.942382   \n",
       "                          xgb    3180  83080   2324  1856         0.953782   \n",
       "                          svr    4029  80498   4906  1007          0.93462   \n",
       "                          mlp    3948  49342  36062  1088          0.58923   \n",
       "\n",
       "                                (hybrid)precision (hybrid)recall (hybrid)f1  \n",
       "experiment                class                                              \n",
       "max_u_regressor_sparse    lr                  NaN            NaN        NaN  \n",
       "                          gb                  NaN            NaN        NaN  \n",
       "                          xgb                 NaN            NaN        NaN  \n",
       "                          svr                 NaN            NaN        NaN  \n",
       "                          mlp                 NaN            NaN        NaN  \n",
       "max_u_regressor_focused   lr                  NaN            NaN        NaN  \n",
       "                          gb                  NaN            NaN        NaN  \n",
       "                          xgb                 NaN            NaN        NaN  \n",
       "                          svr                 NaN            NaN        NaN  \n",
       "                          mlp                 NaN            NaN        NaN  \n",
       "max_u_filtered_regressor  lr                  NaN            NaN        NaN  \n",
       "                          gb                  NaN            NaN        NaN  \n",
       "                          xgb                 NaN            NaN        NaN  \n",
       "                          svr                 NaN            NaN        NaN  \n",
       "                          mlp                 NaN            NaN        NaN  \n",
       "max_u_regressor_balanced  lr                  NaN            NaN        NaN  \n",
       "                          gb                  NaN            NaN        NaN  \n",
       "                          xgb                 NaN            NaN        NaN  \n",
       "                          svr                 NaN            NaN        NaN  \n",
       "                          mlp                 NaN            NaN        NaN  \n",
       "max_u_classifier          lr                  NaN            NaN        NaN  \n",
       "                          gb             0.532595       0.515886   0.524107  \n",
       "                          xgb            0.573051       0.370731   0.450205  \n",
       "                          svr                   0            0.0          0  \n",
       "                          mlp            0.132272       0.787331   0.226494  \n",
       "max_u_classifier_balanced lr                  NaN            NaN        NaN  \n",
       "                          gb             0.487959        0.70413   0.576445  \n",
       "                          xgb            0.577762       0.631454   0.603416  \n",
       "                          svr            0.450923        0.80004   0.576766  \n",
       "                          mlp            0.098675       0.783956   0.175287  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "experiment                 class\n",
       "max_u_regressor_sparse     lr        NaN\n",
       "                           gb        NaN\n",
       "                           xgb       NaN\n",
       "                           svr       NaN\n",
       "                           mlp       NaN\n",
       "max_u_regressor_focused    lr        NaN\n",
       "                           gb        NaN\n",
       "                           xgb       NaN\n",
       "                           svr       NaN\n",
       "                           mlp       NaN\n",
       "max_u_filtered_regressor   lr        NaN\n",
       "                           gb        NaN\n",
       "                           xgb       NaN\n",
       "                           svr       NaN\n",
       "                           mlp       NaN\n",
       "max_u_regressor_balanced   lr        NaN\n",
       "                           gb        NaN\n",
       "                           xgb       NaN\n",
       "                           svr       NaN\n",
       "                           mlp       NaN\n",
       "max_u_classifier           lr        NaN\n",
       "                           gb       5036\n",
       "                           xgb      5036\n",
       "                           svr      5036\n",
       "                           mlp      5036\n",
       "max_u_classifier_balanced  lr        NaN\n",
       "                           gb       5036\n",
       "                           xgb      5036\n",
       "                           svr      5036\n",
       "                           mlp      5036\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tp'] + df['fn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "lr  failed to compute metrics.\n",
      "max_u_filtered_regressor  failed to compute metrics.\n",
      "0.0025\n",
      "gb  failed to compute metrics.\n",
      "max_u_filtered_regressor  failed to compute metrics.\n",
      "0.0025\n",
      "xgb  failed to compute metrics.\n",
      "max_u_filtered_regressor  failed to compute metrics.\n",
      "0.0025\n",
      "svr  failed to compute metrics.\n",
      "max_u_filtered_regressor  failed to compute metrics.\n",
      "0.0025\n",
      "mlp  failed to compute metrics.\n",
      "max_u_filtered_regressor  failed to compute metrics.\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "lr  failed to compute metrics.\n",
      "min_u_filtered_regressor  failed to compute metrics.\n",
      "0.0025\n",
      "gb  failed to compute metrics.\n",
      "min_u_filtered_regressor  failed to compute metrics.\n",
      "0.0025\n",
      "xgb  failed to compute metrics.\n",
      "min_u_filtered_regressor  failed to compute metrics.\n",
      "0.0025\n",
      "svr  failed to compute metrics.\n",
      "min_u_filtered_regressor  failed to compute metrics.\n",
      "0.0025\n",
      "mlp  failed to compute metrics.\n",
      "min_u_filtered_regressor  failed to compute metrics.\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "0.0025\n",
      "svr  failed to compute metrics.\n",
      "min_u_regressor_balanced  failed to compute metrics.\n",
      "0.0025\n"
     ]
    }
   ],
   "source": [
    "# Regressor experiments.\n",
    "for experiment in regressor_experiments:\n",
    "    for model in testing_data[experiment].keys():\n",
    "        try:\n",
    "            test_data = testing_data[experiment][model]['real']\n",
    "            threshold = utils.compute_threshold(test_data)\n",
    "            print(threshold)\n",
    "            hybrid_metrics = metrics.Metrics()\n",
    "            hybrid_metrics.get_prediction_scores(testing_data[experiment][model]['predicted'], testing_data[experiment][model]['real'], threshold=threshold)\n",
    "            df.loc[(experiment, model), 'tp'] = hybrid_metrics.true_positives_ctr\n",
    "            df.loc[(experiment, model), 'tn'] = hybrid_metrics.true_negatives_ctr\n",
    "            df.loc[(experiment, model), 'fp'] = hybrid_metrics.false_positives_ctr\n",
    "            df.loc[(experiment, model), 'fn'] = hybrid_metrics.false_negatives_ctr\n",
    "            df.loc[(experiment, model), '(hybrid)accuracy'] = hybrid_metrics.hybrid_accuracy\n",
    "            df.loc[(experiment, model), '(hybrid)precision'] = hybrid_metrics.hybrid_precision\n",
    "            df.loc[(experiment, model), '(hybrid)recall'] = hybrid_metrics.hybrid_recall\n",
    "            df.loc[(experiment, model), '(hybrid)f1'] = hybrid_metrics.hybrid_f1\n",
    "            # print('Experiment: {}, model: {}, tp: {}, tn: {}, fp: {}, fn: {}'.format(experiment, model, hybrid_metrics.true_positives_ctr, hybrid_metrics.true_negatives_ctr, hybrid_metrics.false_positives_ctr, hybrid_metrics.false_negatives_ctr))\n",
    "            # print('Experiment: {}, model: {}, accuracy: {}, precision: {}, recall: {}, f1: {}'.format(experiment, model, hybrid_metrics.hybrid_accuracy, hybrid_metrics.hybrid_precision, hybrid_metrics.hybrid_recall, hybrid_metrics.hybrid_f1))\n",
    "        except:\n",
    "            print(model, ' failed to compute metrics.')\n",
    "            print(experiment, ' failed to compute metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tp'] + df['fn']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
