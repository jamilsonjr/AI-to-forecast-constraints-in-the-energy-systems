{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of this Article** \n",
    "- Loading best hyperparameters for each model\n",
    "- Model training\n",
    "- Results discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading best hyperparameters for each model\n",
    "\n",
    "TODO... explain this model bench mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hyperparameters dataset.\n",
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse hyperparameters: 8/8\n",
      "Focused hyperparameters: 8/8\n",
      "Balanced hyperparameters: 8/8\n",
      "Filtered hyperparameters: 8/8\n",
      "Sparse classifier hyperparameters: 8/8\n",
      "Balanced classifier hyperparameters: 8/8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparse_hyper_params = {}\n",
    "focused_hyper_params = {}\n",
    "balanced_hyper_params = {}\n",
    "filtered_hyper_params = {}\n",
    "sparse_class_hyper_params = {}\n",
    "balanced_class_hyper_params = {}\n",
    "for file in os.listdir('hyper_params_results'):\n",
    "    if file.endswith('.csv') and 'regression_sparse' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'regression_focused' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        focused_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'regression_balanced' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        balanced_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'filtered' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        filtered_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'sparse_classifier' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        sparse_class_hyper_params[file] = df\n",
    "    elif file.endswith('.csv') and 'balanced_classifier' in file:\n",
    "        df = pd.read_csv(os.path.join('hyper_params_results', file))\n",
    "        balanced_class_hyper_params[file] = df\n",
    "print('Sparse hyperparameters: {}/8'.format(len(sparse_hyper_params)))\n",
    "print('Focused hyperparameters: {}/8'.format(len(focused_hyper_params)))\n",
    "print('Balanced hyperparameters: {}/8'.format(len(balanced_hyper_params)))\n",
    "print('Filtered hyperparameters: {}/8'.format(len(filtered_hyper_params)))\n",
    "print('Sparse classifier hyperparameters: {}/8'.format(len(sparse_class_hyper_params)))\n",
    "print('Balanced classifier hyperparameters: {}/8'.format(len(balanced_class_hyper_params)))\n",
    "print('\\n')\n",
    "# print('Sparse hyper params:\\n')\n",
    "# for key in sparse_hyper_params.keys():\n",
    "#     print(key, ':\\n ',sparse_hyper_params[key])\n",
    "# print('Focused hyper params:\\n')\n",
    "# for key in focused_hyper_params.keys():\n",
    "#     print(key, ':\\n',focused_hyper_params[key])\n",
    "# print('Boolean hyper params:\\n')\n",
    "# for key in sparse_class_hyper_params.keys():\n",
    "#     print(key, ':\\n',sparse_class_hyper_params[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_size': 34,\n",
       " 'n_layers': 3,\n",
       " 'dropout': 0.0030412321477918842,\n",
       " 'activation': 'relu',\n",
       " 'optimizer': 'sgd',\n",
       " 'lr': 9.741292351005151e-05,\n",
       " 'epochs': 55,\n",
       " 'batch_size': 8,\n",
       " 'classifier': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "def get_hyper_params_from_df(df):\n",
    "    output = {}\n",
    "    for row in df.iterrows():\n",
    "        if row[1]['params'] != 'value':\n",
    "            try:\n",
    "                output[row[1]['params']] = ast.literal_eval(row[1]['value'])\n",
    "            except :\n",
    "                output[row[1]['params']] = row[1]['value']\n",
    "    return output\n",
    "get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_max_u.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..');from thesis_package import aimodels as my_ai, utils, metrics\n",
    "from copy import deepcopy\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data sparse\n",
    "y_max_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_sparse = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse, test_size=0.2, scaling=True)\n",
    "data_max_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse, test_size=0.2, scaling=True)\n",
    "data_min_u_sparse = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data sparse\n",
    "y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_min_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_bool_constr.csv').drop(columns=['timestamps'])\n",
    "y_max_u_bool = y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)]\n",
    "y_min_u_bool = y_min_u_bool[utils.cols_with_positive_values(y_min_u_bool)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_bool, test_size=0.2, scaling=True)\n",
    "data_max_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_bool, test_size=0.2, scaling=True)\n",
    "data_min_u_bool = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered data\n",
    "y_max_u_filtered = deepcopy(y_max_u_sparse[utils.cols_with_positive_values(y_max_u_bool)])\n",
    "y_min_u_filtered = deepcopy(y_min_u_sparse[utils.cols_with_positive_values(y_min_u_bool)])\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_filtered, test_size=0.2, scaling=True)\n",
    "data_max_u_filtered = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_filtered, test_size=0.2, scaling=True)\n",
    "data_min_u_filtered = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification data size:  (9044, 10)\n",
      "Regression data size:  (9044, 10)\n",
      "Positive in classification data:  5036.0\n",
      "Positive in regression data:  5036\n",
      "Theshhold:  0.001591058368850724\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the classiciation testing data and the filtered testing data\n",
    "print('Classification data size: ', data_max_u_bool['y_test'].shape)\n",
    "print('Regression data size: ', data_max_u_filtered['y_test'].shape)\n",
    "print('Positive in classification data: ', utils.count_positives_class(data_max_u_bool['y_test']))\n",
    "#unscaled_y_test = pd.DataFrame(data_max_u_filtered['scaler']['y'].inverse_transform(data_max_u_filtered['y_test']), columns=data_max_u_filtered['y_test'].columns)\n",
    "unscaled_y_test = utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y'])\n",
    "print('Positive in regression data: ', utils.count_positives_reg(unscaled_y_test, utils.compute_threshold(y_max_u_sparse)))\n",
    "print('Theshhold: ', utils.compute_threshold(y_max_u_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data focused\n",
    "y_max_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_focused_constr.csv')\n",
    "exogenous_data_focused_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_focused.csv').drop(columns=['date'])\n",
    "y_min_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_focused_constr.csv')\n",
    "exogenous_data_focused_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_focused.csv').drop(columns=['date'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_focused_max_u, y_max_u_focused, test_size=0.2, scaling=True)\n",
    "data_max_u_focused = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_focused_min_u, y_min_u_focused, test_size=0.2, scaling=True)\n",
    "data_min_u_focused = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresison data balanced\n",
    "y_max_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced, test_size=0.2, scaling=True)\n",
    "data_max_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced, test_size=0.2, scaling=True)\n",
    "data_min_u_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data balanced\n",
    "y_max_u_balanced_class = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_balanced_bool_constr.csv')\n",
    "exogenous_data_balanced_max_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_max_balanced.csv').drop(columns=['date'])\n",
    "y_min_u_balanced_class = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_balanced_bool_constr.csv')\n",
    "exogenous_data_balanced_min_u = pd.read_csv('..\\data\\ground_truth\\exogenous_data_vm_pu_min_balanced.csv').drop(columns=['date'])\n",
    "y_max_u_balanced_class = y_max_u_balanced_class[utils.cols_with_positive_values(y_max_u_balanced_class)]\n",
    "y_min_u_balanced_class = y_min_u_balanced_class[utils.cols_with_positive_values(y_min_u_balanced_class)]\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_max_u, y_max_u_balanced_class, test_size=0.2, scaling=True)\n",
    "data_max_u_bool_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}\n",
    "\n",
    "train_x, test_x, train_y, test_y, scaler = utils.split_and_suffle(exogenous_data_balanced_min_u, y_min_u_balanced_class, test_size=0.2, scaling=True)\n",
    "data_min_u_bool_balanced = {'X_train': deepcopy(train_x), 'X_test': deepcopy(test_x), 'y_train': deepcopy(train_y), 'y_test': deepcopy(test_y), 'scaler': deepcopy(scaler)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count in classification data max_u : 5036.0\n",
      "Positive count in regression data max_u with threshold 0.001591058368850724 : 5036\n",
      "\n",
      "\n",
      "Positive count in classification data min_u : 6018.0\n",
      "Positive count in regression data min_u with threshold 0.0020242378560612192 : 6018\n",
      "\n",
      "\n",
      "Negative count in classification data max_u : 85404.0\n",
      "Negative count in regression data max_u with threshold 0.001591058368850724 : 85404\n",
      "\n",
      "\n",
      "Negative count in classification data min_u : 84422.0\n",
      "Negative count in regression data min_u with threshold 0.0020242378560612192 : 84422\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.check_positive_count(utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y']), data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')\n",
    "utils.check_positive_count(utils.unscale_df(data_min_u_filtered['y_test'], data_min_u_filtered['scaler']['y']), data_min_u_bool['y_test'], utils.compute_threshold(y_min_u_sparse), experiment='min_u')\n",
    "utils.check_negative_count(utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y']), data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')\n",
    "utils.check_negative_count(utils.unscale_df(data_min_u_filtered['y_test'], data_min_u_filtered['scaler']['y']), data_min_u_bool['y_test'], utils.compute_threshold(y_min_u_sparse), experiment='min_u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive count in classification data max_u : 5036.0\n",
      "Positive count in regression data max_u with threshold 0.001591058368850724 : 5036\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.check_positive_count(utils.unscale_df(data_max_u_filtered['y_test'], data_max_u_filtered['scaler']['y']), data_max_u_bool['y_test'], utils.compute_threshold(y_max_u_sparse), experiment='max_u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "In this section the models will be trained with the hyperparameters loaded above. All the models will be stored in the same `Context` object for later evaluation. The `Context` object is a class that stores all the models and their respective hyperparameters. The `Context` object is defined in the `aimodels.py` file. The `Context` object is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_models = ['lr', 'gb', 'xgb', 'svr', 'mlp']\n",
    "class_models =  ['gb', 'xgb', 'svr', 'mlp']\n",
    "max_u_threshold = utils.compute_threshold(y_max_u_sparse)\n",
    "min_u_threshold = utils.compute_threshold(y_min_u_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_filtered(df, scaler):\n",
    "    for bus in df.columns:\n",
    "        idx = list(scaler.feature_names_in_).index(bus)\n",
    "        df[bus] = scaler.max_abs_[idx] * df[bus]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['params_gradient_boost_regression_sparse_max_u.csv', 'params_gradient_boost_regression_sparse_min_u.csv', 'params_mlp_regression_sparse_max_u.csv', 'params_mlp_regression_sparse_min_u.csv', 'params_support_vector_regression_sparse_max_u.csv', 'params_support_vector_regression_sparse_min_u.csv', 'params_xgboost_regression_sparse_max_u.csv', 'params_xgboost_regression_sparse_min_u.csv'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_hyper_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression sparse\n"
     ]
    }
   ],
   "source": [
    "# max_u regression sparse\n",
    "if 'max_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression sparse')\n",
    "    # Linear Regression\n",
    "    regressor_max_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_max_u.csv'])\n",
    "    regressor_max_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_xgboost_regression_sparse_max_u.csv']) \n",
    "    regressor_max_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_support_vector_regression_sparse_max_u.csv'])\n",
    "    regressor_max_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_mlp_regression_sparse_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_sparse['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_sparse['y_train'].shape[1]\n",
    "    regressor_max_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u.fit(data=data_max_u_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_sparse', regressor_max_u)\n",
    "else:\n",
    "    print('Loading max_u regression sparse') \n",
    "    regressor_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_regressor_sparse')\n",
    "\n",
    "testing_data = {'max_u_regressor_sparse': {}}\n",
    "for model, strategy in zip(reg_models, regressor_max_u.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_sparse['y_test'].columns)\n",
    "    testing_data['max_u_regressor_sparse'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_sparse'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_sparse'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])\n",
    "    # Unscale\n",
    "    testing_data['max_u_regressor_sparse'][model]['predicted'] = utils.unscale_df(testing_data['max_u_regressor_sparse'][model]['predicted'],\\\n",
    "                                                                                data_max_u_sparse['scaler']['y'])\n",
    "    testing_data['max_u_regressor_sparse'][model]['real'] = utils.unscale_df(testing_data['max_u_regressor_sparse'][model]['real'],\\\n",
    "                                                                        data_max_u_sparse['scaler']['y'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression focused\n"
     ]
    }
   ],
   "source": [
    "# max_u regression focused\n",
    "if 'max_u_regressor_focused.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression focused')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_focused = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_gradient_boost_regression_focused_max_u.csv'])\n",
    "    regressor_max_u_focused.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_xgboost_regression_focused_max_u.csv']) \n",
    "    regressor_max_u_focused.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_support_vector_regression_focused_max_u.csv'])\n",
    "    regressor_max_u_focused.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_focused['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_focused['y_train'].shape[1]\n",
    "    regressor_max_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_focused.fit(data=data_max_u_focused)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_focused', regressor_max_u_focused)\n",
    "else: \n",
    "    print('Loading max_u regression focused')\n",
    "    regressor_max_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\\\max_u_regressor_focused')\n",
    "\n",
    "testing_data['max_u_regressor_focused'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_focused.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_sparse['y_test'].columns)\n",
    "    testing_data['max_u_regressor_focused'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_focused'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_focused'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['max_u_regressor_focused'][model]['predicted'] = utils.unscale_df(testing_data['max_u_regressor_focused'][model]['predicted'],\\\n",
    "                                                                                data_max_u_sparse['scaler']['y'])\n",
    "    testing_data['max_u_regressor_focused'][model]['real'] = utils.unscale_df(testing_data['max_u_regressor_focused'][model]['real'],\\\n",
    "                                                                        data_max_u_sparse['scaler']['y'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u filtered regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n"
     ]
    }
   ],
   "source": [
    "# max_u regression filtered\n",
    "if 'max_u_filtered_regressor.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression filtered')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_filtered = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_gradient_boost_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_xgboost_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_support_vector_regression_filtered_max_u.csv'])\n",
    "    regressor_max_u_filtered.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_mlp_regression_filtered_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_filtered['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_filtered['y_train'].shape[1]\n",
    "    regressor_max_u_filtered.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_filtered.fit(data=data_max_u_filtered)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_filtered_regressor', regressor_max_u_filtered)\n",
    "else: \n",
    "    print('Loading max_u filtered regression')\n",
    "    regressor_max_u_filtered = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_filtered_regressor')\n",
    "\n",
    "testing_data['max_u_filtered_regressor'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_filtered.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_filtered['y_test'].columns)\n",
    "    testing_data['max_u_filtered_regressor'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_filtered_regressor'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_filtered_regressor'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['max_u_filtered_regressor'][model]['predicted'] = inverse_transform_filtered(testing_data['max_u_filtered_regressor'][model]['predicted'],\\\n",
    "                                                                                data_max_u_sparse['scaler']['y'])\n",
    "    testing_data['max_u_filtered_regressor'][model]['real'] = inverse_transform_filtered(testing_data['max_u_filtered_regressor'][model]['real'][utils.cols_with_positive_values(prediction)],\\\n",
    "                                                                        data_max_u_sparse['scaler']['y'])\n",
    "    # Filter real data\n",
    "    #testing_data['max_u_filtered_regressor'][model]['real'] = deepcopy(data_max_u_sparse['y_test'][utils.cols_with_positive_values(prediction)])\n",
    "# print(utils.count_positives_reg(testing_data['max_u_filtered_regressor']['mlp']['real'], max_u_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u regression balanced\n"
     ]
    }
   ],
   "source": [
    "# max u regression balanced\n",
    "if 'max_u_regressor_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u regression balanced')\n",
    "    # Linear Regression\n",
    "    regressor_max_u_balanced = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_gradient_boost_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_xgboost_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_support_vector_regression_balanced_max_u.csv'])\n",
    "    regressor_max_u_balanced.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_mlp_regression_balanced_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_balanced['y_train'].shape[1]\n",
    "    regressor_max_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_max_u_balanced.fit(data=data_max_u_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_regressor_balanced', regressor_max_u_balanced)\n",
    "else: \n",
    "    print('Loading max_u regression balanced')\n",
    "    regressor_max_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_regressor_balanced')\n",
    "\n",
    "testing_data['max_u_regressor_balanced'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_max_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_balanced['y_test'].columns)\n",
    "    testing_data['max_u_regressor_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_regressor_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_regressor_balanced'][model]['real'] = deepcopy(data_max_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['max_u_regressor_balanced'][model]['predicted'] = utils.unscale_df(testing_data['max_u_regressor_balanced'][model]['predicted'],\\\n",
    "                                                                                data_max_u_balanced['scaler']['y'])\n",
    "    testing_data['max_u_regressor_balanced'][model]['real'] = utils.unscale_df(testing_data['max_u_regressor_balanced'][model]['real'],\\\n",
    "                                                                        data_max_u_sparse['scaler']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u classification\n"
     ]
    }
   ],
   "source": [
    "# max_u classification\n",
    "if 'max_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u classification')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_gradient_boost_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_xgboost_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_support_vector_sparse_classifier_max_u.csv'])\n",
    "    classifier_max_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_mlp_sparse_classifier_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_bool['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_bool['y_train'].shape[1]\n",
    "    classifier_max_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_max_u.fit(data=data_max_u_bool)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_classifier', classifier_max_u)\n",
    "else: \n",
    "    print('Loading max_u classification')\n",
    "    classifier_max_u = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_classifier')\n",
    "\n",
    "testing_data['max_u_classifier'] = {}\n",
    "for model, strategy in zip(class_models, classifier_max_u.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_bool['y_test'].columns)\n",
    "    testing_data['max_u_classifier'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_classifier'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_classifier'][model]['real'] = deepcopy(data_max_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testing_data['max_u_classifier']['xgb']['predicted']['bus_15'].plot()\n",
    "testing_data['max_u_classifier']['xgb']['real']['bus_15'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading max_u classification balanced\n"
     ]
    }
   ],
   "source": [
    "# max_u classification balanced\n",
    "if 'max_u_classifier_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training max_u classification balanced')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_gradient_boost_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_xgboost_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_support_vector_balanced_classifier_max_u.csv'])\n",
    "    classifier_max_u_balanced.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_mlp_balanced_classifier_max_u.csv'])\n",
    "    hyper_params['input_size'] = data_max_u_bool_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_max_u_bool_balanced['y_train'].shape[1]\n",
    "    classifier_max_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_max_u_balanced.fit(data=data_max_u_bool_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\max_u_classifier_balanced', classifier_max_u_balanced)\n",
    "else: \n",
    "    print('Loading max_u classification balanced')\n",
    "    classifier_max_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\max_u_classifier_balanced')\n",
    "\n",
    "testing_data['max_u_classifier_balanced'] = {}\n",
    "for model, strategy in zip(class_models, classifier_max_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_max_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_max_u_bool_balanced['y_test'].columns)\n",
    "    testing_data['max_u_classifier_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['max_u_classifier_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['max_u_classifier_balanced'][model]['real'] = deepcopy(data_max_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<thesis_package.aimodels.GradientBoostClassifierStrategy at 0x1e4ccd0a190>,\n",
       " <thesis_package.aimodels.XGBoostClassifierStrategy at 0x1e4d0c41ca0>,\n",
       " <thesis_package.aimodels.SupportVectorClassifierStrategy at 0x1e4cccfdf70>,\n",
       " <thesis_package.aimodels.MultilayerPerceptronStrategy at 0x1e4cf5c2b20>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_max_u_balanced.strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min u regression training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression sparse\n"
     ]
    }
   ],
   "source": [
    "# min_u regression sparse\n",
    "if 'min_u_regressor_sparse.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression sparse')\n",
    "    # Linear Regression\n",
    "    regressor_min_u = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_gradient_boost_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_xgboost_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_support_vector_regression_sparse_min_u.csv'])\n",
    "    regressor_min_u.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_hyper_params['params_mlp_regression_sparse_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_sparse['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_sparse['y_train'].shape[1]\n",
    "    regressor_min_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u.fit(data=data_min_u_sparse)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse', regressor_min_u)\n",
    "else:\n",
    "    print('Loading min_u regression sparse')\n",
    "    regressor_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_sparse')\n",
    "\n",
    "testing_data['min_u_regressor_sparse'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_sparse['y_test'].columns)\n",
    "    testing_data['min_u_regressor_sparse'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_sparse'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_sparse'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['min_u_regressor_sparse'][model]['predicted'] = utils.unscale_df(testing_data['min_u_regressor_sparse'][model]['predicted'],\\\n",
    "                                                                                data_min_u_sparse['scaler']['y'])\n",
    "    testing_data['min_u_regressor_sparse'][model]['real'] = utils.unscale_df(testing_data['min_u_regressor_sparse'][model]['real'],\\\n",
    "                                                                        data_min_u_sparse['scaler']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression focused\n"
     ]
    }
   ],
   "source": [
    "# min_u regression focused\n",
    "if 'min_u_regressor_focused.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression focused')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_focused = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_gradient_boost_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Extreme GBoost Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_xgboost_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # Support Vector Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_support_vector_regression_focused_min_u.csv'])\n",
    "    regressor_min_u_focused.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(focused_hyper_params['params_mlp_regression_focused_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_focused['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_focused['y_train'].shape[1]\n",
    "    regressor_min_u_focused.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_focused.fit(data=data_min_u_focused)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_focused', regressor_min_u_focused)\n",
    "else:\n",
    "    print('Loading min_u regression focused')\n",
    "    regressor_min_u_focused = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_focused')\n",
    "\n",
    "testing_data['min_u_regressor_focused'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_focused.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_focused['y_test'].columns)\n",
    "    testing_data['min_u_regressor_focused'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_focused'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_focused'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['min_u_regressor_focused'][model]['predicted'] = utils.unscale_df(testing_data['min_u_regressor_focused'][model]['predicted'],\\\n",
    "                                                                                data_min_u_focused['scaler']['y'])\n",
    "    testing_data['min_u_regressor_focused'][model]['real'] = utils.unscale_df(testing_data['min_u_regressor_focused'][model]['real'],\\\n",
    "                                                                        data_min_u_sparse['scaler']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u filtered regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n",
      "C:\\Users\\jamil\\AppData\\Local\\Temp\\ipykernel_19944\\1155023926.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[bus] = scaler.max_abs_[idx] * df[bus]\n"
     ]
    }
   ],
   "source": [
    "# min u regression filtered\n",
    "if 'min_u_filtered_regressor.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression filtered')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_filtered = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_gradient_boost_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_xgboost_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_support_vector_regression_filtered_min_u.csv'])\n",
    "    regressor_min_u_filtered.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(filtered_hyper_params['params_mlp_regression_filtered_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_filtered['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_filtered['y_train'].shape[1]\n",
    "    regressor_min_u_filtered.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_filtered.fit(data=data_min_u_filtered)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_filtered_regressor', regressor_min_u_filtered)\n",
    "else: \n",
    "    print('Loading min_u filtered regression')\n",
    "    regressor_min_u_filtered = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_filtered_regressor')\n",
    "\n",
    "testing_data['min_u_filtered_regressor'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_filtered.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_filtered['y_test'].columns)\n",
    "    testing_data['min_u_filtered_regressor'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_filtered_regressor'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_filtered_regressor'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['min_u_filtered_regressor'][model]['predicted'] = inverse_transform_filtered(testing_data['min_u_filtered_regressor'][model]['predicted'],\\\n",
    "                                                                                data_min_u_sparse['scaler']['y'])\n",
    "    testing_data['min_u_filtered_regressor'][model]['real'] = inverse_transform_filtered(testing_data['min_u_filtered_regressor'][model]['real'][utils.cols_with_positive_values(prediction)],\\\n",
    "                                                                        data_min_u_sparse['scaler']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u regression balanced\n"
     ]
    }
   ],
   "source": [
    "# min u regression balanced\n",
    "if 'min_u_regressor_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u regression balanced')\n",
    "    # Linear Regression\n",
    "    regressor_min_u_balanced = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_gradient_boost_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_xgboost_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.XGBoostRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_support_vector_regression_balanced_min_u.csv'])\n",
    "    regressor_min_u_balanced.strategy = my_ai.SupportVectorRegressorStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_hyper_params['params_mlp_regression_balanced_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_balanced['y_train'].shape[1]\n",
    "    regressor_min_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    regressor_min_u_balanced.fit(data=data_min_u_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_regressor_balanced', regressor_min_u_balanced)\n",
    "else: \n",
    "    print('Loading min_u regression balanced')\n",
    "    regressor_min_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_regressor_balanced')\n",
    "\n",
    "testing_data['min_u_regressor_balanced'] = {}\n",
    "for model, strategy in zip(reg_models, regressor_min_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_sparse)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_balanced['y_test'].columns)\n",
    "    testing_data['min_u_regressor_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_regressor_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_regressor_balanced'][model]['real'] = deepcopy(data_min_u_sparse['y_test'])\n",
    "    # Unsacale\n",
    "    testing_data['min_u_regressor_balanced'][model]['predicted'] = utils.unscale_df(testing_data['min_u_regressor_balanced'][model]['predicted'],\\\n",
    "                                                                                data_min_u_balanced['scaler']['y'])\n",
    "    testing_data['min_u_regressor_balanced'][model]['real'] = utils.unscale_df(testing_data['min_u_regressor_balanced'][model]['real'],\\\n",
    "                                                                        data_min_u_sparse['scaler']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u classification\n"
     ]
    }
   ],
   "source": [
    "# min_u classification\n",
    "if 'min_u_classifier.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u classification')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_gradient_boost_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_xgboost_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_support_vector_sparse_classifier_min_u.csv'])\n",
    "    classifier_min_u.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    # MLP Regression\n",
    "    hyper_params = get_hyper_params_from_df(sparse_class_hyper_params['params_mlp_sparse_classifier_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_bool['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_bool['y_train'].shape[1]\n",
    "    classifier_min_u.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_min_u.fit(data=data_min_u_bool)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_classifier', classifier_min_u)\n",
    "else: \n",
    "    print('Loading min_u classification')\n",
    "    classifier_min_u = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_classifier')\n",
    "\n",
    "testing_data['min_u_classifier'] = {}\n",
    "for model, strategy in zip(class_models, classifier_min_u.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_bool['y_test'].columns)\n",
    "    testing_data['min_u_classifier'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_classifier'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_classifier'][model]['real'] = deepcopy(data_min_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<thesis_package.aimodels.GradientBoostClassifierStrategy at 0x1e4d4ef8280>,\n",
       " <thesis_package.aimodels.XGBoostClassifierStrategy at 0x1e4d4fefdf0>,\n",
       " <thesis_package.aimodels.SupportVectorClassifierStrategy at 0x1e4d4fddf10>,\n",
       " <thesis_package.aimodels.MultilayerPerceptronStrategy at 0x1e4de822b80>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_min_u.strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading min_u classification balanced\n"
     ]
    }
   ],
   "source": [
    "# min u classification balanced\n",
    "if 'min_u_classifier_balanced.pickle' not in os.listdir('pickles\\dataset_benchmark'):\n",
    "    print('Training min_u classification balanced')\n",
    "    # Gradient Boost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_gradient_boost_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced = my_ai.Context(strategy=my_ai.GradientBoostClassifierStrategy(hyper_params))\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # Extreme GBoost Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_xgboost_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced.strategy = my_ai.XGBoostClassifierStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # Support Vector Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_support_vector_balanced_classifier_min_u.csv'])\n",
    "    classifier_min_u_balanced.strategy = my_ai.SupportVectorClassifierStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    # MLP Classifier\n",
    "    hyper_params = get_hyper_params_from_df(balanced_class_hyper_params['params_mlp_balanced_classifier_min_u.csv'])\n",
    "    hyper_params['input_size'] = data_min_u_bool_balanced['X_train'].shape[1]\n",
    "    hyper_params['output_size'] = data_min_u_bool_balanced['y_train'].shape[1]\n",
    "    classifier_min_u_balanced.strategy = my_ai.MultilayerPerceptronStrategy(hyper_params)\n",
    "    classifier_min_u_balanced.fit(data=data_min_u_bool_balanced)\n",
    "    utils.serialize_object('pickles\\dataset_benchmark\\min_u_classifier_balanced', classifier_min_u_balanced)\n",
    "else: \n",
    "    print('Loading min_u classification balanced')\n",
    "    classifier_min_u_balanced = utils.deserialize_object('pickles\\dataset_benchmark\\min_u_classifier_balanced')\n",
    "\n",
    "testing_data['min_u_classifier_balanced'] = {}\n",
    "for model, strategy in zip(class_models, classifier_min_u_balanced.strategies):\n",
    "    prediction = strategy.predict(data=data_min_u_bool)\n",
    "    prediction = pd.DataFrame(prediction, columns=data_min_u_bool_balanced['y_test'].columns)\n",
    "    testing_data['min_u_classifier_balanced'][model] = {'real': None, 'predicted': None}\n",
    "    testing_data['min_u_classifier_balanced'][model]['predicted'] = deepcopy(prediction)\n",
    "    testing_data['min_u_classifier_balanced'][model]['real'] = deepcopy(data_min_u_bool['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "In this section the results of the training and testing are presented and compared. The main objectives of this experience is to compare the performance of the regression models in terms of the hybrid metrics confusion matrix and the hybrid metrics rmse. The comparisons will be the following:\n",
    "- Compare the confusion matrices of the classification models and the regression models evaluate with the hybrid metrics.\n",
    "- Compare the error results of the regression models trained with the focused dataset and the sparse dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_u_regressor_sparse :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_regressor_focused :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_filtered_regressor :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_regressor_balanced :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_classifier :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "max_u_classifier_balanced :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_regressor_sparse :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_regressor_focused :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_filtered_regressor :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_regressor_balanced :  dict_keys(['lr', 'gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_classifier :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n",
      "min_u_classifier_balanced :  dict_keys(['gb', 'xgb', 'svr', 'mlp'])\n"
     ]
    }
   ],
   "source": [
    "for experience in testing_data.keys():\n",
    "    print(experience,': ', testing_data[experience].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3990 + 1046 = 5036 = 5036.0 possible positive values.\n",
      "59437 + 25967 = 85404 = 85404.0 possible negative values.\n"
     ]
    }
   ],
   "source": [
    "# Testing all models: Function that receives a dict with the real and predicted values, and outputs a dataframe with the results of the metrics.\n",
    "# Accumulate all the classifications for each bus.\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for bus in testing_data['max_u_classifier']['mlp']['predicted'].columns:\n",
    "    # Compute tp, tn, fp, fn\n",
    "    tp += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 1) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 1))\n",
    "    tn += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 0) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 0))\n",
    "    fp += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 1) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 0))\n",
    "    fn += sum((testing_data['max_u_classifier']['mlp']['predicted'][bus] == 0) & (testing_data['max_u_classifier']['mlp']['real'][bus] == 1))\n",
    "print('{} + {} = {} = {} possible positive values.'.format(tp, fn, tp+fn, testing_data['max_u_classifier']['mlp']['real'].sum().sum()))\n",
    "print('{} + {} = {} = {} possible negative values.'.format(tn, fp, tn+fp, testing_data['max_u_classifier']['mlp']['real'].shape[0]*testing_data['max_u_classifier']['mlp']['real'].shape[1] - testing_data['max_u_classifier']['mlp']['real'].sum().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    df.to_csv('dataset_benchmark.csv')\n",
    "except:\n",
    "    from numpy import sqrt \n",
    "    # Build a multi-index dataframe with the results of the metrics. The first index is the testing_data.keys(), the second index are the tp, tn, fp, fn, and the columns are the models.\n",
    "    columns = ['tp', 'tn', 'fp', 'fn', '(hybrid)accuracy', '(hybrid)precision', '(hybrid)recall', '(hybrid)f1']\n",
    "    index = pd.MultiIndex.from_product([testing_data.keys(), ['lr', 'gb', 'xgb', 'svr', 'mlp']], names=['experiment', 'class'])\n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "    classifier_experiments =[experiment for experiment in testing_data.keys() if 'classifier' in experiment.split('_')] # TODO confirm this\n",
    "    regressor_experiments = [experiment for experiment in testing_data.keys() if 'regressor' in experiment.split('_')]\n",
    "    # Classifier experiments\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for experiment in classifier_experiments:\n",
    "        for model in testing_data[experiment].keys():\n",
    "            for bus in testing_data[experiment][model]['predicted'].columns:\n",
    "                try:\n",
    "                    tp += sum((testing_data[experiment][model]['predicted'][bus] == 1) & (testing_data[experiment][model]['real'][bus] == 1))\n",
    "                    tn += sum((testing_data[experiment][model]['predicted'][bus] == 0) & (testing_data[experiment][model]['real'][bus] == 0))\n",
    "                    fp += sum((testing_data[experiment][model]['predicted'][bus] == 1) & (testing_data[experiment][model]['real'][bus] == 0))\n",
    "                    fn += sum((testing_data[experiment][model]['predicted'][bus] == 0) & (testing_data[experiment][model]['real'][bus] == 1))\n",
    "                except: \n",
    "                    print('In the experiment ', experiment, ' and model ', model, ' there was a problem with bus: ', bus)\n",
    "                    if not testing_data[experiment][model]['real'][bus].any():\n",
    "                        print('Bus {} has no positive data points. Just ignore the little shit.'.format(bus))    \n",
    "            df.loc[(experiment, model), 'tp'] = tp\n",
    "            df.loc[(experiment, model), 'tn'] = tn\n",
    "            df.loc[(experiment, model), 'fp'] = fp\n",
    "            df.loc[(experiment, model), 'fn'] = fn\n",
    "            #print('Experiment: {}, model: {}, tp: {}, tn: {}, fp: {}, fn: {}'.format(experiment, model, tp, tn, fp, fn))\n",
    "            if (tp + tn + fp + fn) != 0:\n",
    "                accuracy = (tp + tn ) / (tp + tn + fp + fn)\n",
    "            else: \n",
    "                accuracy = 0\n",
    "            if (tp + fp) != 0:\n",
    "                precision = tp / (tp + fp)\n",
    "            else:\n",
    "                precision = 0\n",
    "            if (tp + fn) != 0:\n",
    "                recall = tp / (tp + fn)\n",
    "            else:\n",
    "                recall = 0\n",
    "            if (precision + recall) != 0:\n",
    "                f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            else:\n",
    "                f1 = 0\n",
    "            if (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) > 0:\n",
    "                mcc = (tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "            df.loc[(experiment, model), '(hybrid)accuracy'] = accuracy\n",
    "            df.loc[(experiment, model), '(hybrid)precision'] = precision\n",
    "            df.loc[(experiment, model), '(hybrid)recall'] = recall\n",
    "            df.loc[(experiment, model), '(hybrid)f1'] = f1\n",
    "            df.loc[(experiment, model), '(hybrid)mcc'] = mcc\n",
    "            # print('Experiment: {}, model: {}, accuracy: {}, precision: {}, recall: {}, f1: {}'.format(experiment, model, accuracy, precision, recall, f1))\n",
    "            tp = 0\n",
    "            tn = 0\n",
    "            fp = 0\n",
    "            fn = 0 \n",
    "    # Regressor experiments\n",
    "    _threshold = lambda experiment: max_u_threshold if 'max_u' in experiment else min_u_threshold\n",
    "    for experiment in regressor_experiments:\n",
    "        for model in testing_data[experiment].keys():\n",
    "            try:\n",
    "                threshold = _threshold(experiment)\n",
    "                print('Experiment: {}, model: {}, threshold: {}'.format(experiment, model, threshold))\n",
    "                hybrid_metrics = metrics.Metrics()\n",
    "                hybrid_metrics.get_prediction_scores(testing_data[experiment][model]['predicted'], testing_data[experiment][model]['real'], threshold=threshold)\n",
    "                df.loc[(experiment, model), 'tp'] = hybrid_metrics.true_positives_ctr\n",
    "                df.loc[(experiment, model), 'tn'] = hybrid_metrics.true_negatives_ctr\n",
    "                df.loc[(experiment, model), 'fp'] = hybrid_metrics.false_positives_ctr\n",
    "                df.loc[(experiment, model), 'fn'] = hybrid_metrics.false_negatives_ctr\n",
    "                df.loc[(experiment, model), '(hybrid)accuracy'] = hybrid_metrics.hybrid_accuracy\n",
    "                df.loc[(experiment, model), '(hybrid)precision'] = hybrid_metrics.hybrid_precision\n",
    "                df.loc[(experiment, model), '(hybrid)recall'] = hybrid_metrics.hybrid_recall\n",
    "                df.loc[(experiment, model), '(hybrid)f1'] = hybrid_metrics.hybrid_f1\n",
    "                df.loc[(experiment, model), '(hybrid)mcc'] = hybrid_metrics.hybrid_mcc\n",
    "                # print('Experiment: {}, model: {}, tp: {}, tn: {}, fp: {}, fn: {}'.format(experiment, model, hybrid_metrics.true_positives_ctr, hybrid_metrics.true_negatives_ctr, hybrid_metrics.false_positives_ctr, hybrid_metrics.false_negatives_ctr))\n",
    "                # print('Experiment: {}, model: {}, accuracy: {}, precision: {}, recall: {}, f1: {}'.format(experiment, model, hybrid_metrics.hybrid_accuracy, hybrid_metrics.hybrid_precision, hybrid_metrics.hybrid_recall, hybrid_metrics.hybrid_f1))\n",
    "            except(Exception) as e:\n",
    "                print('In the experiment ', experiment, ' and model ', model, ' there was a problem')\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>298021</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.979047</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.479834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3361</td>\n",
       "      <td>299916</td>\n",
       "      <td>2544</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>0.568695</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.613786</td>\n",
       "      <td>0.608879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3983</td>\n",
       "      <td>97550</td>\n",
       "      <td>204910</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.338473</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.790252</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.032998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3885</td>\n",
       "      <td>290445</td>\n",
       "      <td>12015</td>\n",
       "      <td>1151</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>0.257948</td>\n",
       "      <td>0.770778</td>\n",
       "      <td>0.386537</td>\n",
       "      <td>0.431802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>211703</td>\n",
       "      <td>90757</td>\n",
       "      <td>0</td>\n",
       "      <td>1.466242</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099287</td>\n",
       "      <td>-0.275648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>4089</td>\n",
       "      <td>298466</td>\n",
       "      <td>3994</td>\n",
       "      <td>947</td>\n",
       "      <td>0.983766</td>\n",
       "      <td>0.505694</td>\n",
       "      <td>0.81232</td>\n",
       "      <td>0.62334</td>\n",
       "      <td>0.633630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4416</td>\n",
       "      <td>252698</td>\n",
       "      <td>49762</td>\n",
       "      <td>620</td>\n",
       "      <td>0.836891</td>\n",
       "      <td>0.081427</td>\n",
       "      <td>0.876656</td>\n",
       "      <td>0.149013</td>\n",
       "      <td>0.237300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4700</td>\n",
       "      <td>71618</td>\n",
       "      <td>230842</td>\n",
       "      <td>336</td>\n",
       "      <td>0.256491</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.933189</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>0.053516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4300</td>\n",
       "      <td>275628</td>\n",
       "      <td>26832</td>\n",
       "      <td>736</td>\n",
       "      <td>0.91493</td>\n",
       "      <td>0.144597</td>\n",
       "      <td>0.85364</td>\n",
       "      <td>0.247304</td>\n",
       "      <td>0.330513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3247</td>\n",
       "      <td>194204</td>\n",
       "      <td>108256</td>\n",
       "      <td>1789</td>\n",
       "      <td>0.631347</td>\n",
       "      <td>0.030686</td>\n",
       "      <td>0.645414</td>\n",
       "      <td>0.058587</td>\n",
       "      <td>0.075512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>80965</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.453823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3260</td>\n",
       "      <td>83067</td>\n",
       "      <td>2337</td>\n",
       "      <td>1776</td>\n",
       "      <td>0.954708</td>\n",
       "      <td>0.581068</td>\n",
       "      <td>0.646568</td>\n",
       "      <td>0.612071</td>\n",
       "      <td>0.589039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4315</td>\n",
       "      <td>71932</td>\n",
       "      <td>13472</td>\n",
       "      <td>721</td>\n",
       "      <td>0.843229</td>\n",
       "      <td>0.241617</td>\n",
       "      <td>0.85642</td>\n",
       "      <td>0.376901</td>\n",
       "      <td>0.402428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3613</td>\n",
       "      <td>75525</td>\n",
       "      <td>9879</td>\n",
       "      <td>1423</td>\n",
       "      <td>0.875203</td>\n",
       "      <td>0.266493</td>\n",
       "      <td>0.716784</td>\n",
       "      <td>0.388533</td>\n",
       "      <td>0.386221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>22</td>\n",
       "      <td>85382</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.05415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.004214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4513</td>\n",
       "      <td>282576</td>\n",
       "      <td>19884</td>\n",
       "      <td>523</td>\n",
       "      <td>0.933826</td>\n",
       "      <td>0.184598</td>\n",
       "      <td>0.895985</td>\n",
       "      <td>0.306125</td>\n",
       "      <td>0.389575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4109</td>\n",
       "      <td>297630</td>\n",
       "      <td>4830</td>\n",
       "      <td>927</td>\n",
       "      <td>0.981642</td>\n",
       "      <td>0.463762</td>\n",
       "      <td>0.815659</td>\n",
       "      <td>0.591317</td>\n",
       "      <td>0.607091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4242</td>\n",
       "      <td>87755</td>\n",
       "      <td>214705</td>\n",
       "      <td>794</td>\n",
       "      <td>0.304284</td>\n",
       "      <td>0.019722</td>\n",
       "      <td>0.842058</td>\n",
       "      <td>0.038541</td>\n",
       "      <td>0.038495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3974</td>\n",
       "      <td>290777</td>\n",
       "      <td>11683</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.959239</td>\n",
       "      <td>0.258391</td>\n",
       "      <td>0.789028</td>\n",
       "      <td>0.389296</td>\n",
       "      <td>0.437418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5008</td>\n",
       "      <td>226465</td>\n",
       "      <td>75995</td>\n",
       "      <td>28</td>\n",
       "      <td>1.008493</td>\n",
       "      <td>0.061813</td>\n",
       "      <td>0.994026</td>\n",
       "      <td>0.116388</td>\n",
       "      <td>-0.248941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>2598</td>\n",
       "      <td>83124</td>\n",
       "      <td>2280</td>\n",
       "      <td>2438</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>0.532595</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.524107</td>\n",
       "      <td>0.496589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1867</td>\n",
       "      <td>84013</td>\n",
       "      <td>1391</td>\n",
       "      <td>3169</td>\n",
       "      <td>0.94958</td>\n",
       "      <td>0.573051</td>\n",
       "      <td>0.370731</td>\n",
       "      <td>0.450205</td>\n",
       "      <td>0.436154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>0</td>\n",
       "      <td>85404</td>\n",
       "      <td>0</td>\n",
       "      <td>5036</td>\n",
       "      <td>0.944317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3990</td>\n",
       "      <td>59437</td>\n",
       "      <td>25967</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.701316</td>\n",
       "      <td>0.133191</td>\n",
       "      <td>0.792295</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.237879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3546</td>\n",
       "      <td>81683</td>\n",
       "      <td>3721</td>\n",
       "      <td>1490</td>\n",
       "      <td>0.942382</td>\n",
       "      <td>0.487959</td>\n",
       "      <td>0.70413</td>\n",
       "      <td>0.576445</td>\n",
       "      <td>0.557219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3180</td>\n",
       "      <td>83080</td>\n",
       "      <td>2324</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.953782</td>\n",
       "      <td>0.577762</td>\n",
       "      <td>0.631454</td>\n",
       "      <td>0.603416</td>\n",
       "      <td>0.579572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4029</td>\n",
       "      <td>80498</td>\n",
       "      <td>4906</td>\n",
       "      <td>1007</td>\n",
       "      <td>0.93462</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>0.80004</td>\n",
       "      <td>0.576766</td>\n",
       "      <td>0.570683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3967</td>\n",
       "      <td>49322</td>\n",
       "      <td>36082</td>\n",
       "      <td>1069</td>\n",
       "      <td>0.589219</td>\n",
       "      <td>0.099054</td>\n",
       "      <td>0.787728</td>\n",
       "      <td>0.175979</td>\n",
       "      <td>0.168613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>297640</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.976169</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.391930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5039</td>\n",
       "      <td>294700</td>\n",
       "      <td>6778</td>\n",
       "      <td>979</td>\n",
       "      <td>0.974861</td>\n",
       "      <td>0.426091</td>\n",
       "      <td>0.837325</td>\n",
       "      <td>0.564781</td>\n",
       "      <td>0.586979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4781</td>\n",
       "      <td>293809</td>\n",
       "      <td>7669</td>\n",
       "      <td>1237</td>\n",
       "      <td>0.971135</td>\n",
       "      <td>0.383609</td>\n",
       "      <td>0.794475</td>\n",
       "      <td>0.517396</td>\n",
       "      <td>0.540216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5208</td>\n",
       "      <td>288486</td>\n",
       "      <td>12992</td>\n",
       "      <td>810</td>\n",
       "      <td>0.955228</td>\n",
       "      <td>0.285838</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>0.42972</td>\n",
       "      <td>0.482436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6011</td>\n",
       "      <td>229483</td>\n",
       "      <td>71995</td>\n",
       "      <td>7</td>\n",
       "      <td>1.023041</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>-0.280415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>5929</td>\n",
       "      <td>242474</td>\n",
       "      <td>59004</td>\n",
       "      <td>89</td>\n",
       "      <td>0.808951</td>\n",
       "      <td>0.091561</td>\n",
       "      <td>0.985199</td>\n",
       "      <td>0.167551</td>\n",
       "      <td>0.268520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5657</td>\n",
       "      <td>257440</td>\n",
       "      <td>44038</td>\n",
       "      <td>361</td>\n",
       "      <td>0.856304</td>\n",
       "      <td>0.114014</td>\n",
       "      <td>0.939976</td>\n",
       "      <td>0.203362</td>\n",
       "      <td>0.299146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5721</td>\n",
       "      <td>213689</td>\n",
       "      <td>87789</td>\n",
       "      <td>297</td>\n",
       "      <td>0.714145</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>0.95063</td>\n",
       "      <td>0.115166</td>\n",
       "      <td>0.198854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5779</td>\n",
       "      <td>261411</td>\n",
       "      <td>40067</td>\n",
       "      <td>239</td>\n",
       "      <td>0.869434</td>\n",
       "      <td>0.126096</td>\n",
       "      <td>0.960264</td>\n",
       "      <td>0.222919</td>\n",
       "      <td>0.321932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>2425</td>\n",
       "      <td>203328</td>\n",
       "      <td>98150</td>\n",
       "      <td>3593</td>\n",
       "      <td>0.660005</td>\n",
       "      <td>0.025915</td>\n",
       "      <td>0.40367</td>\n",
       "      <td>0.048704</td>\n",
       "      <td>0.021320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>80584</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.918923</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.360620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4997</td>\n",
       "      <td>77824</td>\n",
       "      <td>6598</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.430647</td>\n",
       "      <td>0.830352</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>0.560655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4950</td>\n",
       "      <td>78375</td>\n",
       "      <td>6047</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.921556</td>\n",
       "      <td>0.449796</td>\n",
       "      <td>0.822501</td>\n",
       "      <td>0.581558</td>\n",
       "      <td>0.572522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5100</td>\n",
       "      <td>72437</td>\n",
       "      <td>11985</td>\n",
       "      <td>918</td>\n",
       "      <td>0.857577</td>\n",
       "      <td>0.298173</td>\n",
       "      <td>0.847308</td>\n",
       "      <td>0.441115</td>\n",
       "      <td>0.448986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5751</td>\n",
       "      <td>30442</td>\n",
       "      <td>53980</td>\n",
       "      <td>267</td>\n",
       "      <td>0.406414</td>\n",
       "      <td>0.098079</td>\n",
       "      <td>0.954641</td>\n",
       "      <td>0.177883</td>\n",
       "      <td>0.169389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4817</td>\n",
       "      <td>285372</td>\n",
       "      <td>16106</td>\n",
       "      <td>1201</td>\n",
       "      <td>0.943881</td>\n",
       "      <td>0.229869</td>\n",
       "      <td>0.800296</td>\n",
       "      <td>0.357153</td>\n",
       "      <td>0.410611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5032</td>\n",
       "      <td>291795</td>\n",
       "      <td>9683</td>\n",
       "      <td>986</td>\n",
       "      <td>0.965425</td>\n",
       "      <td>0.341713</td>\n",
       "      <td>0.836223</td>\n",
       "      <td>0.485168</td>\n",
       "      <td>0.521648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5270</td>\n",
       "      <td>82675</td>\n",
       "      <td>218803</td>\n",
       "      <td>748</td>\n",
       "      <td>0.289631</td>\n",
       "      <td>0.023833</td>\n",
       "      <td>0.875709</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.047840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5168</td>\n",
       "      <td>288985</td>\n",
       "      <td>12493</td>\n",
       "      <td>850</td>\n",
       "      <td>0.956723</td>\n",
       "      <td>0.292504</td>\n",
       "      <td>0.858663</td>\n",
       "      <td>0.436361</td>\n",
       "      <td>0.486502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6015</td>\n",
       "      <td>222433</td>\n",
       "      <td>79045</td>\n",
       "      <td>3</td>\n",
       "      <td>1.399868</td>\n",
       "      <td>0.068243</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>0.127762</td>\n",
       "      <td>-0.307781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4149</td>\n",
       "      <td>80988</td>\n",
       "      <td>3434</td>\n",
       "      <td>1869</td>\n",
       "      <td>0.941364</td>\n",
       "      <td>0.547145</td>\n",
       "      <td>0.689432</td>\n",
       "      <td>0.610102</td>\n",
       "      <td>0.583377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4092</td>\n",
       "      <td>80554</td>\n",
       "      <td>3868</td>\n",
       "      <td>1926</td>\n",
       "      <td>0.935935</td>\n",
       "      <td>0.51407</td>\n",
       "      <td>0.67996</td>\n",
       "      <td>0.585491</td>\n",
       "      <td>0.557840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>2976</td>\n",
       "      <td>82972</td>\n",
       "      <td>1450</td>\n",
       "      <td>3042</td>\n",
       "      <td>0.950332</td>\n",
       "      <td>0.67239</td>\n",
       "      <td>0.494516</td>\n",
       "      <td>0.569897</td>\n",
       "      <td>0.551432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5323</td>\n",
       "      <td>47507</td>\n",
       "      <td>36915</td>\n",
       "      <td>695</td>\n",
       "      <td>0.584144</td>\n",
       "      <td>0.126024</td>\n",
       "      <td>0.884513</td>\n",
       "      <td>0.220615</td>\n",
       "      <td>0.223417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4478</td>\n",
       "      <td>77802</td>\n",
       "      <td>6620</td>\n",
       "      <td>1540</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.403496</td>\n",
       "      <td>0.744101</td>\n",
       "      <td>0.523253</td>\n",
       "      <td>0.505649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4267</td>\n",
       "      <td>78129</td>\n",
       "      <td>6293</td>\n",
       "      <td>1751</td>\n",
       "      <td>0.911057</td>\n",
       "      <td>0.404072</td>\n",
       "      <td>0.70904</td>\n",
       "      <td>0.514779</td>\n",
       "      <td>0.492417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4934</td>\n",
       "      <td>79490</td>\n",
       "      <td>4932</td>\n",
       "      <td>1084</td>\n",
       "      <td>0.933481</td>\n",
       "      <td>0.500101</td>\n",
       "      <td>0.819874</td>\n",
       "      <td>0.621254</td>\n",
       "      <td>0.608736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5425</td>\n",
       "      <td>46582</td>\n",
       "      <td>37840</td>\n",
       "      <td>593</td>\n",
       "      <td>0.575044</td>\n",
       "      <td>0.12539</td>\n",
       "      <td>0.901462</td>\n",
       "      <td>0.220157</td>\n",
       "      <td>0.226129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tp      tn      fp    fn (hybrid)accuracy  \\\n",
       "experiment                class                                                \n",
       "max_u_regressor_sparse    lr     3011  298021    4439  2025         0.979047   \n",
       "                          gb     3361  299916    2544  1675         0.986368   \n",
       "                          xgb    3983   97550  204910  1053         0.338473   \n",
       "                          svr    3885  290445   12015  1151         0.960099   \n",
       "                          mlp    5036  211703   90757     0         1.466242   \n",
       "max_u_regressor_focused   lr     4089  298466    3994   947         0.983766   \n",
       "                          gb     4416  252698   49762   620         0.836891   \n",
       "                          xgb    4700   71618  230842   336         0.256491   \n",
       "                          svr    4300  275628   26832   736          0.91493   \n",
       "                          mlp    3247  194204  108256  1789         0.631347   \n",
       "max_u_filtered_regressor  lr     3011   80965    4439  2025         0.928714   \n",
       "                          gb     3260   83067    2337  1776         0.954708   \n",
       "                          xgb    4315   71932   13472   721         0.843229   \n",
       "                          svr    3613   75525    9879  1423         0.875203   \n",
       "                          mlp    5036      22   85382     0         0.054443   \n",
       "max_u_regressor_balanced  lr     4513  282576   19884   523         0.933826   \n",
       "                          gb     4109  297630    4830   927         0.981642   \n",
       "                          xgb    4242   87755  214705   794         0.304284   \n",
       "                          svr    3974  290777   11683  1062         0.959239   \n",
       "                          mlp    5008  226465   75995    28         1.008493   \n",
       "max_u_classifier          lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     2598   83124    2280  2438         0.947833   \n",
       "                          xgb    1867   84013    1391  3169          0.94958   \n",
       "                          svr       0   85404       0  5036         0.944317   \n",
       "                          mlp    3990   59437   25967  1046         0.701316   \n",
       "max_u_classifier_balanced lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     3546   81683    3721  1490         0.942382   \n",
       "                          xgb    3180   83080    2324  1856         0.953782   \n",
       "                          svr    4029   80498    4906  1007          0.93462   \n",
       "                          mlp    3967   49322   36082  1069         0.589219   \n",
       "min_u_regressor_sparse    lr     2502  297640    3838  3516         0.976169   \n",
       "                          gb     5039  294700    6778   979         0.974861   \n",
       "                          xgb    4781  293809    7669  1237         0.971135   \n",
       "                          svr    5208  288486   12992   810         0.955228   \n",
       "                          mlp    6011  229483   71995     7         1.023041   \n",
       "min_u_regressor_focused   lr     5929  242474   59004    89         0.808951   \n",
       "                          gb     5657  257440   44038   361         0.856304   \n",
       "                          xgb    5721  213689   87789   297         0.714145   \n",
       "                          svr    5779  261411   40067   239         0.869434   \n",
       "                          mlp    2425  203328   98150  3593         0.660005   \n",
       "min_u_filtered_regressor  lr     2502   80584    3838  3516         0.918923   \n",
       "                          gb     4997   77824    6598  1021         0.916002   \n",
       "                          xgb    4950   78375    6047  1068         0.921556   \n",
       "                          svr    5100   72437   11985   918         0.857577   \n",
       "                          mlp    5751   30442   53980   267         0.406414   \n",
       "min_u_regressor_balanced  lr     4817  285372   16106  1201         0.943881   \n",
       "                          gb     5032  291795    9683   986         0.965425   \n",
       "                          xgb    5270   82675  218803   748         0.289631   \n",
       "                          svr    5168  288985   12493   850         0.956723   \n",
       "                          mlp    6015  222433   79045     3         1.399868   \n",
       "min_u_classifier          lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     4149   80988    3434  1869         0.941364   \n",
       "                          xgb    4092   80554    3868  1926         0.935935   \n",
       "                          svr    2976   82972    1450  3042         0.950332   \n",
       "                          mlp    5323   47507   36915   695         0.584144   \n",
       "min_u_classifier_balanced lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     4478   77802    6620  1540         0.909774   \n",
       "                          xgb    4267   78129    6293  1751         0.911057   \n",
       "                          svr    4934   79490    4932  1084         0.933481   \n",
       "                          mlp    5425   46582   37840   593         0.575044   \n",
       "\n",
       "                                (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment                class                                               \n",
       "max_u_regressor_sparse    lr             0.402003       0.597092   0.480501   \n",
       "                          gb             0.568695       0.666644   0.613786   \n",
       "                          xgb             0.01963       0.790252   0.038308   \n",
       "                          svr            0.257948       0.770778   0.386537   \n",
       "                          mlp            0.052237            1.0   0.099287   \n",
       "max_u_regressor_focused   lr             0.505694        0.81232    0.62334   \n",
       "                          gb             0.081427       0.876656   0.149013   \n",
       "                          xgb            0.020734       0.933189   0.040567   \n",
       "                          svr            0.144597        0.85364   0.247304   \n",
       "                          mlp            0.030686       0.645414   0.058587   \n",
       "max_u_filtered_regressor  lr             0.402003       0.597092   0.480501   \n",
       "                          gb             0.581068       0.646568   0.612071   \n",
       "                          xgb            0.241617        0.85642   0.376901   \n",
       "                          svr            0.266493       0.716784   0.388533   \n",
       "                          mlp             0.05415            1.0   0.102736   \n",
       "max_u_regressor_balanced  lr             0.184598       0.895985   0.306125   \n",
       "                          gb             0.463762       0.815659   0.591317   \n",
       "                          xgb            0.019722       0.842058   0.038541   \n",
       "                          svr            0.258391       0.789028   0.389296   \n",
       "                          mlp            0.061813       0.994026   0.116388   \n",
       "max_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.532595       0.515886   0.524107   \n",
       "                          xgb            0.573051       0.370731   0.450205   \n",
       "                          svr                   0            0.0          0   \n",
       "                          mlp            0.133191       0.792295   0.228046   \n",
       "max_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.487959        0.70413   0.576445   \n",
       "                          xgb            0.577762       0.631454   0.603416   \n",
       "                          svr            0.450923        0.80004   0.576766   \n",
       "                          mlp            0.099054       0.787728   0.175979   \n",
       "min_u_regressor_sparse    lr             0.393263       0.415191   0.403929   \n",
       "                          gb             0.426091       0.837325   0.564781   \n",
       "                          xgb            0.383609       0.794475   0.517396   \n",
       "                          svr            0.285838       0.865268    0.42972   \n",
       "                          mlp            0.076957       0.998749   0.142903   \n",
       "min_u_regressor_focused   lr             0.091561       0.985199   0.167551   \n",
       "                          gb             0.114014       0.939976   0.203362   \n",
       "                          xgb            0.061296        0.95063   0.115166   \n",
       "                          svr            0.126096       0.960264   0.222919   \n",
       "                          mlp            0.025915        0.40367   0.048704   \n",
       "min_u_filtered_regressor  lr             0.393263       0.415191   0.403929   \n",
       "                          gb             0.430647       0.830352   0.567151   \n",
       "                          xgb            0.449796       0.822501   0.581558   \n",
       "                          svr            0.298173       0.847308   0.441115   \n",
       "                          mlp            0.098079       0.954641   0.177883   \n",
       "min_u_regressor_balanced  lr             0.229869       0.800296   0.357153   \n",
       "                          gb             0.341713       0.836223   0.485168   \n",
       "                          xgb            0.023833       0.875709   0.046404   \n",
       "                          svr            0.292504       0.858663   0.436361   \n",
       "                          mlp            0.068243       0.999402   0.127762   \n",
       "min_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.547145       0.689432   0.610102   \n",
       "                          xgb             0.51407        0.67996   0.585491   \n",
       "                          svr             0.67239       0.494516   0.569897   \n",
       "                          mlp            0.126024       0.884513   0.220615   \n",
       "min_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.403496       0.744101   0.523253   \n",
       "                          xgb            0.404072        0.70904   0.514779   \n",
       "                          svr            0.500101       0.819874   0.621254   \n",
       "                          mlp             0.12539       0.901462   0.220157   \n",
       "\n",
       "                                 (hybrid)mcc  \n",
       "experiment                class               \n",
       "max_u_regressor_sparse    lr        0.479834  \n",
       "                          gb        0.608879  \n",
       "                          xgb       0.032998  \n",
       "                          svr       0.431802  \n",
       "                          mlp      -0.275648  \n",
       "max_u_regressor_focused   lr        0.633630  \n",
       "                          gb        0.237300  \n",
       "                          xgb       0.053516  \n",
       "                          svr       0.330513  \n",
       "                          mlp       0.075512  \n",
       "max_u_filtered_regressor  lr        0.453823  \n",
       "                          gb        0.589039  \n",
       "                          xgb       0.402428  \n",
       "                          svr       0.386221  \n",
       "                          mlp       0.004214  \n",
       "max_u_regressor_balanced  lr        0.389575  \n",
       "                          gb        0.607091  \n",
       "                          xgb       0.038495  \n",
       "                          svr       0.437418  \n",
       "                          mlp      -0.248941  \n",
       "max_u_classifier          lr             NaN  \n",
       "                          gb        0.496589  \n",
       "                          xgb       0.436154  \n",
       "                          svr       0.436154  \n",
       "                          mlp       0.237879  \n",
       "max_u_classifier_balanced lr             NaN  \n",
       "                          gb        0.557219  \n",
       "                          xgb       0.579572  \n",
       "                          svr       0.570683  \n",
       "                          mlp       0.168613  \n",
       "min_u_regressor_sparse    lr        0.391930  \n",
       "                          gb        0.586979  \n",
       "                          xgb       0.540216  \n",
       "                          svr       0.482436  \n",
       "                          mlp      -0.280415  \n",
       "min_u_regressor_focused   lr        0.268520  \n",
       "                          gb        0.299146  \n",
       "                          xgb       0.198854  \n",
       "                          svr       0.321932  \n",
       "                          mlp       0.021320  \n",
       "min_u_filtered_regressor  lr        0.360620  \n",
       "                          gb        0.560655  \n",
       "                          xgb       0.572522  \n",
       "                          svr       0.448986  \n",
       "                          mlp       0.169389  \n",
       "min_u_regressor_balanced  lr        0.410611  \n",
       "                          gb        0.521648  \n",
       "                          xgb       0.047840  \n",
       "                          svr       0.486502  \n",
       "                          mlp      -0.307781  \n",
       "min_u_classifier          lr             NaN  \n",
       "                          gb        0.583377  \n",
       "                          xgb       0.557840  \n",
       "                          svr       0.551432  \n",
       "                          mlp       0.223417  \n",
       "min_u_classifier_balanced lr             NaN  \n",
       "                          gb        0.505649  \n",
       "                          xgb       0.492417  \n",
       "                          svr       0.608736  \n",
       "                          mlp       0.226129  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>possible_positives</th>\n",
       "      <th>possible_negatives</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>302460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>85404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6018</td>\n",
       "      <td>84422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                possible_positives possible_negatives\n",
       "experiment                class                                      \n",
       "max_u_regressor_sparse    lr                  5036             302460\n",
       "                          gb                  5036             302460\n",
       "                          xgb                 5036             302460\n",
       "                          svr                 5036             302460\n",
       "                          mlp                 5036             302460\n",
       "max_u_regressor_focused   lr                  5036             302460\n",
       "                          gb                  5036             302460\n",
       "                          xgb                 5036             302460\n",
       "                          svr                 5036             302460\n",
       "                          mlp                 5036             302460\n",
       "max_u_filtered_regressor  lr                  5036              85404\n",
       "                          gb                  5036              85404\n",
       "                          xgb                 5036              85404\n",
       "                          svr                 5036              85404\n",
       "                          mlp                 5036              85404\n",
       "max_u_regressor_balanced  lr                  5036             302460\n",
       "                          gb                  5036             302460\n",
       "                          xgb                 5036             302460\n",
       "                          svr                 5036             302460\n",
       "                          mlp                 5036             302460\n",
       "max_u_classifier          lr                   NaN                NaN\n",
       "                          gb                  5036              85404\n",
       "                          xgb                 5036              85404\n",
       "                          svr                 5036              85404\n",
       "                          mlp                 5036              85404\n",
       "max_u_classifier_balanced lr                   NaN                NaN\n",
       "                          gb                  5036              85404\n",
       "                          xgb                 5036              85404\n",
       "                          svr                 5036              85404\n",
       "                          mlp                 5036              85404\n",
       "min_u_regressor_sparse    lr                  6018             301478\n",
       "                          gb                  6018             301478\n",
       "                          xgb                 6018             301478\n",
       "                          svr                 6018             301478\n",
       "                          mlp                 6018             301478\n",
       "min_u_regressor_focused   lr                  6018             301478\n",
       "                          gb                  6018             301478\n",
       "                          xgb                 6018             301478\n",
       "                          svr                 6018             301478\n",
       "                          mlp                 6018             301478\n",
       "min_u_filtered_regressor  lr                  6018              84422\n",
       "                          gb                  6018              84422\n",
       "                          xgb                 6018              84422\n",
       "                          svr                 6018              84422\n",
       "                          mlp                 6018              84422\n",
       "min_u_regressor_balanced  lr                  6018             301478\n",
       "                          gb                  6018             301478\n",
       "                          xgb                 6018             301478\n",
       "                          svr                 6018             301478\n",
       "                          mlp                 6018             301478\n",
       "min_u_classifier          lr                   NaN                NaN\n",
       "                          gb                  6018              84422\n",
       "                          xgb                 6018              84422\n",
       "                          svr                 6018              84422\n",
       "                          mlp                 6018              84422\n",
       "min_u_classifier_balanced lr                   NaN                NaN\n",
       "                          gb                  6018              84422\n",
       "                          xgb                 6018              84422\n",
       "                          svr                 6018              84422\n",
       "                          mlp                 6018              84422"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmation_df = pd.DataFrame()\n",
    "confirmation_df['possible_positives'] = df['tp'] + df['fn']\n",
    "confirmation_df['possible_negatives'] = df['fp'] + df['tn']\n",
    "confirmation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to evaluate all the results obtained above. This benchmarking has the objective of obtaining the answer following questions:\n",
    "- What is the optimum number of rows for the training data set? What is the respective model?\n",
    "    - sparse reg. vs balanced reg. vs focused reg.\n",
    "- What is the optimum number present busses in regresison?\n",
    "    - sparse reg. vs filtered reg.\n",
    "- Regression vs Classification\n",
    "    - filtered reg. vs sparse class.\n",
    "- What is the optimum number of rows in class?\n",
    "    - sparse class. vs balanced class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimum number of rows for the training data set? What is the respective model?\n",
    "In order to understand the optinum number of rows for the training set of the regression data set the data sets used will be:\n",
    "\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Maximum Voltage Constraints|Balanced Regression|6971|34|\n",
    "|Maximum Voltage Constraints|Focused Regression|3486|34|\n",
    "|Minimum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Minimum Voltage Constraints|Balanced Regression|13917|34|\n",
    "|Minimum Voltage Constraints|Focused Regression|6958|34|\n",
    "\n",
    "The **Sparse Regression data set** is generated directly from the power flow results. The important moments are those where the constraints are violated, so the output feature contains null values for when there is no constraint, and positive value for when there is a constraint. The positive values represent the amplitude of the constraint violation. It can be expressed as follows:\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\text{Target} &= \\begin{cases}\n",
    "            0 & \\text{if} \\; \\text{constraint} \\; \\text{is not violated} \\\\\n",
    "            \\text{amplitude of constraint} & \\text{if} \\; \\text{constraint} \\; \\text{is violated} \\\\\n",
    "        \\end{cases}\n",
    "    \\end{align}\n",
    "$$\n",
    "In our case, the constraints are being considered as the following:\n",
    "- Minimal voltage on bus: $v_bus < 0.95 \\text{ [pu]}$ (constraint is violated if the voltage is below $0.95 \\text{ [pu]} $)\n",
    "- Maximal voltage on bus: $v_bus > 1.05 \\text{ [pu]}$ (constraint is violated if the voltage is above $1.05 \\text{ [pu]} $)\n",
    "- Maximal current on line: $i_{line} > 1 \\text{ [kA]}$ (constraint is violated if the current is above $1 \\text{ [kA]} $)\n",
    "\n",
    "The **Balanced Regression** data set is created from the **Sparse Regression data set**. It is created by taking all the rows that containt at least one constraint violation and then taking the same number of rows that do not contain any constraint violation. Finally, the **Focused Regression data set** is created by taking all the rows that contain at least one constraint violation.\n",
    "\n",
    "Since these data sets have the same number of possible negative and possible positives, all the metrics can be used to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>298021</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.979047</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.479834</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3361</td>\n",
       "      <td>299916</td>\n",
       "      <td>2544</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>0.568695</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.613786</td>\n",
       "      <td>0.608879</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3983</td>\n",
       "      <td>97550</td>\n",
       "      <td>204910</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.338473</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.790252</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3885</td>\n",
       "      <td>290445</td>\n",
       "      <td>12015</td>\n",
       "      <td>1151</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>0.257948</td>\n",
       "      <td>0.770778</td>\n",
       "      <td>0.386537</td>\n",
       "      <td>0.431802</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>211703</td>\n",
       "      <td>90757</td>\n",
       "      <td>0</td>\n",
       "      <td>1.466242</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099287</td>\n",
       "      <td>-0.275648</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4513</td>\n",
       "      <td>282576</td>\n",
       "      <td>19884</td>\n",
       "      <td>523</td>\n",
       "      <td>0.933826</td>\n",
       "      <td>0.184598</td>\n",
       "      <td>0.895985</td>\n",
       "      <td>0.306125</td>\n",
       "      <td>0.389575</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4109</td>\n",
       "      <td>297630</td>\n",
       "      <td>4830</td>\n",
       "      <td>927</td>\n",
       "      <td>0.981642</td>\n",
       "      <td>0.463762</td>\n",
       "      <td>0.815659</td>\n",
       "      <td>0.591317</td>\n",
       "      <td>0.607091</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4242</td>\n",
       "      <td>87755</td>\n",
       "      <td>214705</td>\n",
       "      <td>794</td>\n",
       "      <td>0.304284</td>\n",
       "      <td>0.019722</td>\n",
       "      <td>0.842058</td>\n",
       "      <td>0.038541</td>\n",
       "      <td>0.038495</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3974</td>\n",
       "      <td>290777</td>\n",
       "      <td>11683</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.959239</td>\n",
       "      <td>0.258391</td>\n",
       "      <td>0.789028</td>\n",
       "      <td>0.389296</td>\n",
       "      <td>0.437418</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5008</td>\n",
       "      <td>226465</td>\n",
       "      <td>75995</td>\n",
       "      <td>28</td>\n",
       "      <td>1.008493</td>\n",
       "      <td>0.061813</td>\n",
       "      <td>0.994026</td>\n",
       "      <td>0.116388</td>\n",
       "      <td>-0.248941</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>4089</td>\n",
       "      <td>298466</td>\n",
       "      <td>3994</td>\n",
       "      <td>947</td>\n",
       "      <td>0.983766</td>\n",
       "      <td>0.505694</td>\n",
       "      <td>0.81232</td>\n",
       "      <td>0.62334</td>\n",
       "      <td>0.633630</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4416</td>\n",
       "      <td>252698</td>\n",
       "      <td>49762</td>\n",
       "      <td>620</td>\n",
       "      <td>0.836891</td>\n",
       "      <td>0.081427</td>\n",
       "      <td>0.876656</td>\n",
       "      <td>0.149013</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4700</td>\n",
       "      <td>71618</td>\n",
       "      <td>230842</td>\n",
       "      <td>336</td>\n",
       "      <td>0.256491</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.933189</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>0.053516</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4300</td>\n",
       "      <td>275628</td>\n",
       "      <td>26832</td>\n",
       "      <td>736</td>\n",
       "      <td>0.91493</td>\n",
       "      <td>0.144597</td>\n",
       "      <td>0.85364</td>\n",
       "      <td>0.247304</td>\n",
       "      <td>0.330513</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3247</td>\n",
       "      <td>194204</td>\n",
       "      <td>108256</td>\n",
       "      <td>1789</td>\n",
       "      <td>0.631347</td>\n",
       "      <td>0.030686</td>\n",
       "      <td>0.645414</td>\n",
       "      <td>0.058587</td>\n",
       "      <td>0.075512</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp      tn      fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                                \n",
       "max_u_regressor_sparse   lr     3011  298021    4439  2025         0.979047   \n",
       "                         gb     3361  299916    2544  1675         0.986368   \n",
       "                         xgb    3983   97550  204910  1053         0.338473   \n",
       "                         svr    3885  290445   12015  1151         0.960099   \n",
       "                         mlp    5036  211703   90757     0         1.466242   \n",
       "max_u_regressor_balanced lr     4513  282576   19884   523         0.933826   \n",
       "                         gb     4109  297630    4830   927         0.981642   \n",
       "                         xgb    4242   87755  214705   794         0.304284   \n",
       "                         svr    3974  290777   11683  1062         0.959239   \n",
       "                         mlp    5008  226465   75995    28         1.008493   \n",
       "max_u_regressor_focused  lr     4089  298466    3994   947         0.983766   \n",
       "                         gb     4416  252698   49762   620         0.836891   \n",
       "                         xgb    4700   71618  230842   336         0.256491   \n",
       "                         svr    4300  275628   26832   736          0.91493   \n",
       "                         mlp    3247  194204  108256  1789         0.631347   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "max_u_regressor_sparse   lr             0.402003       0.597092   0.480501   \n",
       "                         gb             0.568695       0.666644   0.613786   \n",
       "                         xgb             0.01963       0.790252   0.038308   \n",
       "                         svr            0.257948       0.770778   0.386537   \n",
       "                         mlp            0.052237            1.0   0.099287   \n",
       "max_u_regressor_balanced lr             0.184598       0.895985   0.306125   \n",
       "                         gb             0.463762       0.815659   0.591317   \n",
       "                         xgb            0.019722       0.842058   0.038541   \n",
       "                         svr            0.258391       0.789028   0.389296   \n",
       "                         mlp            0.061813       0.994026   0.116388   \n",
       "max_u_regressor_focused  lr             0.505694        0.81232    0.62334   \n",
       "                         gb             0.081427       0.876656   0.149013   \n",
       "                         xgb            0.020734       0.933189   0.040567   \n",
       "                         svr            0.144597        0.85364   0.247304   \n",
       "                         mlp            0.030686       0.645414   0.058587   \n",
       "\n",
       "                                (hybrid)mcc         q   f1_coin  \n",
       "experiment               class                                   \n",
       "max_u_regressor_sparse   lr        0.479834  0.016377  0.032227  \n",
       "                         gb        0.608879  0.016377  0.032227  \n",
       "                         xgb       0.032998  0.016377  0.032227  \n",
       "                         svr       0.431802  0.016377  0.032227  \n",
       "                         mlp      -0.275648  0.016377  0.032227  \n",
       "max_u_regressor_balanced lr        0.389575  0.016377  0.032227  \n",
       "                         gb        0.607091  0.016377  0.032227  \n",
       "                         xgb       0.038495  0.016377  0.032227  \n",
       "                         svr       0.437418  0.016377  0.032227  \n",
       "                         mlp      -0.248941  0.016377  0.032227  \n",
       "max_u_regressor_focused  lr        0.633630  0.016377  0.032227  \n",
       "                         gb        0.237300  0.016377  0.032227  \n",
       "                         xgb       0.053516  0.016377  0.032227  \n",
       "                         svr       0.330513  0.016377  0.032227  \n",
       "                         mlp       0.075512  0.016377  0.032227  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['max_u_regressor_sparse', 'max_u_regressor_balanced', 'max_u_regressor_focused']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best alternative is to use the focused data set with a the linear regression model, because it presents a the best values for F1 and MCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>297640</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.976169</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.391930</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5039</td>\n",
       "      <td>294700</td>\n",
       "      <td>6778</td>\n",
       "      <td>979</td>\n",
       "      <td>0.974861</td>\n",
       "      <td>0.426091</td>\n",
       "      <td>0.837325</td>\n",
       "      <td>0.564781</td>\n",
       "      <td>0.586979</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4781</td>\n",
       "      <td>293809</td>\n",
       "      <td>7669</td>\n",
       "      <td>1237</td>\n",
       "      <td>0.971135</td>\n",
       "      <td>0.383609</td>\n",
       "      <td>0.794475</td>\n",
       "      <td>0.517396</td>\n",
       "      <td>0.540216</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5208</td>\n",
       "      <td>288486</td>\n",
       "      <td>12992</td>\n",
       "      <td>810</td>\n",
       "      <td>0.955228</td>\n",
       "      <td>0.285838</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>0.42972</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6011</td>\n",
       "      <td>229483</td>\n",
       "      <td>71995</td>\n",
       "      <td>7</td>\n",
       "      <td>1.023041</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>-0.280415</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4817</td>\n",
       "      <td>285372</td>\n",
       "      <td>16106</td>\n",
       "      <td>1201</td>\n",
       "      <td>0.943881</td>\n",
       "      <td>0.229869</td>\n",
       "      <td>0.800296</td>\n",
       "      <td>0.357153</td>\n",
       "      <td>0.410611</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5032</td>\n",
       "      <td>291795</td>\n",
       "      <td>9683</td>\n",
       "      <td>986</td>\n",
       "      <td>0.965425</td>\n",
       "      <td>0.341713</td>\n",
       "      <td>0.836223</td>\n",
       "      <td>0.485168</td>\n",
       "      <td>0.521648</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5270</td>\n",
       "      <td>82675</td>\n",
       "      <td>218803</td>\n",
       "      <td>748</td>\n",
       "      <td>0.289631</td>\n",
       "      <td>0.023833</td>\n",
       "      <td>0.875709</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.047840</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5168</td>\n",
       "      <td>288985</td>\n",
       "      <td>12493</td>\n",
       "      <td>850</td>\n",
       "      <td>0.956723</td>\n",
       "      <td>0.292504</td>\n",
       "      <td>0.858663</td>\n",
       "      <td>0.436361</td>\n",
       "      <td>0.486502</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6015</td>\n",
       "      <td>222433</td>\n",
       "      <td>79045</td>\n",
       "      <td>3</td>\n",
       "      <td>1.399868</td>\n",
       "      <td>0.068243</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>0.127762</td>\n",
       "      <td>-0.307781</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>5929</td>\n",
       "      <td>242474</td>\n",
       "      <td>59004</td>\n",
       "      <td>89</td>\n",
       "      <td>0.808951</td>\n",
       "      <td>0.091561</td>\n",
       "      <td>0.985199</td>\n",
       "      <td>0.167551</td>\n",
       "      <td>0.268520</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5657</td>\n",
       "      <td>257440</td>\n",
       "      <td>44038</td>\n",
       "      <td>361</td>\n",
       "      <td>0.856304</td>\n",
       "      <td>0.114014</td>\n",
       "      <td>0.939976</td>\n",
       "      <td>0.203362</td>\n",
       "      <td>0.299146</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5721</td>\n",
       "      <td>213689</td>\n",
       "      <td>87789</td>\n",
       "      <td>297</td>\n",
       "      <td>0.714145</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>0.95063</td>\n",
       "      <td>0.115166</td>\n",
       "      <td>0.198854</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5779</td>\n",
       "      <td>261411</td>\n",
       "      <td>40067</td>\n",
       "      <td>239</td>\n",
       "      <td>0.869434</td>\n",
       "      <td>0.126096</td>\n",
       "      <td>0.960264</td>\n",
       "      <td>0.222919</td>\n",
       "      <td>0.321932</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>2425</td>\n",
       "      <td>203328</td>\n",
       "      <td>98150</td>\n",
       "      <td>3593</td>\n",
       "      <td>0.660005</td>\n",
       "      <td>0.025915</td>\n",
       "      <td>0.40367</td>\n",
       "      <td>0.048704</td>\n",
       "      <td>0.021320</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp      tn      fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                                \n",
       "min_u_regressor_sparse   lr     2502  297640    3838  3516         0.976169   \n",
       "                         gb     5039  294700    6778   979         0.974861   \n",
       "                         xgb    4781  293809    7669  1237         0.971135   \n",
       "                         svr    5208  288486   12992   810         0.955228   \n",
       "                         mlp    6011  229483   71995     7         1.023041   \n",
       "min_u_regressor_balanced lr     4817  285372   16106  1201         0.943881   \n",
       "                         gb     5032  291795    9683   986         0.965425   \n",
       "                         xgb    5270   82675  218803   748         0.289631   \n",
       "                         svr    5168  288985   12493   850         0.956723   \n",
       "                         mlp    6015  222433   79045     3         1.399868   \n",
       "min_u_regressor_focused  lr     5929  242474   59004    89         0.808951   \n",
       "                         gb     5657  257440   44038   361         0.856304   \n",
       "                         xgb    5721  213689   87789   297         0.714145   \n",
       "                         svr    5779  261411   40067   239         0.869434   \n",
       "                         mlp    2425  203328   98150  3593         0.660005   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "min_u_regressor_sparse   lr             0.393263       0.415191   0.403929   \n",
       "                         gb             0.426091       0.837325   0.564781   \n",
       "                         xgb            0.383609       0.794475   0.517396   \n",
       "                         svr            0.285838       0.865268    0.42972   \n",
       "                         mlp            0.076957       0.998749   0.142903   \n",
       "min_u_regressor_balanced lr             0.229869       0.800296   0.357153   \n",
       "                         gb             0.341713       0.836223   0.485168   \n",
       "                         xgb            0.023833       0.875709   0.046404   \n",
       "                         svr            0.292504       0.858663   0.436361   \n",
       "                         mlp            0.068243       0.999402   0.127762   \n",
       "min_u_regressor_focused  lr             0.091561       0.985199   0.167551   \n",
       "                         gb             0.114014       0.939976   0.203362   \n",
       "                         xgb            0.061296        0.95063   0.115166   \n",
       "                         svr            0.126096       0.960264   0.222919   \n",
       "                         mlp            0.025915        0.40367   0.048704   \n",
       "\n",
       "                                (hybrid)mcc         q   f1_coin  \n",
       "experiment               class                                   \n",
       "min_u_regressor_sparse   lr        0.391930  0.019571  0.038391  \n",
       "                         gb        0.586979  0.019571  0.038391  \n",
       "                         xgb       0.540216  0.019571  0.038391  \n",
       "                         svr       0.482436  0.019571  0.038391  \n",
       "                         mlp      -0.280415  0.019571  0.038391  \n",
       "min_u_regressor_balanced lr        0.410611  0.019571  0.038391  \n",
       "                         gb        0.521648  0.019571  0.038391  \n",
       "                         xgb       0.047840  0.019571  0.038391  \n",
       "                         svr       0.486502  0.019571  0.038391  \n",
       "                         mlp      -0.307781  0.019571  0.038391  \n",
       "min_u_regressor_focused  lr        0.268520  0.019571  0.038391  \n",
       "                         gb        0.299146  0.019571  0.038391  \n",
       "                         xgb       0.198854  0.019571  0.038391  \n",
       "                         svr       0.321932  0.019571  0.038391  \n",
       "                         mlp       0.021320  0.019571  0.038391  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['min_u_regressor_sparse', 'min_u_regressor_balanced', 'min_u_regressor_focused']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best sparse, with gradient boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimum number present busses in regresison?\n",
    "In order to understand the optinum number of busses for the training set of the regression data set the data sets used will be:\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Maximum Voltage Constraints|Filtered Regression|45216|10|\n",
    "|Minimum Voltage Constraints|Sparse Regression|45216|34|\n",
    "|Minimum Voltage Constraints|Filtered Regression|45216|10|\n",
    "\n",
    "The **Filtered Regression data set** is created from the Sparse Regression data set, but only keeping the columns that contain at least one time step with a constraint violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>298021</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.979047</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.479834</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3361</td>\n",
       "      <td>299916</td>\n",
       "      <td>2544</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>0.568695</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.613786</td>\n",
       "      <td>0.608879</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3983</td>\n",
       "      <td>97550</td>\n",
       "      <td>204910</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.338473</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.790252</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3885</td>\n",
       "      <td>290445</td>\n",
       "      <td>12015</td>\n",
       "      <td>1151</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>0.257948</td>\n",
       "      <td>0.770778</td>\n",
       "      <td>0.386537</td>\n",
       "      <td>0.431802</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>211703</td>\n",
       "      <td>90757</td>\n",
       "      <td>0</td>\n",
       "      <td>1.466242</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099287</td>\n",
       "      <td>-0.275648</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>80965</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.453823</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3260</td>\n",
       "      <td>83067</td>\n",
       "      <td>2337</td>\n",
       "      <td>1776</td>\n",
       "      <td>0.954708</td>\n",
       "      <td>0.581068</td>\n",
       "      <td>0.646568</td>\n",
       "      <td>0.612071</td>\n",
       "      <td>0.589039</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4315</td>\n",
       "      <td>71932</td>\n",
       "      <td>13472</td>\n",
       "      <td>721</td>\n",
       "      <td>0.843229</td>\n",
       "      <td>0.241617</td>\n",
       "      <td>0.85642</td>\n",
       "      <td>0.376901</td>\n",
       "      <td>0.402428</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3613</td>\n",
       "      <td>75525</td>\n",
       "      <td>9879</td>\n",
       "      <td>1423</td>\n",
       "      <td>0.875203</td>\n",
       "      <td>0.266493</td>\n",
       "      <td>0.716784</td>\n",
       "      <td>0.388533</td>\n",
       "      <td>0.386221</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>22</td>\n",
       "      <td>85382</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.05415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp      tn      fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                                \n",
       "max_u_regressor_sparse   lr     3011  298021    4439  2025         0.979047   \n",
       "                         gb     3361  299916    2544  1675         0.986368   \n",
       "                         xgb    3983   97550  204910  1053         0.338473   \n",
       "                         svr    3885  290445   12015  1151         0.960099   \n",
       "                         mlp    5036  211703   90757     0         1.466242   \n",
       "max_u_filtered_regressor lr     3011   80965    4439  2025         0.928714   \n",
       "                         gb     3260   83067    2337  1776         0.954708   \n",
       "                         xgb    4315   71932   13472   721         0.843229   \n",
       "                         svr    3613   75525    9879  1423         0.875203   \n",
       "                         mlp    5036      22   85382     0         0.054443   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "max_u_regressor_sparse   lr             0.402003       0.597092   0.480501   \n",
       "                         gb             0.568695       0.666644   0.613786   \n",
       "                         xgb             0.01963       0.790252   0.038308   \n",
       "                         svr            0.257948       0.770778   0.386537   \n",
       "                         mlp            0.052237            1.0   0.099287   \n",
       "max_u_filtered_regressor lr             0.402003       0.597092   0.480501   \n",
       "                         gb             0.581068       0.646568   0.612071   \n",
       "                         xgb            0.241617        0.85642   0.376901   \n",
       "                         svr            0.266493       0.716784   0.388533   \n",
       "                         mlp             0.05415            1.0   0.102736   \n",
       "\n",
       "                                (hybrid)mcc         q   f1_coin  \n",
       "experiment               class                                   \n",
       "max_u_regressor_sparse   lr        0.479834  0.016377  0.032227  \n",
       "                         gb        0.608879  0.016377  0.032227  \n",
       "                         xgb       0.032998  0.016377  0.032227  \n",
       "                         svr       0.431802  0.016377  0.032227  \n",
       "                         mlp      -0.275648  0.016377  0.032227  \n",
       "max_u_filtered_regressor lr        0.453823  0.055683  0.105492  \n",
       "                         gb        0.589039  0.055683  0.105492  \n",
       "                         xgb       0.402428  0.055683  0.105492  \n",
       "                         svr       0.386221  0.055683  0.105492  \n",
       "                         mlp       0.004214  0.055683  0.105492  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['max_u_regressor_sparse', 'max_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse, with GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>297640</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.976169</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.391930</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5039</td>\n",
       "      <td>294700</td>\n",
       "      <td>6778</td>\n",
       "      <td>979</td>\n",
       "      <td>0.974861</td>\n",
       "      <td>0.426091</td>\n",
       "      <td>0.837325</td>\n",
       "      <td>0.564781</td>\n",
       "      <td>0.586979</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4781</td>\n",
       "      <td>293809</td>\n",
       "      <td>7669</td>\n",
       "      <td>1237</td>\n",
       "      <td>0.971135</td>\n",
       "      <td>0.383609</td>\n",
       "      <td>0.794475</td>\n",
       "      <td>0.517396</td>\n",
       "      <td>0.540216</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5208</td>\n",
       "      <td>288486</td>\n",
       "      <td>12992</td>\n",
       "      <td>810</td>\n",
       "      <td>0.955228</td>\n",
       "      <td>0.285838</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>0.42972</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6011</td>\n",
       "      <td>229483</td>\n",
       "      <td>71995</td>\n",
       "      <td>7</td>\n",
       "      <td>1.023041</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>-0.280415</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>80584</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.918923</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.360620</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4997</td>\n",
       "      <td>77824</td>\n",
       "      <td>6598</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.430647</td>\n",
       "      <td>0.830352</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4950</td>\n",
       "      <td>78375</td>\n",
       "      <td>6047</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.921556</td>\n",
       "      <td>0.449796</td>\n",
       "      <td>0.822501</td>\n",
       "      <td>0.581558</td>\n",
       "      <td>0.572522</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5100</td>\n",
       "      <td>72437</td>\n",
       "      <td>11985</td>\n",
       "      <td>918</td>\n",
       "      <td>0.857577</td>\n",
       "      <td>0.298173</td>\n",
       "      <td>0.847308</td>\n",
       "      <td>0.441115</td>\n",
       "      <td>0.448986</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5751</td>\n",
       "      <td>30442</td>\n",
       "      <td>53980</td>\n",
       "      <td>267</td>\n",
       "      <td>0.406414</td>\n",
       "      <td>0.098079</td>\n",
       "      <td>0.954641</td>\n",
       "      <td>0.177883</td>\n",
       "      <td>0.169389</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp      tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                               \n",
       "min_u_regressor_sparse   lr     2502  297640   3838  3516         0.976169   \n",
       "                         gb     5039  294700   6778   979         0.974861   \n",
       "                         xgb    4781  293809   7669  1237         0.971135   \n",
       "                         svr    5208  288486  12992   810         0.955228   \n",
       "                         mlp    6011  229483  71995     7         1.023041   \n",
       "min_u_filtered_regressor lr     2502   80584   3838  3516         0.918923   \n",
       "                         gb     4997   77824   6598  1021         0.916002   \n",
       "                         xgb    4950   78375   6047  1068         0.921556   \n",
       "                         svr    5100   72437  11985   918         0.857577   \n",
       "                         mlp    5751   30442  53980   267         0.406414   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "min_u_regressor_sparse   lr             0.393263       0.415191   0.403929   \n",
       "                         gb             0.426091       0.837325   0.564781   \n",
       "                         xgb            0.383609       0.794475   0.517396   \n",
       "                         svr            0.285838       0.865268    0.42972   \n",
       "                         mlp            0.076957       0.998749   0.142903   \n",
       "min_u_filtered_regressor lr             0.393263       0.415191   0.403929   \n",
       "                         gb             0.430647       0.830352   0.567151   \n",
       "                         xgb            0.449796       0.822501   0.581558   \n",
       "                         svr            0.298173       0.847308   0.441115   \n",
       "                         mlp            0.098079       0.954641   0.177883   \n",
       "\n",
       "                                (hybrid)mcc         q   f1_coin  \n",
       "experiment               class                                   \n",
       "min_u_regressor_sparse   lr        0.391930  0.019571  0.038391  \n",
       "                         gb        0.586979  0.019571  0.038391  \n",
       "                         xgb       0.540216  0.019571  0.038391  \n",
       "                         svr       0.482436  0.019571  0.038391  \n",
       "                         mlp      -0.280415  0.019571  0.038391  \n",
       "min_u_filtered_regressor lr        0.360620  0.066541   0.12478  \n",
       "                         gb        0.560655  0.066541   0.12478  \n",
       "                         xgb       0.572522  0.066541   0.12478  \n",
       "                         svr       0.448986  0.066541   0.12478  \n",
       "                         mlp       0.169389  0.066541   0.12478  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['min_u_regressor_sparse', 'min_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse, with GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression vs Classification\n",
    "In order to understand the optinum number of busses for the training set of the regression data set the data sets used will be:\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Filtered Regression|45216|10|\n",
    "|Maximum Voltage Constraints|Sparse Classification|45216|10|\n",
    "|Minimum Voltage Constraints|Filtered Regression|45216|10|\n",
    "|Minimum Voltage Constraints|Sparse Classification|45216|10|\n",
    "\n",
    "The **Sparse Classification data set** is created from the Sparse Regression data set, but instead of having the target feature as the amplitude of the constraint violation, it is a binary feature that indicates if there is a constraint violation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>2598</td>\n",
       "      <td>83124</td>\n",
       "      <td>2280</td>\n",
       "      <td>2438</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>0.532595</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.524107</td>\n",
       "      <td>0.496589</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1867</td>\n",
       "      <td>84013</td>\n",
       "      <td>1391</td>\n",
       "      <td>3169</td>\n",
       "      <td>0.94958</td>\n",
       "      <td>0.573051</td>\n",
       "      <td>0.370731</td>\n",
       "      <td>0.450205</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>0</td>\n",
       "      <td>85404</td>\n",
       "      <td>0</td>\n",
       "      <td>5036</td>\n",
       "      <td>0.944317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3990</td>\n",
       "      <td>59437</td>\n",
       "      <td>25967</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.701316</td>\n",
       "      <td>0.133191</td>\n",
       "      <td>0.792295</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.237879</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>80965</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.453823</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3260</td>\n",
       "      <td>83067</td>\n",
       "      <td>2337</td>\n",
       "      <td>1776</td>\n",
       "      <td>0.954708</td>\n",
       "      <td>0.581068</td>\n",
       "      <td>0.646568</td>\n",
       "      <td>0.612071</td>\n",
       "      <td>0.589039</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4315</td>\n",
       "      <td>71932</td>\n",
       "      <td>13472</td>\n",
       "      <td>721</td>\n",
       "      <td>0.843229</td>\n",
       "      <td>0.241617</td>\n",
       "      <td>0.85642</td>\n",
       "      <td>0.376901</td>\n",
       "      <td>0.402428</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3613</td>\n",
       "      <td>75525</td>\n",
       "      <td>9879</td>\n",
       "      <td>1423</td>\n",
       "      <td>0.875203</td>\n",
       "      <td>0.266493</td>\n",
       "      <td>0.716784</td>\n",
       "      <td>0.388533</td>\n",
       "      <td>0.386221</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>22</td>\n",
       "      <td>85382</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.05415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp     tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                              \n",
       "max_u_classifier         lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                         gb     2598  83124   2280  2438         0.947833   \n",
       "                         xgb    1867  84013   1391  3169          0.94958   \n",
       "                         svr       0  85404      0  5036         0.944317   \n",
       "                         mlp    3990  59437  25967  1046         0.701316   \n",
       "max_u_filtered_regressor lr     3011  80965   4439  2025         0.928714   \n",
       "                         gb     3260  83067   2337  1776         0.954708   \n",
       "                         xgb    4315  71932  13472   721         0.843229   \n",
       "                         svr    3613  75525   9879  1423         0.875203   \n",
       "                         mlp    5036     22  85382     0         0.054443   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "max_u_classifier         lr                  NaN            NaN        NaN   \n",
       "                         gb             0.532595       0.515886   0.524107   \n",
       "                         xgb            0.573051       0.370731   0.450205   \n",
       "                         svr                   0            0.0          0   \n",
       "                         mlp            0.133191       0.792295   0.228046   \n",
       "max_u_filtered_regressor lr             0.402003       0.597092   0.480501   \n",
       "                         gb             0.581068       0.646568   0.612071   \n",
       "                         xgb            0.241617        0.85642   0.376901   \n",
       "                         svr            0.266493       0.716784   0.388533   \n",
       "                         mlp             0.05415            1.0   0.102736   \n",
       "\n",
       "                                (hybrid)mcc         q   f1_coin  \n",
       "experiment               class                                   \n",
       "max_u_classifier         lr             NaN       NaN       NaN  \n",
       "                         gb        0.496589  0.055683  0.105492  \n",
       "                         xgb       0.436154  0.055683  0.105492  \n",
       "                         svr       0.436154  0.055683  0.105492  \n",
       "                         mlp       0.237879  0.055683  0.105492  \n",
       "max_u_filtered_regressor lr        0.453823  0.055683  0.105492  \n",
       "                         gb        0.589039  0.055683  0.105492  \n",
       "                         xgb       0.402428  0.055683  0.105492  \n",
       "                         svr       0.386221  0.055683  0.105492  \n",
       "                         mlp       0.004214  0.055683  0.105492  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['max_u_classifier', 'max_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_u_filtered regressor with the gradient boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4149</td>\n",
       "      <td>80988</td>\n",
       "      <td>3434</td>\n",
       "      <td>1869</td>\n",
       "      <td>0.941364</td>\n",
       "      <td>0.547145</td>\n",
       "      <td>0.689432</td>\n",
       "      <td>0.610102</td>\n",
       "      <td>0.583377</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4092</td>\n",
       "      <td>80554</td>\n",
       "      <td>3868</td>\n",
       "      <td>1926</td>\n",
       "      <td>0.935935</td>\n",
       "      <td>0.51407</td>\n",
       "      <td>0.67996</td>\n",
       "      <td>0.585491</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>2976</td>\n",
       "      <td>82972</td>\n",
       "      <td>1450</td>\n",
       "      <td>3042</td>\n",
       "      <td>0.950332</td>\n",
       "      <td>0.67239</td>\n",
       "      <td>0.494516</td>\n",
       "      <td>0.569897</td>\n",
       "      <td>0.551432</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5323</td>\n",
       "      <td>47507</td>\n",
       "      <td>36915</td>\n",
       "      <td>695</td>\n",
       "      <td>0.584144</td>\n",
       "      <td>0.126024</td>\n",
       "      <td>0.884513</td>\n",
       "      <td>0.220615</td>\n",
       "      <td>0.223417</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>80584</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.918923</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.360620</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4997</td>\n",
       "      <td>77824</td>\n",
       "      <td>6598</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.430647</td>\n",
       "      <td>0.830352</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4950</td>\n",
       "      <td>78375</td>\n",
       "      <td>6047</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.921556</td>\n",
       "      <td>0.449796</td>\n",
       "      <td>0.822501</td>\n",
       "      <td>0.581558</td>\n",
       "      <td>0.572522</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5100</td>\n",
       "      <td>72437</td>\n",
       "      <td>11985</td>\n",
       "      <td>918</td>\n",
       "      <td>0.857577</td>\n",
       "      <td>0.298173</td>\n",
       "      <td>0.847308</td>\n",
       "      <td>0.441115</td>\n",
       "      <td>0.448986</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5751</td>\n",
       "      <td>30442</td>\n",
       "      <td>53980</td>\n",
       "      <td>267</td>\n",
       "      <td>0.406414</td>\n",
       "      <td>0.098079</td>\n",
       "      <td>0.954641</td>\n",
       "      <td>0.177883</td>\n",
       "      <td>0.169389</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tp     tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment               class                                              \n",
       "min_u_classifier         lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                         gb     4149  80988   3434  1869         0.941364   \n",
       "                         xgb    4092  80554   3868  1926         0.935935   \n",
       "                         svr    2976  82972   1450  3042         0.950332   \n",
       "                         mlp    5323  47507  36915   695         0.584144   \n",
       "min_u_filtered_regressor lr     2502  80584   3838  3516         0.918923   \n",
       "                         gb     4997  77824   6598  1021         0.916002   \n",
       "                         xgb    4950  78375   6047  1068         0.921556   \n",
       "                         svr    5100  72437  11985   918         0.857577   \n",
       "                         mlp    5751  30442  53980   267         0.406414   \n",
       "\n",
       "                               (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment               class                                               \n",
       "min_u_classifier         lr                  NaN            NaN        NaN   \n",
       "                         gb             0.547145       0.689432   0.610102   \n",
       "                         xgb             0.51407        0.67996   0.585491   \n",
       "                         svr             0.67239       0.494516   0.569897   \n",
       "                         mlp            0.126024       0.884513   0.220615   \n",
       "min_u_filtered_regressor lr             0.393263       0.415191   0.403929   \n",
       "                         gb             0.430647       0.830352   0.567151   \n",
       "                         xgb            0.449796       0.822501   0.581558   \n",
       "                         svr            0.298173       0.847308   0.441115   \n",
       "                         mlp            0.098079       0.954641   0.177883   \n",
       "\n",
       "                                (hybrid)mcc         q  f1_coin  \n",
       "experiment               class                                  \n",
       "min_u_classifier         lr             NaN       NaN      NaN  \n",
       "                         gb        0.583377  0.066541  0.12478  \n",
       "                         xgb       0.557840  0.066541  0.12478  \n",
       "                         svr       0.551432  0.066541  0.12478  \n",
       "                         mlp       0.223417  0.066541  0.12478  \n",
       "min_u_filtered_regressor lr        0.360620  0.066541  0.12478  \n",
       "                         gb        0.560655  0.066541  0.12478  \n",
       "                         xgb       0.572522  0.066541  0.12478  \n",
       "                         svr       0.448986  0.066541  0.12478  \n",
       "                         mlp       0.169389  0.066541  0.12478  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['min_u_classifier', 'min_u_filtered_regressor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_u_classifier with the gradient boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimum number of rows in class?\n",
    "In order to understand the optinum number of rows for the training set of the classification data set the data sets used will be:\n",
    "|Experience| Data Set Name | Rows | Busses |\n",
    "|----------|---------------|------|--------|\n",
    "|Maximum Voltage Constraints|Sparse Classification|45216|10|\n",
    "|Maximum Voltage Constraints|Balanced Classification|6971|10|\n",
    "|Minimum Voltage Constraints|Sparse Classification|45216|10|\n",
    "|Minimum Voltage Constraints|Balanced Classification|13917|10|\n",
    "\n",
    "The **Balanced Classification data set** is created from the Sparse Classification data set. It is created by taking all the rows that containt at least one constraint violation and then taking the same number of rows that do not contain any constraint violation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>2598</td>\n",
       "      <td>83124</td>\n",
       "      <td>2280</td>\n",
       "      <td>2438</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>0.532595</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.524107</td>\n",
       "      <td>0.496589</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1867</td>\n",
       "      <td>84013</td>\n",
       "      <td>1391</td>\n",
       "      <td>3169</td>\n",
       "      <td>0.94958</td>\n",
       "      <td>0.573051</td>\n",
       "      <td>0.370731</td>\n",
       "      <td>0.450205</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>0</td>\n",
       "      <td>85404</td>\n",
       "      <td>0</td>\n",
       "      <td>5036</td>\n",
       "      <td>0.944317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3990</td>\n",
       "      <td>59437</td>\n",
       "      <td>25967</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.701316</td>\n",
       "      <td>0.133191</td>\n",
       "      <td>0.792295</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.237879</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3546</td>\n",
       "      <td>81683</td>\n",
       "      <td>3721</td>\n",
       "      <td>1490</td>\n",
       "      <td>0.942382</td>\n",
       "      <td>0.487959</td>\n",
       "      <td>0.70413</td>\n",
       "      <td>0.576445</td>\n",
       "      <td>0.557219</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3180</td>\n",
       "      <td>83080</td>\n",
       "      <td>2324</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.953782</td>\n",
       "      <td>0.577762</td>\n",
       "      <td>0.631454</td>\n",
       "      <td>0.603416</td>\n",
       "      <td>0.579572</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4029</td>\n",
       "      <td>80498</td>\n",
       "      <td>4906</td>\n",
       "      <td>1007</td>\n",
       "      <td>0.93462</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>0.80004</td>\n",
       "      <td>0.576766</td>\n",
       "      <td>0.570683</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3967</td>\n",
       "      <td>49322</td>\n",
       "      <td>36082</td>\n",
       "      <td>1069</td>\n",
       "      <td>0.589219</td>\n",
       "      <td>0.099054</td>\n",
       "      <td>0.787728</td>\n",
       "      <td>0.175979</td>\n",
       "      <td>0.168613</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tp     tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment                class                                              \n",
       "max_u_classifier          lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb     2598  83124   2280  2438         0.947833   \n",
       "                          xgb    1867  84013   1391  3169          0.94958   \n",
       "                          svr       0  85404      0  5036         0.944317   \n",
       "                          mlp    3990  59437  25967  1046         0.701316   \n",
       "max_u_classifier_balanced lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb     3546  81683   3721  1490         0.942382   \n",
       "                          xgb    3180  83080   2324  1856         0.953782   \n",
       "                          svr    4029  80498   4906  1007          0.93462   \n",
       "                          mlp    3967  49322  36082  1069         0.589219   \n",
       "\n",
       "                                (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment                class                                               \n",
       "max_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.532595       0.515886   0.524107   \n",
       "                          xgb            0.573051       0.370731   0.450205   \n",
       "                          svr                   0            0.0          0   \n",
       "                          mlp            0.133191       0.792295   0.228046   \n",
       "max_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.487959        0.70413   0.576445   \n",
       "                          xgb            0.577762       0.631454   0.603416   \n",
       "                          svr            0.450923        0.80004   0.576766   \n",
       "                          mlp            0.099054       0.787728   0.175979   \n",
       "\n",
       "                                 (hybrid)mcc         q   f1_coin  \n",
       "experiment                class                                   \n",
       "max_u_classifier          lr             NaN       NaN       NaN  \n",
       "                          gb        0.496589  0.055683  0.105492  \n",
       "                          xgb       0.436154  0.055683  0.105492  \n",
       "                          svr       0.436154  0.055683  0.105492  \n",
       "                          mlp       0.237879  0.055683  0.105492  \n",
       "max_u_classifier_balanced lr             NaN       NaN       NaN  \n",
       "                          gb        0.557219  0.055683  0.105492  \n",
       "                          xgb       0.579572  0.055683  0.105492  \n",
       "                          svr       0.570683  0.055683  0.105492  \n",
       "                          mlp       0.168613  0.055683  0.105492  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['max_u_classifier', 'max_u_classifier_balanced']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier balanced with xgb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4149</td>\n",
       "      <td>80988</td>\n",
       "      <td>3434</td>\n",
       "      <td>1869</td>\n",
       "      <td>0.941364</td>\n",
       "      <td>0.547145</td>\n",
       "      <td>0.689432</td>\n",
       "      <td>0.610102</td>\n",
       "      <td>0.583377</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4092</td>\n",
       "      <td>80554</td>\n",
       "      <td>3868</td>\n",
       "      <td>1926</td>\n",
       "      <td>0.935935</td>\n",
       "      <td>0.51407</td>\n",
       "      <td>0.67996</td>\n",
       "      <td>0.585491</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>2976</td>\n",
       "      <td>82972</td>\n",
       "      <td>1450</td>\n",
       "      <td>3042</td>\n",
       "      <td>0.950332</td>\n",
       "      <td>0.67239</td>\n",
       "      <td>0.494516</td>\n",
       "      <td>0.569897</td>\n",
       "      <td>0.551432</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5323</td>\n",
       "      <td>47507</td>\n",
       "      <td>36915</td>\n",
       "      <td>695</td>\n",
       "      <td>0.584144</td>\n",
       "      <td>0.126024</td>\n",
       "      <td>0.884513</td>\n",
       "      <td>0.220615</td>\n",
       "      <td>0.223417</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4478</td>\n",
       "      <td>77802</td>\n",
       "      <td>6620</td>\n",
       "      <td>1540</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.403496</td>\n",
       "      <td>0.744101</td>\n",
       "      <td>0.523253</td>\n",
       "      <td>0.505649</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4267</td>\n",
       "      <td>78129</td>\n",
       "      <td>6293</td>\n",
       "      <td>1751</td>\n",
       "      <td>0.911057</td>\n",
       "      <td>0.404072</td>\n",
       "      <td>0.70904</td>\n",
       "      <td>0.514779</td>\n",
       "      <td>0.492417</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4934</td>\n",
       "      <td>79490</td>\n",
       "      <td>4932</td>\n",
       "      <td>1084</td>\n",
       "      <td>0.933481</td>\n",
       "      <td>0.500101</td>\n",
       "      <td>0.819874</td>\n",
       "      <td>0.621254</td>\n",
       "      <td>0.608736</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5425</td>\n",
       "      <td>46582</td>\n",
       "      <td>37840</td>\n",
       "      <td>593</td>\n",
       "      <td>0.575044</td>\n",
       "      <td>0.12539</td>\n",
       "      <td>0.901462</td>\n",
       "      <td>0.220157</td>\n",
       "      <td>0.226129</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tp     tn     fp    fn (hybrid)accuracy  \\\n",
       "experiment                class                                              \n",
       "min_u_classifier          lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb     4149  80988   3434  1869         0.941364   \n",
       "                          xgb    4092  80554   3868  1926         0.935935   \n",
       "                          svr    2976  82972   1450  3042         0.950332   \n",
       "                          mlp    5323  47507  36915   695         0.584144   \n",
       "min_u_classifier_balanced lr      NaN    NaN    NaN   NaN              NaN   \n",
       "                          gb     4478  77802   6620  1540         0.909774   \n",
       "                          xgb    4267  78129   6293  1751         0.911057   \n",
       "                          svr    4934  79490   4932  1084         0.933481   \n",
       "                          mlp    5425  46582  37840   593         0.575044   \n",
       "\n",
       "                                (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment                class                                               \n",
       "min_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.547145       0.689432   0.610102   \n",
       "                          xgb             0.51407        0.67996   0.585491   \n",
       "                          svr             0.67239       0.494516   0.569897   \n",
       "                          mlp            0.126024       0.884513   0.220615   \n",
       "min_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.403496       0.744101   0.523253   \n",
       "                          xgb            0.404072        0.70904   0.514779   \n",
       "                          svr            0.500101       0.819874   0.621254   \n",
       "                          mlp             0.12539       0.901462   0.220157   \n",
       "\n",
       "                                 (hybrid)mcc         q  f1_coin  \n",
       "experiment                class                                  \n",
       "min_u_classifier          lr             NaN       NaN      NaN  \n",
       "                          gb        0.583377  0.066541  0.12478  \n",
       "                          xgb       0.557840  0.066541  0.12478  \n",
       "                          svr       0.551432  0.066541  0.12478  \n",
       "                          mlp       0.223417  0.066541  0.12478  \n",
       "min_u_classifier_balanced lr             NaN       NaN      NaN  \n",
       "                          gb        0.505649  0.066541  0.12478  \n",
       "                          xgb       0.492417  0.066541  0.12478  \n",
       "                          svr       0.608736  0.066541  0.12478  \n",
       "                          mlp       0.226129  0.066541  0.12478  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['min_u_classifier', 'min_u_classifier_balanced']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier balanced with svr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "In order to understand how good our predictions really are, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>(hybrid)accuracy</th>\n",
       "      <th>(hybrid)precision</th>\n",
       "      <th>(hybrid)recall</th>\n",
       "      <th>(hybrid)f1</th>\n",
       "      <th>(hybrid)mcc</th>\n",
       "      <th>q</th>\n",
       "      <th>f1_coin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>298021</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.979047</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.479834</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3361</td>\n",
       "      <td>299916</td>\n",
       "      <td>2544</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>0.568695</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.613786</td>\n",
       "      <td>0.608879</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3983</td>\n",
       "      <td>97550</td>\n",
       "      <td>204910</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.338473</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.790252</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3885</td>\n",
       "      <td>290445</td>\n",
       "      <td>12015</td>\n",
       "      <td>1151</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>0.257948</td>\n",
       "      <td>0.770778</td>\n",
       "      <td>0.386537</td>\n",
       "      <td>0.431802</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>211703</td>\n",
       "      <td>90757</td>\n",
       "      <td>0</td>\n",
       "      <td>1.466242</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099287</td>\n",
       "      <td>-0.275648</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>4089</td>\n",
       "      <td>298466</td>\n",
       "      <td>3994</td>\n",
       "      <td>947</td>\n",
       "      <td>0.983766</td>\n",
       "      <td>0.505694</td>\n",
       "      <td>0.81232</td>\n",
       "      <td>0.62334</td>\n",
       "      <td>0.633630</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4416</td>\n",
       "      <td>252698</td>\n",
       "      <td>49762</td>\n",
       "      <td>620</td>\n",
       "      <td>0.836891</td>\n",
       "      <td>0.081427</td>\n",
       "      <td>0.876656</td>\n",
       "      <td>0.149013</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4700</td>\n",
       "      <td>71618</td>\n",
       "      <td>230842</td>\n",
       "      <td>336</td>\n",
       "      <td>0.256491</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.933189</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>0.053516</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4300</td>\n",
       "      <td>275628</td>\n",
       "      <td>26832</td>\n",
       "      <td>736</td>\n",
       "      <td>0.91493</td>\n",
       "      <td>0.144597</td>\n",
       "      <td>0.85364</td>\n",
       "      <td>0.247304</td>\n",
       "      <td>0.330513</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3247</td>\n",
       "      <td>194204</td>\n",
       "      <td>108256</td>\n",
       "      <td>1789</td>\n",
       "      <td>0.631347</td>\n",
       "      <td>0.030686</td>\n",
       "      <td>0.645414</td>\n",
       "      <td>0.058587</td>\n",
       "      <td>0.075512</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>3011</td>\n",
       "      <td>80965</td>\n",
       "      <td>4439</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.928714</td>\n",
       "      <td>0.402003</td>\n",
       "      <td>0.597092</td>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.453823</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3260</td>\n",
       "      <td>83067</td>\n",
       "      <td>2337</td>\n",
       "      <td>1776</td>\n",
       "      <td>0.954708</td>\n",
       "      <td>0.581068</td>\n",
       "      <td>0.646568</td>\n",
       "      <td>0.612071</td>\n",
       "      <td>0.589039</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4315</td>\n",
       "      <td>71932</td>\n",
       "      <td>13472</td>\n",
       "      <td>721</td>\n",
       "      <td>0.843229</td>\n",
       "      <td>0.241617</td>\n",
       "      <td>0.85642</td>\n",
       "      <td>0.376901</td>\n",
       "      <td>0.402428</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3613</td>\n",
       "      <td>75525</td>\n",
       "      <td>9879</td>\n",
       "      <td>1423</td>\n",
       "      <td>0.875203</td>\n",
       "      <td>0.266493</td>\n",
       "      <td>0.716784</td>\n",
       "      <td>0.388533</td>\n",
       "      <td>0.386221</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5036</td>\n",
       "      <td>22</td>\n",
       "      <td>85382</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.05415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4513</td>\n",
       "      <td>282576</td>\n",
       "      <td>19884</td>\n",
       "      <td>523</td>\n",
       "      <td>0.933826</td>\n",
       "      <td>0.184598</td>\n",
       "      <td>0.895985</td>\n",
       "      <td>0.306125</td>\n",
       "      <td>0.389575</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4109</td>\n",
       "      <td>297630</td>\n",
       "      <td>4830</td>\n",
       "      <td>927</td>\n",
       "      <td>0.981642</td>\n",
       "      <td>0.463762</td>\n",
       "      <td>0.815659</td>\n",
       "      <td>0.591317</td>\n",
       "      <td>0.607091</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4242</td>\n",
       "      <td>87755</td>\n",
       "      <td>214705</td>\n",
       "      <td>794</td>\n",
       "      <td>0.304284</td>\n",
       "      <td>0.019722</td>\n",
       "      <td>0.842058</td>\n",
       "      <td>0.038541</td>\n",
       "      <td>0.038495</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>3974</td>\n",
       "      <td>290777</td>\n",
       "      <td>11683</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.959239</td>\n",
       "      <td>0.258391</td>\n",
       "      <td>0.789028</td>\n",
       "      <td>0.389296</td>\n",
       "      <td>0.437418</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5008</td>\n",
       "      <td>226465</td>\n",
       "      <td>75995</td>\n",
       "      <td>28</td>\n",
       "      <td>1.008493</td>\n",
       "      <td>0.061813</td>\n",
       "      <td>0.994026</td>\n",
       "      <td>0.116388</td>\n",
       "      <td>-0.248941</td>\n",
       "      <td>0.016377</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>2598</td>\n",
       "      <td>83124</td>\n",
       "      <td>2280</td>\n",
       "      <td>2438</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>0.532595</td>\n",
       "      <td>0.515886</td>\n",
       "      <td>0.524107</td>\n",
       "      <td>0.496589</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1867</td>\n",
       "      <td>84013</td>\n",
       "      <td>1391</td>\n",
       "      <td>3169</td>\n",
       "      <td>0.94958</td>\n",
       "      <td>0.573051</td>\n",
       "      <td>0.370731</td>\n",
       "      <td>0.450205</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>0</td>\n",
       "      <td>85404</td>\n",
       "      <td>0</td>\n",
       "      <td>5036</td>\n",
       "      <td>0.944317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436154</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3990</td>\n",
       "      <td>59437</td>\n",
       "      <td>25967</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.701316</td>\n",
       "      <td>0.133191</td>\n",
       "      <td>0.792295</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.237879</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">max_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>3546</td>\n",
       "      <td>81683</td>\n",
       "      <td>3721</td>\n",
       "      <td>1490</td>\n",
       "      <td>0.942382</td>\n",
       "      <td>0.487959</td>\n",
       "      <td>0.70413</td>\n",
       "      <td>0.576445</td>\n",
       "      <td>0.557219</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>3180</td>\n",
       "      <td>83080</td>\n",
       "      <td>2324</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.953782</td>\n",
       "      <td>0.577762</td>\n",
       "      <td>0.631454</td>\n",
       "      <td>0.603416</td>\n",
       "      <td>0.579572</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4029</td>\n",
       "      <td>80498</td>\n",
       "      <td>4906</td>\n",
       "      <td>1007</td>\n",
       "      <td>0.93462</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>0.80004</td>\n",
       "      <td>0.576766</td>\n",
       "      <td>0.570683</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>3967</td>\n",
       "      <td>49322</td>\n",
       "      <td>36082</td>\n",
       "      <td>1069</td>\n",
       "      <td>0.589219</td>\n",
       "      <td>0.099054</td>\n",
       "      <td>0.787728</td>\n",
       "      <td>0.175979</td>\n",
       "      <td>0.168613</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_sparse</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>297640</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.976169</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.391930</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5039</td>\n",
       "      <td>294700</td>\n",
       "      <td>6778</td>\n",
       "      <td>979</td>\n",
       "      <td>0.974861</td>\n",
       "      <td>0.426091</td>\n",
       "      <td>0.837325</td>\n",
       "      <td>0.564781</td>\n",
       "      <td>0.586979</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4781</td>\n",
       "      <td>293809</td>\n",
       "      <td>7669</td>\n",
       "      <td>1237</td>\n",
       "      <td>0.971135</td>\n",
       "      <td>0.383609</td>\n",
       "      <td>0.794475</td>\n",
       "      <td>0.517396</td>\n",
       "      <td>0.540216</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5208</td>\n",
       "      <td>288486</td>\n",
       "      <td>12992</td>\n",
       "      <td>810</td>\n",
       "      <td>0.955228</td>\n",
       "      <td>0.285838</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>0.42972</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6011</td>\n",
       "      <td>229483</td>\n",
       "      <td>71995</td>\n",
       "      <td>7</td>\n",
       "      <td>1.023041</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>-0.280415</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_focused</th>\n",
       "      <th>lr</th>\n",
       "      <td>5929</td>\n",
       "      <td>242474</td>\n",
       "      <td>59004</td>\n",
       "      <td>89</td>\n",
       "      <td>0.808951</td>\n",
       "      <td>0.091561</td>\n",
       "      <td>0.985199</td>\n",
       "      <td>0.167551</td>\n",
       "      <td>0.268520</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5657</td>\n",
       "      <td>257440</td>\n",
       "      <td>44038</td>\n",
       "      <td>361</td>\n",
       "      <td>0.856304</td>\n",
       "      <td>0.114014</td>\n",
       "      <td>0.939976</td>\n",
       "      <td>0.203362</td>\n",
       "      <td>0.299146</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5721</td>\n",
       "      <td>213689</td>\n",
       "      <td>87789</td>\n",
       "      <td>297</td>\n",
       "      <td>0.714145</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>0.95063</td>\n",
       "      <td>0.115166</td>\n",
       "      <td>0.198854</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5779</td>\n",
       "      <td>261411</td>\n",
       "      <td>40067</td>\n",
       "      <td>239</td>\n",
       "      <td>0.869434</td>\n",
       "      <td>0.126096</td>\n",
       "      <td>0.960264</td>\n",
       "      <td>0.222919</td>\n",
       "      <td>0.321932</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>2425</td>\n",
       "      <td>203328</td>\n",
       "      <td>98150</td>\n",
       "      <td>3593</td>\n",
       "      <td>0.660005</td>\n",
       "      <td>0.025915</td>\n",
       "      <td>0.40367</td>\n",
       "      <td>0.048704</td>\n",
       "      <td>0.021320</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_filtered_regressor</th>\n",
       "      <th>lr</th>\n",
       "      <td>2502</td>\n",
       "      <td>80584</td>\n",
       "      <td>3838</td>\n",
       "      <td>3516</td>\n",
       "      <td>0.918923</td>\n",
       "      <td>0.393263</td>\n",
       "      <td>0.415191</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.360620</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4997</td>\n",
       "      <td>77824</td>\n",
       "      <td>6598</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.430647</td>\n",
       "      <td>0.830352</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4950</td>\n",
       "      <td>78375</td>\n",
       "      <td>6047</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.921556</td>\n",
       "      <td>0.449796</td>\n",
       "      <td>0.822501</td>\n",
       "      <td>0.581558</td>\n",
       "      <td>0.572522</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5100</td>\n",
       "      <td>72437</td>\n",
       "      <td>11985</td>\n",
       "      <td>918</td>\n",
       "      <td>0.857577</td>\n",
       "      <td>0.298173</td>\n",
       "      <td>0.847308</td>\n",
       "      <td>0.441115</td>\n",
       "      <td>0.448986</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5751</td>\n",
       "      <td>30442</td>\n",
       "      <td>53980</td>\n",
       "      <td>267</td>\n",
       "      <td>0.406414</td>\n",
       "      <td>0.098079</td>\n",
       "      <td>0.954641</td>\n",
       "      <td>0.177883</td>\n",
       "      <td>0.169389</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_regressor_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>4817</td>\n",
       "      <td>285372</td>\n",
       "      <td>16106</td>\n",
       "      <td>1201</td>\n",
       "      <td>0.943881</td>\n",
       "      <td>0.229869</td>\n",
       "      <td>0.800296</td>\n",
       "      <td>0.357153</td>\n",
       "      <td>0.410611</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>5032</td>\n",
       "      <td>291795</td>\n",
       "      <td>9683</td>\n",
       "      <td>986</td>\n",
       "      <td>0.965425</td>\n",
       "      <td>0.341713</td>\n",
       "      <td>0.836223</td>\n",
       "      <td>0.485168</td>\n",
       "      <td>0.521648</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>5270</td>\n",
       "      <td>82675</td>\n",
       "      <td>218803</td>\n",
       "      <td>748</td>\n",
       "      <td>0.289631</td>\n",
       "      <td>0.023833</td>\n",
       "      <td>0.875709</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.047840</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>5168</td>\n",
       "      <td>288985</td>\n",
       "      <td>12493</td>\n",
       "      <td>850</td>\n",
       "      <td>0.956723</td>\n",
       "      <td>0.292504</td>\n",
       "      <td>0.858663</td>\n",
       "      <td>0.436361</td>\n",
       "      <td>0.486502</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>6015</td>\n",
       "      <td>222433</td>\n",
       "      <td>79045</td>\n",
       "      <td>3</td>\n",
       "      <td>1.399868</td>\n",
       "      <td>0.068243</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>0.127762</td>\n",
       "      <td>-0.307781</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.038391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4149</td>\n",
       "      <td>80988</td>\n",
       "      <td>3434</td>\n",
       "      <td>1869</td>\n",
       "      <td>0.941364</td>\n",
       "      <td>0.547145</td>\n",
       "      <td>0.689432</td>\n",
       "      <td>0.610102</td>\n",
       "      <td>0.583377</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4092</td>\n",
       "      <td>80554</td>\n",
       "      <td>3868</td>\n",
       "      <td>1926</td>\n",
       "      <td>0.935935</td>\n",
       "      <td>0.51407</td>\n",
       "      <td>0.67996</td>\n",
       "      <td>0.585491</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>2976</td>\n",
       "      <td>82972</td>\n",
       "      <td>1450</td>\n",
       "      <td>3042</td>\n",
       "      <td>0.950332</td>\n",
       "      <td>0.67239</td>\n",
       "      <td>0.494516</td>\n",
       "      <td>0.569897</td>\n",
       "      <td>0.551432</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5323</td>\n",
       "      <td>47507</td>\n",
       "      <td>36915</td>\n",
       "      <td>695</td>\n",
       "      <td>0.584144</td>\n",
       "      <td>0.126024</td>\n",
       "      <td>0.884513</td>\n",
       "      <td>0.220615</td>\n",
       "      <td>0.223417</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">min_u_classifier_balanced</th>\n",
       "      <th>lr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>4478</td>\n",
       "      <td>77802</td>\n",
       "      <td>6620</td>\n",
       "      <td>1540</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.403496</td>\n",
       "      <td>0.744101</td>\n",
       "      <td>0.523253</td>\n",
       "      <td>0.505649</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>4267</td>\n",
       "      <td>78129</td>\n",
       "      <td>6293</td>\n",
       "      <td>1751</td>\n",
       "      <td>0.911057</td>\n",
       "      <td>0.404072</td>\n",
       "      <td>0.70904</td>\n",
       "      <td>0.514779</td>\n",
       "      <td>0.492417</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svr</th>\n",
       "      <td>4934</td>\n",
       "      <td>79490</td>\n",
       "      <td>4932</td>\n",
       "      <td>1084</td>\n",
       "      <td>0.933481</td>\n",
       "      <td>0.500101</td>\n",
       "      <td>0.819874</td>\n",
       "      <td>0.621254</td>\n",
       "      <td>0.608736</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>5425</td>\n",
       "      <td>46582</td>\n",
       "      <td>37840</td>\n",
       "      <td>593</td>\n",
       "      <td>0.575044</td>\n",
       "      <td>0.12539</td>\n",
       "      <td>0.901462</td>\n",
       "      <td>0.220157</td>\n",
       "      <td>0.226129</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.12478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tp      tn      fp    fn (hybrid)accuracy  \\\n",
       "experiment                class                                                \n",
       "max_u_regressor_sparse    lr     3011  298021    4439  2025         0.979047   \n",
       "                          gb     3361  299916    2544  1675         0.986368   \n",
       "                          xgb    3983   97550  204910  1053         0.338473   \n",
       "                          svr    3885  290445   12015  1151         0.960099   \n",
       "                          mlp    5036  211703   90757     0         1.466242   \n",
       "max_u_regressor_focused   lr     4089  298466    3994   947         0.983766   \n",
       "                          gb     4416  252698   49762   620         0.836891   \n",
       "                          xgb    4700   71618  230842   336         0.256491   \n",
       "                          svr    4300  275628   26832   736          0.91493   \n",
       "                          mlp    3247  194204  108256  1789         0.631347   \n",
       "max_u_filtered_regressor  lr     3011   80965    4439  2025         0.928714   \n",
       "                          gb     3260   83067    2337  1776         0.954708   \n",
       "                          xgb    4315   71932   13472   721         0.843229   \n",
       "                          svr    3613   75525    9879  1423         0.875203   \n",
       "                          mlp    5036      22   85382     0         0.054443   \n",
       "max_u_regressor_balanced  lr     4513  282576   19884   523         0.933826   \n",
       "                          gb     4109  297630    4830   927         0.981642   \n",
       "                          xgb    4242   87755  214705   794         0.304284   \n",
       "                          svr    3974  290777   11683  1062         0.959239   \n",
       "                          mlp    5008  226465   75995    28         1.008493   \n",
       "max_u_classifier          lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     2598   83124    2280  2438         0.947833   \n",
       "                          xgb    1867   84013    1391  3169          0.94958   \n",
       "                          svr       0   85404       0  5036         0.944317   \n",
       "                          mlp    3990   59437   25967  1046         0.701316   \n",
       "max_u_classifier_balanced lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     3546   81683    3721  1490         0.942382   \n",
       "                          xgb    3180   83080    2324  1856         0.953782   \n",
       "                          svr    4029   80498    4906  1007          0.93462   \n",
       "                          mlp    3967   49322   36082  1069         0.589219   \n",
       "min_u_regressor_sparse    lr     2502  297640    3838  3516         0.976169   \n",
       "                          gb     5039  294700    6778   979         0.974861   \n",
       "                          xgb    4781  293809    7669  1237         0.971135   \n",
       "                          svr    5208  288486   12992   810         0.955228   \n",
       "                          mlp    6011  229483   71995     7         1.023041   \n",
       "min_u_regressor_focused   lr     5929  242474   59004    89         0.808951   \n",
       "                          gb     5657  257440   44038   361         0.856304   \n",
       "                          xgb    5721  213689   87789   297         0.714145   \n",
       "                          svr    5779  261411   40067   239         0.869434   \n",
       "                          mlp    2425  203328   98150  3593         0.660005   \n",
       "min_u_filtered_regressor  lr     2502   80584    3838  3516         0.918923   \n",
       "                          gb     4997   77824    6598  1021         0.916002   \n",
       "                          xgb    4950   78375    6047  1068         0.921556   \n",
       "                          svr    5100   72437   11985   918         0.857577   \n",
       "                          mlp    5751   30442   53980   267         0.406414   \n",
       "min_u_regressor_balanced  lr     4817  285372   16106  1201         0.943881   \n",
       "                          gb     5032  291795    9683   986         0.965425   \n",
       "                          xgb    5270   82675  218803   748         0.289631   \n",
       "                          svr    5168  288985   12493   850         0.956723   \n",
       "                          mlp    6015  222433   79045     3         1.399868   \n",
       "min_u_classifier          lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     4149   80988    3434  1869         0.941364   \n",
       "                          xgb    4092   80554    3868  1926         0.935935   \n",
       "                          svr    2976   82972    1450  3042         0.950332   \n",
       "                          mlp    5323   47507   36915   695         0.584144   \n",
       "min_u_classifier_balanced lr      NaN     NaN     NaN   NaN              NaN   \n",
       "                          gb     4478   77802    6620  1540         0.909774   \n",
       "                          xgb    4267   78129    6293  1751         0.911057   \n",
       "                          svr    4934   79490    4932  1084         0.933481   \n",
       "                          mlp    5425   46582   37840   593         0.575044   \n",
       "\n",
       "                                (hybrid)precision (hybrid)recall (hybrid)f1  \\\n",
       "experiment                class                                               \n",
       "max_u_regressor_sparse    lr             0.402003       0.597092   0.480501   \n",
       "                          gb             0.568695       0.666644   0.613786   \n",
       "                          xgb             0.01963       0.790252   0.038308   \n",
       "                          svr            0.257948       0.770778   0.386537   \n",
       "                          mlp            0.052237            1.0   0.099287   \n",
       "max_u_regressor_focused   lr             0.505694        0.81232    0.62334   \n",
       "                          gb             0.081427       0.876656   0.149013   \n",
       "                          xgb            0.020734       0.933189   0.040567   \n",
       "                          svr            0.144597        0.85364   0.247304   \n",
       "                          mlp            0.030686       0.645414   0.058587   \n",
       "max_u_filtered_regressor  lr             0.402003       0.597092   0.480501   \n",
       "                          gb             0.581068       0.646568   0.612071   \n",
       "                          xgb            0.241617        0.85642   0.376901   \n",
       "                          svr            0.266493       0.716784   0.388533   \n",
       "                          mlp             0.05415            1.0   0.102736   \n",
       "max_u_regressor_balanced  lr             0.184598       0.895985   0.306125   \n",
       "                          gb             0.463762       0.815659   0.591317   \n",
       "                          xgb            0.019722       0.842058   0.038541   \n",
       "                          svr            0.258391       0.789028   0.389296   \n",
       "                          mlp            0.061813       0.994026   0.116388   \n",
       "max_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.532595       0.515886   0.524107   \n",
       "                          xgb            0.573051       0.370731   0.450205   \n",
       "                          svr                   0            0.0          0   \n",
       "                          mlp            0.133191       0.792295   0.228046   \n",
       "max_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.487959        0.70413   0.576445   \n",
       "                          xgb            0.577762       0.631454   0.603416   \n",
       "                          svr            0.450923        0.80004   0.576766   \n",
       "                          mlp            0.099054       0.787728   0.175979   \n",
       "min_u_regressor_sparse    lr             0.393263       0.415191   0.403929   \n",
       "                          gb             0.426091       0.837325   0.564781   \n",
       "                          xgb            0.383609       0.794475   0.517396   \n",
       "                          svr            0.285838       0.865268    0.42972   \n",
       "                          mlp            0.076957       0.998749   0.142903   \n",
       "min_u_regressor_focused   lr             0.091561       0.985199   0.167551   \n",
       "                          gb             0.114014       0.939976   0.203362   \n",
       "                          xgb            0.061296        0.95063   0.115166   \n",
       "                          svr            0.126096       0.960264   0.222919   \n",
       "                          mlp            0.025915        0.40367   0.048704   \n",
       "min_u_filtered_regressor  lr             0.393263       0.415191   0.403929   \n",
       "                          gb             0.430647       0.830352   0.567151   \n",
       "                          xgb            0.449796       0.822501   0.581558   \n",
       "                          svr            0.298173       0.847308   0.441115   \n",
       "                          mlp            0.098079       0.954641   0.177883   \n",
       "min_u_regressor_balanced  lr             0.229869       0.800296   0.357153   \n",
       "                          gb             0.341713       0.836223   0.485168   \n",
       "                          xgb            0.023833       0.875709   0.046404   \n",
       "                          svr            0.292504       0.858663   0.436361   \n",
       "                          mlp            0.068243       0.999402   0.127762   \n",
       "min_u_classifier          lr                  NaN            NaN        NaN   \n",
       "                          gb             0.547145       0.689432   0.610102   \n",
       "                          xgb             0.51407        0.67996   0.585491   \n",
       "                          svr             0.67239       0.494516   0.569897   \n",
       "                          mlp            0.126024       0.884513   0.220615   \n",
       "min_u_classifier_balanced lr                  NaN            NaN        NaN   \n",
       "                          gb             0.403496       0.744101   0.523253   \n",
       "                          xgb            0.404072        0.70904   0.514779   \n",
       "                          svr            0.500101       0.819874   0.621254   \n",
       "                          mlp             0.12539       0.901462   0.220157   \n",
       "\n",
       "                                 (hybrid)mcc         q   f1_coin  \n",
       "experiment                class                                   \n",
       "max_u_regressor_sparse    lr        0.479834  0.016377  0.032227  \n",
       "                          gb        0.608879  0.016377  0.032227  \n",
       "                          xgb       0.032998  0.016377  0.032227  \n",
       "                          svr       0.431802  0.016377  0.032227  \n",
       "                          mlp      -0.275648  0.016377  0.032227  \n",
       "max_u_regressor_focused   lr        0.633630  0.016377  0.032227  \n",
       "                          gb        0.237300  0.016377  0.032227  \n",
       "                          xgb       0.053516  0.016377  0.032227  \n",
       "                          svr       0.330513  0.016377  0.032227  \n",
       "                          mlp       0.075512  0.016377  0.032227  \n",
       "max_u_filtered_regressor  lr        0.453823  0.055683  0.105492  \n",
       "                          gb        0.589039  0.055683  0.105492  \n",
       "                          xgb       0.402428  0.055683  0.105492  \n",
       "                          svr       0.386221  0.055683  0.105492  \n",
       "                          mlp       0.004214  0.055683  0.105492  \n",
       "max_u_regressor_balanced  lr        0.389575  0.016377  0.032227  \n",
       "                          gb        0.607091  0.016377  0.032227  \n",
       "                          xgb       0.038495  0.016377  0.032227  \n",
       "                          svr       0.437418  0.016377  0.032227  \n",
       "                          mlp      -0.248941  0.016377  0.032227  \n",
       "max_u_classifier          lr             NaN       NaN       NaN  \n",
       "                          gb        0.496589  0.055683  0.105492  \n",
       "                          xgb       0.436154  0.055683  0.105492  \n",
       "                          svr       0.436154  0.055683  0.105492  \n",
       "                          mlp       0.237879  0.055683  0.105492  \n",
       "max_u_classifier_balanced lr             NaN       NaN       NaN  \n",
       "                          gb        0.557219  0.055683  0.105492  \n",
       "                          xgb       0.579572  0.055683  0.105492  \n",
       "                          svr       0.570683  0.055683  0.105492  \n",
       "                          mlp       0.168613  0.055683  0.105492  \n",
       "min_u_regressor_sparse    lr        0.391930  0.019571  0.038391  \n",
       "                          gb        0.586979  0.019571  0.038391  \n",
       "                          xgb       0.540216  0.019571  0.038391  \n",
       "                          svr       0.482436  0.019571  0.038391  \n",
       "                          mlp      -0.280415  0.019571  0.038391  \n",
       "min_u_regressor_focused   lr        0.268520  0.019571  0.038391  \n",
       "                          gb        0.299146  0.019571  0.038391  \n",
       "                          xgb       0.198854  0.019571  0.038391  \n",
       "                          svr       0.321932  0.019571  0.038391  \n",
       "                          mlp       0.021320  0.019571  0.038391  \n",
       "min_u_filtered_regressor  lr        0.360620  0.066541   0.12478  \n",
       "                          gb        0.560655  0.066541   0.12478  \n",
       "                          xgb       0.572522  0.066541   0.12478  \n",
       "                          svr       0.448986  0.066541   0.12478  \n",
       "                          mlp       0.169389  0.066541   0.12478  \n",
       "min_u_regressor_balanced  lr        0.410611  0.019571  0.038391  \n",
       "                          gb        0.521648  0.019571  0.038391  \n",
       "                          xgb       0.047840  0.019571  0.038391  \n",
       "                          svr       0.486502  0.019571  0.038391  \n",
       "                          mlp      -0.307781  0.019571  0.038391  \n",
       "min_u_classifier          lr             NaN       NaN       NaN  \n",
       "                          gb        0.583377  0.066541   0.12478  \n",
       "                          xgb       0.557840  0.066541   0.12478  \n",
       "                          svr       0.551432  0.066541   0.12478  \n",
       "                          mlp       0.223417  0.066541   0.12478  \n",
       "min_u_classifier_balanced lr             NaN       NaN       NaN  \n",
       "                          gb        0.505649  0.066541   0.12478  \n",
       "                          xgb       0.492417  0.066541   0.12478  \n",
       "                          svr       0.608736  0.066541   0.12478  \n",
       "                          mlp       0.226129  0.066541   0.12478  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['q'] =  (df['tp'] + df['fn']) / (df['fp'] + df['tn'] + df['tp'] + df['fn'])\n",
    "df['f1_coin'] = (2*df['q'])/(df['q']+1)\n",
    "# write df to csv in this directory, with the name dataset_benchmark.csv\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5425"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[('min_u_classifier_balanced', 'mlp'), 'tp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: max_u_filtered_regressor, model: mlp, threshold: 0.001591058368850724\n",
      "hybrid_metrics.true_positives_ctr:  5036\n",
      "hybrid_metrics.true_negatives_ctr:  22\n",
      "hybrid_metrics.false_positives_ctr:  85382\n",
      "hybrid_metrics.false_negatives_ctr:  0\n",
      "\n",
      "\n",
      "true_positives_hybrid_error 3836.445581894285\n",
      "true_negatives_hybrid_error 21.97816105587368\n",
      "false_positives_hybrid_error 67012.36356472886\n",
      "false_negatives_hybrid_error 0\n",
      "\n",
      "\n",
      "true_positives_rmse 0.23819587333314438\n",
      "true_negatives_rmse 0.0009926792784690425\n",
      "false_positives_rmse 0.21514647625109676\n",
      "false_negatives_rmse 0\n",
      "\n",
      "\n",
      "hybrid_metrics.hybrid_accuracy:  0.05444307717648411\n",
      "hybrid_metrics.hybrid_precision:  0.054149753935238035\n",
      "hybrid_metrics.hybrid_recall:  1.0\n",
      "hybrid_metrics.hybrid_f1:  0.10273635929447789\n",
      "hybrid_metrics.hybrid_mcc:  0.004213521798060436\n"
     ]
    }
   ],
   "source": [
    "threshold = _threshold(experiment)\n",
    "experiment = 'max_u_filtered_regressor'\n",
    "model = 'mlp' \n",
    "threshold = _threshold(experiment)\n",
    "print('Experiment: {}, model: {}, threshold: {}'.format(experiment, model, threshold))\n",
    "hybrid_metrics = metrics.Metrics()\n",
    "hybrid_metrics.get_prediction_scores(testing_data[experiment][model]['predicted'], testing_data[experiment][model]['real'], threshold=threshold)\n",
    "print('hybrid_metrics.true_positives_ctr: ', hybrid_metrics.true_positives_ctr)\n",
    "print('hybrid_metrics.true_negatives_ctr: ', hybrid_metrics.true_negatives_ctr)\n",
    "print('hybrid_metrics.false_positives_ctr: ', hybrid_metrics.false_positives_ctr)\n",
    "print('hybrid_metrics.false_negatives_ctr: ', hybrid_metrics.false_negatives_ctr)\n",
    "print('\\n')\n",
    "print('true_positives_hybrid_error', hybrid_metrics.true_positives_hybrid_error)\n",
    "print('true_negatives_hybrid_error', hybrid_metrics.true_negatives_hybrid_error)\n",
    "print('false_positives_hybrid_error', hybrid_metrics.false_positives_hybrid_error)\n",
    "print('false_negatives_hybrid_error', hybrid_metrics.false_negatives_hybrid_error)\n",
    "print('\\n')\n",
    "print('true_positives_rmse', hybrid_metrics.true_positives_rmse)\n",
    "print('true_negatives_rmse', hybrid_metrics.true_negatives_rmse)\n",
    "print('false_positives_rmse', hybrid_metrics.false_positives_rmse)\n",
    "print('false_negatives_rmse', hybrid_metrics.false_negatives_rmse)\n",
    "print('\\n')\n",
    "print('hybrid_metrics.hybrid_accuracy: ', hybrid_metrics.hybrid_accuracy)\n",
    "print('hybrid_metrics.hybrid_precision: ', hybrid_metrics.hybrid_precision)\n",
    "print('hybrid_metrics.hybrid_recall: ', hybrid_metrics.hybrid_recall)\n",
    "print('hybrid_metrics.hybrid_f1: ', hybrid_metrics.hybrid_f1)\n",
    "print('hybrid_metrics.hybrid_mcc: ', hybrid_metrics.hybrid_mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e50fe2de20>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIcAAAHSCAYAAAB2Cqt4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAC/2ElEQVR4nOzddZwb1doH8N9Z6W7d3bbu7rSUQpGiRS/uXJx774sWdyju7u5OoZRSLy016i5bo+62XTvvH8kkk8nImclEdvP73g93t5vJ5GQyGXnOc54jpJQgIiIiIiIiIqL0lJHsBhARERERERERUfIwOERERERERERElMYYHCIiIiIiIiIiSmMMDhERERERERERpTEGh4iIiIiIiIiI0hiDQ0REREREREREaSwr2Q0wqlOnjszLy0t2M4iIiIiIiIiIyo3Zs2dvl1LWNXss5YJDeXl5mDVrVrKbQURERERERERUbggh1lo9xmFlRERERERERERpjMEhIiIiIiIiIqI0xuAQEREREREREVEaY3CIiIiIiIiIiCiNMThERERERERERJTGGBwiIiIiIiIiIkpjDA4REREREREREaUxBoeIiIiIiIiIiNIYg0NERERERERERGmMwSEiIiIiIiIiojTG4BARERERERERURpjcIiIiIiIiIiIKI0xOERERERERERElMYYHCIiIiIiIiIiSmMMDhERERERERERpTEGh4iIiIiIiIiI0hiDQ0REREREREREaYzBIaIU89G0fOSNGIW9BUXJbgoRERERERGlAQaHiFLM+3/mAwC27i1IbkOIiIiIiIgoLTA4RERERERERESUxhgcIkoxMtkNICIiIiIiorTC4BAREVGaeXXCSsxYszPZzSAiIiKiFMHgEFGKEcluQAqYunI71u44kOxmEJVbT45ehn+9MS3ZzSAiIiKiFMHgEFGK4bAy4MK3/8JRT01IdjOIqBx5e/JqPPDjomQ3g4iIiCglMThElLKYQ0RE5JdHRi0JzQZJRERERJEYHCJKWcwhIiIiIiIiovhjcKgMKyopxfWfzMbyLfuS3RRl38zegP2Hi5PdDKKEWbP9AKRkoI/i58ZP5+DZ35cnuxlEZcbWfQVYuXV/sptBlFDf/70Ri//Zm+xmUDl046dz8OXM9cluBvmAwaEybOHGPfhlwWbc9tW8ZDdFydz1u3HLV/Nw17cLkt2UMsK/YWWHCksSFqC44ZM5GL1wU0Jey8lrE1bh8vdmJO31Z6zZiaOfnoDPecIkH23bdxilpeHv88/zN+HFP1YksUUUiwd+XMSL6jjJ334ALe4cFRUIGvD4OBz77MQktYooOf73xVyc9OLkZDeDyqGf52/C7d/MT3YzyAcMDqWABRv2eMqmEaJs1aQ5WBh4j1v3FSS5JWWFP8GcLXsL0OG+0Xhnyhpf1udk1IJNuPbjOZ6e+9G0fMxdv9u3tjwxeinGL9vm2/rcWrUtcEMyz8f3ROlt694C9Hl0bLnNFJqyYju+/3tjspuRUO//mc+L6jj5ef4/kBL4ds6GiL+XlJbdbM5fF2zCnHW7TB/burcAs9eaP5Zq7vh6Pi59133nzZuTVqVkYO+1CavwxsRVysuXlkrMWLMzji1KXcNfmYr/fv53TOt4b+oa5I0Yhb0FRY7LTly+DXkjRiF/e2rMgrtiyz78uXJ7sptBZIrBoSTRgkGHCktw6stTcN3HswEAPR4ag6OeGp/MplE5s2HXIQCBqH6qu/eHRTj9lamhf785aRV27D+ckNfesrcA45duTchrlVdrdxzA9gR9XgCwY/9hPDl6aZm+0XNr677A9h1XTvfVi975C//7Ym6ym6HsydFL437cOFxcgl8XpP7x26tfF2yK2/D4rMzAZW55OkZc98kcnPnqn6aPDXthMs56zfyxVPPFrPWYuNx9581jvyxNySGBT4xeisd/Xaq8/DtT1uBfb0zDhGXl81huZ9763fhh7j8xreOjaWsBAFv3Ol9zaB0OqRI4Pe65Sbjg7b/w1+odWFGGSoNQemBwKAl+X7wFne//DbPX7kJRaSkA4O91uwEAuw4WYe2Og67WV34ueSiSP5lhGcHVlMX95LFfluLmLxMzbPLMV//E5e/PTMhrlVdHPTUBfR4dm7DXu/u7hXh1wipMXpG87LBkidf3ubikFGMXbzEdhiqlxNjFWyKGtKW7Vyesivtx48nRy3DdJ3PKbU/zdZ/MwfHPTYrLurOCJ8CikvTYZ3ceKDT9+6J/9jCD1SUpJd6evBpb98Yn213LLN60h9n0npStwROmzn1zOo6L07GPyCsGh5JgavACb+763TEd214YGxhWUGZq3ZaVdsbZwo178OyYZQl7vdDwQ4UdZf3Og9i055Cr9e8rKArt0/HgpWfRi4273b1vN5L9HR2/dCs+mpZvu8zstTt92QaJfK8FxSUAgNJkb2AHJaXSl8yFfQVFOFRU4kOLrL0yfhWu+nAWxpv0Zn81ewOu+nAWPpu5Lq5tSEVSytDQ6HibvnpHRDbBxmD2555DzsMnKJIWHCoJdsSlq5NfnILhuqxccrZq2wE8MmoJrvvEepj8HV/PR96IUQlsVfl0sLDYcqikM/VzaxmrxhF3+w8X44WxK1Bckt7HRxXrdhzEfz//G4XF5X9bMTiUBOF79fABzW2x4NJSmdRaKqlASomZ+TvL3ExQZ7w6FS+OW5nwNHeVVzvyyfEY8Pg4V+u96bO/ceHbf2HbvvgNJ4o1+PT25NU+tcQdv65DCopK8MTopSjwGBi4/P2ZuPeHRbbLnPXaNAwc6e6z1xSXlGJzEns/RYp3IR75xDh0feC3mNfT5YExOOf1aT60KGz9zoNof++voV7s9bsCmavb90dnIGzaHfiMk/lZJ8vL41ai432/WWZm+Om8N6fjsvf8zUbaeaAwoUM+U0VmcFhZUZKz3YY9Pwkd7xud1DZQtGOenoAbLII/WqfDXpug7Bez0ruQ/L6CIpz60hSs3Brb0Kj/+2Iuznz1T1dlBLyc9VVvF176YwWu/Wg2CopK8NfqHR5eqWx4avRSPDd2eZkoO5Fsd323AD/M/Qd/rSm/+4OGwaEk0N/IeC0qnSrhkJn5O5MWRf185nqc8/o0/Lpws+UyRSWlWL0ttcama0GhRAW1tD0sXi+3fHPgouBwcfwyGmK9qXlk1BKfWpIc705dg9cmrEpYUXG3Rv66FP0f/yPm9azYsg+HCuObGZMM/+wpwAGf35fT8ePj6Wvx+K/O+/2P8/5BQVEpvp69wXFZTWqH4uLjh3mB+hjJDLC4PYQXFJVg4cY9AICeD/+O3o8kbsinitJS6TngrSo7mDmU7J7xpZv34WA5PLaVdau3H8Aoi3pe2pD8VM9MjYddBwqVrukmLd+OBRv3xDxBwoINgeNUgYf7CZWPx+0565nfl2P0os1of+9onPvmdKzdkRqFrP2mHZPifR93y5fzcMFb0+Oy7ikrtptmOntVWFyakE6gVMbgUJJ5DRBEZB0lKVS0ZNNenPP6tKgbkFXb9mP4K1NdXYyNXrjJ9YFjTXDWgfU7I2s0/TB3I/JGjEJBUQkeHbUExzwz0Xao1LO/L8cr41e6eu1YZAQDgl46MktLJQ64nNkulKmWMiHF1HfWa386ZtEs2bQXeSNGRe1/fvtoWj6eHB0Yhpiq6ax+nJgLikpw3HOTcNNn1jOYLNy4B6Pmb8Kbk1Zh9tqdSR+ul8ru+X4h3pgYQ8Yct62pZATGvA6FuPPbBTjlpSlxP0Z59cBPi9D+3vhm07gYVZ0U38zegLwRo7BPYcalskhKiV8XbEJxSSmkjH8w0F+BnSde+04q75s9Hv4dl7zjPJOcX9eV2lrcHOoSOWPz3kPurrullHhh7ApXndNfz96A//NhMoa9BUW46oNZWPTPHuXnxPv+4Js5G/DnqsiMm4Ub9+Du7xbE3FF+0Tt/4XIfM22v+WgWej78u2/rK4sYHEoC/QnBj69jsk4sO4LDDpZt3oeJy7eF0kqHPjMR89bvxncupiO+9uM5UQcOJ6GMGMPftRvpbfsOY3owHdQuCvziHyvw1G/+1ACSUuKxX5aEemvNCKfeKJvP89FflqDT/b+5usDSMtUOF5XilJcmY2Z+fKZOtTpRl5RKPP7rEmzdZz8U5R+bejeJ3MellJi9dpdj/Z3PZwTqroxdsiVubfln96GI4WAZKTpg3vjZe7kwKgwGk+1SuE95aQpu+HQOHvtlKc56TTe8KjU3i6PV2/ZHHZtWbt2X5CF6AWUxmDx77U78MNf8vFPWhh/74e9gDY8L3/4ryS0x9/kM90NyTnxhMl6boD5duCZFD514PTj1+T+7/f3Op0rx6V8WbMZ1n8zBG5NW441Jq9H+3tFlZnhjuGMtbq8QXH9qHpv+WqN+rSgg8NfqHfjD5Hro+k9m49qPZts+X7se9vI91bZecUkphr88xXWdSpV6cm6zx3YeKMRzY5fjIhfH3lu/mufqvsnK3+t2Y+ySLUqd3sk8Ll723gx88tc60yHsyeRUsiUdLiUYHIqz9TsPYvE/eyP+pr/49rqTpcK+qZ3QhAAufXcGjn02suL+YYssB9/qgyj0umg3rV62c5GHNPTDxaV4c9Lq0FSy+w8X4/pPZkfU4xEx9EZ9Oycw9MNNerp28F+1bT8WbtyLe79f6Pp1l2zaiyvfn2maueL0Nv5ctR1vTFyNu75dYLvcETaZOqVSYuLybcgbMQpLN++1XM4P8Tjwe734u8UwU5ufJ/K9BUU45pkJrnqXrBibtcPiwuj3xVvw/lT7oXH7DhfjS10dh8krAp/7KpNAUzKOg1v3Fvg29ewxz0zE0GcmRPzt2Gcn+TJEzy3tM/RzHysplQkdznPWa9Pw38/nmj72wZ/5Ma1bNbh03w8LXQ3R88Ofq7abdkho5791FplDpaUSLe4c5SnYkixLNu3FE6PVpwtPpuKSUnz394aI2f3W7zwY+vdrE1ZhhcOU7FNWbI8IeGrDbwD7fXL4K1N96Qj64M98vDnJ+/6hBYI27ynAj8Gpy8tKzbKM0PWjf2eaScu34ZvZG7BlbwE+m5H6hf2dJk/Rb5pz35yOKz+YFbXMLws2Y/Qi6/IP+vVo18ffzN7gODuj8VS180Ah5m3Yg1u/Cl83Ldiwx/Hz+3TGOnS87zfbDEu3e4C2vJdhcn7ZV5CYCRS8cvu1Ki2VeGbMMmyJ0+yBACJqwV6ZxrMXMzgUZ0c+OR4nvTg54m8RF9/B/bCwpBQbdqmnfrv9Uo1ftjVu6bypWgxWCO8JBfnbD6DN3b/ie49R/OLgAeab2Rvwy4LNeGncCl3DAj8seyJsGi1CQ9LcX6zEcnlzxzfz8cfSrViyyX1gpjg4hbDVVMKHi0tCPdxWpARGB2tLzcpXm9FCfxHthuowB+0GMJ57v7FoudlrjVu6Be3u+dX1cMNpq3Zg9bYDeO73Fc4LuyR1xzW9f384Cw/8tNjx+bd/PT/0+0/BWi8P/GhdUFv1M3h01GLHHkw7eSNGoe9jfzhOPfv93xuVh/LsOuhtOEk8gpi7Dxbiy1nmgY2CohIUlrg7h5zw/CS0vvtXP5oWsykr/Ski6RQ8+3Da2oibk0S44K2/cMpLU6L+btbUVdv2h4ZZF5WWQkpEBVv2Hy6OucBsWVRcUopdPtaaeGvyGvzfF5HZAEc+OR4vBq8H9Nu9VErTzpeL3vkrIuCp7xxxOgbMXbcb3R4c4+m8rbn/x0V47JfoYFxJqXTMBi7rtO+Pdho+89WpuNOhk8vJJe/OwC1fzXOdcbxq237kjRiFKSviNzOsmRfH2WeffPLXWgDAthizwULDyoIb/Zav5uECxawb4/dA+/fohZtx6stTIoL1Zl8Z7dpy9XbrukJuA4R/rQ4EZstK7ZqykA3z9/rdeGncSl+G3lnRTxzyx1LzcgmpmoXqJwaHEmje+t0RNwyBYWXhG+dBT4xXXpc+E0H/pR69cBPOfSNyNpslm/bi8vdm4r4fzDNG1u88iG/i0NOZqGONMStDhtJThecx3UuDRZatChU60W7qtSCOfjhQLEUOvRSXNh7IVJ57uLjEt94yfYaZmQd+XIwzXv3Tdh0LNu4JbTetVVv2FmD4y1MsL1BPfTn6ZunA4WLTHnZ9doMxoGFFKzAc1/3csM30Mx1qva9P/7Ych4tLQzW4nExbtQODnxwfChb7caKzWoefFxyTTS6K3e6jb01eE9GDOXvtLuSNGBVV6LugqAS/ePzuA8D/vpiL0y2mjZ6VvxN5I0ZhVoy9+st8yl7Sb8NHbQq3t793NF4Z7y6DYKVDVoQKbXvlK+7fVmLdz93uypOWb4t7lqMjk/c89JmJoRkpf19sfoN6mUkmsF7n+3/Dh9PylZqwZNPemM4l//38b+SNGBUVOPnKp1miduw/jH0FRdhzqAjDXpiMHg//7tvkCtq5adfByBvE6SZDZ+/5fiHa3uMcSNUP4S2RElJKPPDjItPz2tglW7DnUBHen5rvsuXRjMO+nxy9FH0f/SOUGfTQT4tNa8+VpRupj6avxey14eNyKHMo+O2fs263b9k+bjtVZwSHeP08/x9fXt8v04NBkFjrmoUzh6LNXrsLd3w9P+o4ErVvGf6tXROt3LY/5l48tzVCJ7kc2qbX6b7ROO7ZiZ6f70aqdu6b0e6Z/ly1A+e/6a5G7R1fz8dHCucslYlDykIgLVYMDiXQ8Fem4sgnx4eHOsGfnWzxpr2hqTiv/XhOxDjhCcu2hmoDWd08HvnkeNzy1TwUlZTi/Den44TnJilNsx46mPt4bFG5iCwoKkHeiFGWAS19YTuvxZi15/2+eAu++1s9cGYM+JhtRu1g7KUgter72bq3wDJzRmVbtLtnNN7VXVBqb2tvQVFUhordSV3lcZVhTe//mR/ez4Ir/ODPfMzbsAdfWWQ6mLn+kzk45aUpUTNiqWQ3rNy6Hye/OBl7DNPaTl6xHVd9MNN23/1y1gYs93AzH3XtE9wIn89cj/6P/4EFG/bYfv/u/X4h8kaMivjbo78sxrqdB7FiS/xm8fPjmOCmt9trYcpP/wpc6D/8c2Q204M/Lcb1n8zB7LVqWWpmdlj0GE4KBrnMgl2xuuqDmTEFUYqTPN23mW/mBLIupq6KX4/5ocISrNuhenOjtq9d8u4MDHt+svOCSXTjp+bF32fZ7PdSSuw/XIz7dLXQZq/dZVpHZuziLTjxhcn4do5aBu6UFduxcfchtL/3VywLdtD8EByK9KUhGHSbLrswFr0eGYsjnxyPK96fGQpmWmW5uhW+RjLWZIsOnDoda274NHCNp19TqZTYfbAI7/+Zj4veic6yCHVOebjSl1JGZFwYt7+W+aJlWr07dY1pUdiyciO1cus+3Pv9wohadqH6kAkcGTR7bSAYru0Pm/cUKAcrL313Bt6eHMMkBD7RAkXb9h3GkKdUO73Dnbr6IeT7CopwwVvT8cWs9SgoMv8gtGvagsLS0F/85+0ewosDhSWOw01VFZeU4stZ60NDWfcWFEVkM2nb2urdvTxuBY580n5ylljZbav87QdCpQj0i01bvcOxRIHeF7PWR9Tv9KIsBbpjxeBQEpiMKlO260AhXh63IuqEa5bhMnf9blz23kyMHB3oDTa77tf3Nv25agemrd6BZVv2Kc2c4fXwu27nwYiidW6G0wEI1e/RipjZ1hzyGBXXP0u7OFWhb8ud384P3XTqDyr67A/X7VKskDj0mYmhzBljEWPtZccs2mzbC/XjvOjHLn5nBgY9YX2iMHtPZhfIUgbGDrv97IHwW/ey/80JXnAVebjae+GPFVj0z15MMPSOjlu6FWOXbHW8ofCzELjW87xyWzjgZLY7fTR9reU69rschmYnHr1PyZxhSdsv4zGDkFUhfaMbgzeDboxdsjUq0OVGxLnJroEpdpU08teljj3q+hZ3um803tVli1378WwMVr6JcWfrvgJc+u4M7FEYPmi8CfzNoU6Hk3h8Smb7xVmv/YnhL0dnymk3HpZZboYGXvTOX7j03RkoKCrFp39FHru81ABUtftgERbYTCJhR0qJvQ7HCbPPwe2xZdT8TXh94qqIQI/TJUS4k8/9nvDnqh0RwyONx3gvAfl4HzYOFZZg6srtmLh8Gy5/bwZKSyUmr9imdK318/zoa2iv7b3ly3m2s24CwP0/mmfyT1weCIJPWh5od//H/8BNFkHc6OduwyM22Z9ONu8psJ3ZV9VXwaDi6IWbkK8QdC8oKgldzwsRuH7V3PTZ35afg3GfvPv7wJC/0L2By6vES9+dgdNMMs4Bb525yaLf39+avAa3fz0/FOjt88jYiJm47DoCAODpMcuxfqe3feLA4WJMdagb5eScN6bhgZ8Wm5ZFeeCnxVi7w11n2J6DgVqbXjpry0qg2w8MDiWBVqhZSvcBgju/XYCnxyzHNIeZvQqLS0OpzGu3Bw7OZsOY1uoO3Id1Xz4/bvb+XrfLtIjsxt2HIorW6YthxvLl27j7EHo8NAabgsNt9CcUt+uNSN3WnRWklHh78uqIAtN6+tTrz3QzsWzYFf676lT2xz47Marug9aqvo/9gZttxt3u0934G4fWaZ/51R/Ntuw51r8WEHmStauRot/OLe8chcd/WWI6RenyLfvx0riVuO7jOdh7SO0i2VjIOx6Za7FwuhDx8p0yvjdt3xE2y6h636JA7w2fzMEOh/oBhcWlEccuv4Y4mVH97mo98TsPFEYUgP18xjr893O1i2tNvKbIvfu7BXjhD7UaTz/P34SeD//uOGuekdemCwFfogkv/bHCdsY5VXsLijBuqXVdjoKiklAw5fWJq2yPZUYHCkvwkC6Ips1uE48ZzV6fsBoTl2/DV7Odh0H98HdkgOurWevR+q5f8OvCyCDRhl0Ho7IfzXjZj/VBY9Ngv+53/QW72/3UilY82yzbxs7if/ZGZHWWlkr8tmiz+rlft9wLY5cr3zx8OmMduj4wxtXMjID1+d9uHxz569KI80hJqcSB4CxLZvtDqc05csOug7Z1pYznZavh+164eernM9bhp3n/YP6G3ZiyYnvE8MLdhuF693y/EBe+HQgwjl+2DV/MWo+L35kRlfWk2qbwhCbu3us3czaEauVZ0Xcm6VefqXvNUEfe4i2eax1JKfHhtHzH2bgAoP/jf4SGnMYitLsZdrynf1tmOtx65K/WBeZVMpyN17axmG+VcV8GAwNCiNB9oPZTu/9cvW0/zntzmuVz7XzwZz76PDrWcbmbv5yLC9/+KyrgOHvtLvR7bGyoYPbWvdbXm073B26zPI9+ZgJWbzuAlxxqaRkdLCzGlBgDXWUJg0NJoN2USQ9zGGkXbk69aFd/pJsxwBAcOPrpCbj7u8CJRn/s1l+sCMOeUVIq8fviLREnSacT5rdzNoZ6AEpLJQ5ZFMT26z7s53n/RAQuBHQ1h3TLFZeUIm/EKDz3+3Kl9U5esR3nvzkd45dtxQd/5uORUUssbzZv/8Y81V2/qbS3qw867TpQiHPfmBaoI6NbdtrqyGwT/bb69u+N2Lj7EPJGjLItvmwMSqjW1IlVqQTemLQ6ogZU+LHA34pKSpUzWLQZ2rT1hdbr4Y7Wy4leP6rNroD25BXbQmm7T48J72Nm+/lvizbjru+sL/qie2ut26dyNJmVvzNqnVsNMz+MWrAJL1tMgfrOlDV4feIqtL3n14jgp3WbAsXwZ1hMibu3oAj3/bDQtFdI5cZW+xzfmbIGxz47EeOWbkHPh3/HU2OWhfaPEd8uwA9z/8HTv4VnXdGGk6m030+f6F9XYSfceaAwNMOP+g2K9wOqH50Cz/y+HOda1AOQUuKnef9Ynr/0++KNn/6NK96fFfqb8e23v3c0jnSo06d9D6WUGGNSX2fXgcKIm6549A67KdorISNmVSuV5kP9Bj0xHpe/PyOmdpllu+zYfxid7w8X5DTbHvr98LdFmx2zZtwKny/Un7Nw4x6c9OJknKfb7z75ay2u+Wg2vpmjNuxYf/x8a/IanPXanygoKnGcyGPckkAm6ept1j3YZu/F6vvs5txUKiXemhQYRmQ2O2xoenCT5w56YrxlXal29/yK58f6O1GBEN6u9UZ8uwA3ffY3Tnt5Ki5656+IIt7dH/odl7w7I7QtjcEuLftU3zmnIm/EKCzbvE9XH9J9u93QHw+113xp/ErTDl39n/YcKnIsAfHHkq2474dFePyXpXEJftsxnmdfHr8Sc9fvjvhbUUmp7exTBwuLQ8PJAjM8S7S8cxTen7oman+y2r/Wbj8YGtpql91uR/+8m7+YG5o52Hp553XGi/bSk5ZvCw+NNLTnmGcmhupFAcAjPy+Omh3O+Flp7v9xEbbtO+x47F8eDOwZA9fP/b4cW/YeDt2LGCdtsmJxxFR6rkZ/TVBUUop5Fu/R6I2JyR+umUgMDiVRIHPI/LExizabFhhUDSdNWLYt9J3RorPaM9dsPxC6SYkcRhBet/EY++ak1fj3h7Mi0txDGSEKZ/xHRi0xneLSSFundkFd6mIq5MyM6HaYtUw7IL1hMz2r8XnTVu/A5e/NDM209JfFza5KrSazYWVfzV6Pv9bsjB4vbthBjEPExger6X9qUSQxlqK3VtlRRpu1mzeYH6a1TaL/eLycOLWL/DcnrUbeiFGhMdmuLjhDNQQk5jjMkBb1VN3rWBXQLiopxcXvzAhN4W5Wh0Pvmo9mKwcqgPB+ueNAeEildkO/W2HYytmvR/cUzTMJLFp9Pg//vDjUy2fXM6r/SC5/byb+9YZ5D9WLY1fgw2lr8dmMdZ7CElpPztzgZzln7W4AgWzE2w01SfQBLy0gZ3U89StvSEqpdExQ4ffNmiYicB3jG//3h/bH+NELN+Omz/7GKxbBx76P/RH6fc32wPfbLpi91eYYNWbRZvR8+HdMX73DMjj5wh8rIgrMWk0SsHTzXtveTY3ZjYfZcBXr5wcuvO0e10xfHf2eNuw6iFXb9mP+ht0A7Pfjx3+JHn5inHHI9ObU8PhdFhkNH01fi/Uehg2Hsl0Uv4UTl28LzdSmr1P2TzCDeIvC52amsLgU7e8djd6POPeQuyVhHXBwmqjC2KFXZHN8MZsQAzDPMtI7XFzqmA2qXfet2X4gqq6dnt8BCWM2+qTl28LDXozZZsGfAoEgg12njrGVX89er6sPGd87/Qd/WoydBwqxYMOe0PFCSthmKhwsLEa3B8c4DiM+GAxu7jpYGFOQ6+zX/kTru36xfNzs26pSN3DENwsijmvGTa3v8C0qliiVgf1en/np9PFMjKE4tEa/7b79eyNu/nKe9cJwP6TNzMbdhzwNf9JT3YcPFJZEzA73v8//tpxUQ9P1gTHYsrcAM9bsNL1P8+O7bxb09suTo5di+CtTlSaOsLtfLI8YHIqT0lKJAY//4bic1QHk6o9mm05Na1XgUInDFzUic8iwfu0ib9v+6CKrKi1RnVnk69nr8euCTej76B+YsGwrbvr8b8diwdoBKMsQHAoMk4hOC/ajd1y74fti5jpc/t6MUA+VyrEwI8N6WJnT0A7jQ1o7rHaHs1+fppRObGbj7kP4Ya59IVFjJtl4k6kftULofmWIaRf944Kvpf/Yt+07jC9nmu9rV74fLpb54rgVOPPVPz0VHLY76Wubw+yE7uXtG7fZroNF+Hr2BtNixpe8q5ZJ4NfnoHLB7HRxUBJ8fNT8TZZBFJXMMu07pf9svpq9wTlrw+ktxHht886UNWhlc0GtQusptwoAm3lWMSvSSHXXsFrOavYrzc5garvKDXus15VaQGjBhj2WqefZmZHvZJRFIGfY85ND+6Hd98cugOfl/Ri/P7sOFtoO3xr0xHgMfWYiTgvW/7Frq0o6vlmbjTdyZlM1Hywsxr3fL8TH08332W37DqPFndEzkAXWaX5Os2qt25oTVqw+H6fjj/a0NyatChXRDq/TInNHWh8/nT4V/XGy24NjMHfdbstli0u04FDk3x/6ORyAzBsxyrQjUpXZMJzTX5mKaz+abfs8u2wRr4zvUz95xs1fzMMZr/7pOGRas/9wSWjfUvnqfh7jLGab9xTg1JenRATlTL8fwdYcOBwI+qjWWpOI7WZ91tpdthMW6B9xc40xZtFm5UDKh9Pybd+D1csa94tDhYFJbdwUNDZ7XeOwRj39pjKbxU/FwJHjcPxz4ey+0lKptP/qRxKY9Jkr+d5Qa9VqqPjohZvxrzem4Rmbaw79/eThYudsTDNWH3ss1wkLNwaCQvf9sMiy6PuO/YcxbdUOy2Lo5RWDQ3FSXCpDtW8ARKTCRnC5Y7v6IhgOCk69BmYXK9v2Hba+OHLRlgybI5Q+WHPHNwtwXXDmtY+nr7W8WI9oRrAdxsyhg4UlpsVftYNAQVFp1JCaUJsUD6h3fLMA45dtwwnPBw7gVhd8Y5dsCV2ohIaVWR7trF/PGLQLTUdu00anaeJXbt1neXE4Kz8QPInqzTlQiLwRoyJOelICj/0a3Rut7furtx0IpanqLwa2mwQc3dDvP1d/NAu3fzM/atpdAPhj6dbQklqP1jYXQz6051rNvCNl+H0Vl8qo3tQR3y7A9Z/YXzBHvabhg3194qqI4VFSRl5ASinxwtgVOO7ZiZb7tl/cnpTNiutq9QRmrd1lOrPTIz8vjqq3YkY/5E+v76POAXq9hRv34FBhiVJPo5ZW/uBPi9DtwTGhv9/+dbhH0a7Whb6pX8/egFNfmmJ7sakcuBHAi7q6Rk43fvp9TJ9hYPfxvvDHCheze3lj7Ah5Y9IqTzc3EtLyeF4xOzPi3/+zqeOmWfTP3tD2NU5r/bXFDJpeGc/Z93y/EANHqtUEkVKG0vrN5GY7X/6Znc/0fyuV5hm7Ttly01fvsDx+hDOHIkkp8YvJxBt+Md74uu2xnpm/K6qYrV12danFNrrr2wW22dLGpy22yc4Id8xEvv4/uyPPDfrJJ6y+Y2461eau343RweO9VUemUw2dvBGj8KTJdbPdIcD4Sh9OWwsAOFRUEio4ftAqa8qw4s9mrAsNj9Um0LDzmUmn1E/z/sE7usL3dswCJJ9bdHTp7S0oVjsuyshj+p6DRTEFi3o/8runSROA8Ax3mog6ZzZnnv2FxVj4T/T+rj3H6pnGzDltONQrE9SzQczWfdxz5sMygcjdaZHhHGz13Xfy4rgV6PXIWNvA6j+7D0V0DoVn3Iutt2Xc0q2QUkZ19GsdcGa1ocxeceDI8Y5FsM14zd4rKZWmHRh6M9bstPyenv/WdJz/lvkw+fKMwaE4MZ4PXzM5CO06UOgYXzEOC9IOgsaLUlOGlZt9uSJTlMOPaxd3fR4di+OfnWh6WaC1RSWQYnYB6WTsEvNou9XrZRrma31+bPgAqX/r+inanVJDVR0sLMGzY5ZhkcmJS6MPFvql2CFzSMWxz04yzVIDwp+xcfp27X3qx+FKCds7yhVb90elqfpR+FdC4uinJ+CFsStCQ+Gcbk7C+4Pa6+86UBjqObCbgtzp/PXLAvXZh96evBpTV0b31hgvnvQ9jdNW7cBzY5djxdb9lpkmZr2RUa+hcCJ2e7K+xqQnWZ82v9nkgudtxQtrbT9yc/1jfI8bdh3EKS9Nwc1fzg0vo9vWxuW1wtLvTc2P+H58OSscINis+J2/9at5WLBxD578zf4GxIyxEK5xjz7lpSm2w0u9Diu767sFGPa89cXxJEOA7cNp+Vi11TzLQ6sDF9muyKyL9TsPYfGmvSgtlUrZImrnJetLoIKikqjpxgHgP5/9Hbr49lokVlUsl/NPOexLxpt9KWXUBb7KV1x/HNhzsAi3fDkvNJTdisr7MvvuX/9J9M2oX0NA/WAMKNkNC7I6Vn01e4PpUN/Q81wc5LwMyVAdBqt9N1bYFLUGzM8lElLpeuhVFzfvgPX1RFGJdYBYhZRwLGBr9j5v+uzvmGaPHGeSiR16veC3qLC4NCpjUd8hoL3vklKJtyeHv1PdHhoT8W87+plWtcDO9v2FrobM6uUbjt8R1zkOu592DSmEiPq8ra5JjJ99hczAcV/LYFQpNKytWx/Isiu9YBfkcnvtdPOXc1FQVBLKzLUt4GyoA6TV24p1dPsbk1bj2o9n4zbDcH0V+s3vVGoBAN6fuiZqtj+r5ju9rcd/WRIxO5tmseFezSozyK6DpTxTCg4JIYYJIZYJIVYKIUaYPH6tEGKBEGKuEGKKEKKj7rE7g89bJoQ4wc/GpzKV7/7bU9Y4HlyNNUK09ZqdNIwX18abWGObAvUhhOnj+p7wf/YU2EaqVc65ZrEhbTp7v4a5GIeVFRaX6tYdfnOHdMOsDlgMuVJpk3HIyosOFw+lUkYM8dIuJvJGjMJjv1jP1mDXLm1K83hMJQ6E94kDhuyxi975K3pZF6Os/RzCP2/9HqzZfgDP6YKBdunPgPoFsKbHw7+HekKt2NUQ88JqSlr9kJwPpuVHPHZA1ytqVUxw6WbnMewqb8PuAsePoJ+bNWhffTfj/D+evjZi6UHB4sZW2+0PQ7DabB8y1raxuzkzK5JaZLO8VX2dY3TT/gLmx67HflmCvg6zi+gL+OsdLi4xrSkyZeV2231JP8xRSon7fliEdy3S+M22UzjrIvy3klKJV8avxFFPTQj9zW5Gs8BrWz9m12dx61fzcOyzE5UuZlV4qUGxYaf37Cy3N9bPjFkedUHuVBBXP6MSAHw2cx2+mbMBr04wPxeu2rZfaeaoWIz4Zr7pzYDbosRGG3cfwlO/LQ0FZwqLnSdT0IpUm32vLDOHEd4vzQJBdsddrdZU9PrUj6ZO506j3xZZf/++nLkec3TD3nbqsoQzPXZNm7UuEGw6ZHnOUJm51u5dm23zrfsKIobvxHruV72G+3LWhqhhSsYZMPUdAtp6Ry/aHDWCYewS+2On5hzdfYhKwNHV9aiLRfWdkfqnadve6jPQj1wwLqL6uWnL3fv9woi/G6+NLV9Ix+y7b5eZ9u2cjZajKEpKpW3NnB+Cw8P8qJtl910342X2OCklHvhpcVRNSykl9ijU1Swplej9yO/47u9AJ51V5vnq7QcwTTdUzs31po+X+SnL8fAshMgE8AqAEwF0BHC+PvgT9KmUsouUsjuAJwE8G3xuRwDnAegEYBiAV4PrK/dULwSdaroA4RkXzn9zumUhZDPGC3HjwcFYH+KZ360PTloWj9MXyDIl2eTi5MoPZqGgqCTmk+q7U9cEpmQ0vMSvCzeHTt67DhSh9yO/Y/6G3REXP8YIfGmpxLRVatMwux2ycttX89Dxvt9006M6P0dbZOXW/Xh94qqoi0wtADh11XbfCt8alZZKy+nr9a8YKBTorg1+hLRysqIPY4+Osu+t09JarW5W9T75a61SO6RieGy0ycnKa3q31bSrQLAovUf65hwsLDatJaXS4sSN0w5Fh5R997fzsVfKwP4/deV2rDXcqJt9ZHPXq6dLm73+FosAkJfaWHpz1u22DC7p34bZcXrvIfsb4Jn5Ox1nLTEemj6bsS5iVi6zm9fQsDLD36auiuz0uOJ98yLY+veictOo0Yb0aOeBI1xM7aw6nXthcSleGLvCsfbC6u3+1NNRMdMku8y8ILU+my5yGW0a+v0WmUNDn5kYVSg+ZoYP8fOZ67HzQGFo/9Ha9Gfw85RS4hsPw/8GjhyHV8avwsJ/Asfci97+K2JmN81HwYD9FzOts7slJFaZZKVptO+D2Qx7dkElrdaUkXE/tysUbhUcei4iE1vtQHv7N/NDQ9Z+mPtPqF6glMD2fZFDPd6fugZ5I0YFrwndnQ/nrt+NAY+Psx2uEp65NrDuxf/sxRW6OoR2jK3JGzEKfR/9A6fqhhEucBi+u2nPodAwv1g99NNixwCMyrHIKmh49mvWpQj86My1KyXg5pPXN+W9qWssA8D69zln7S7TY50TrV3Gadlfn2gehLcNNppcFjlmpiG87bXafRt2HUSru37BsOcnR2XCGK3feRD3/7DQdplY+NXJrw0FNZIArv04Ovtcf6iYtHwbDhQWY/v+Qtz3vfXEDlYSPZtfKlOJ3fcFsFJKuVpKWQjgcwDD9QtIKfV7ZWWEvxfDAXwupTwspVwDYGVwfeWe6j6mUhTyyCfH47FflkREOf1qk/4LHZrxwW4dCPdmma3vzUnm0/1lWhw5tu077GlGE71dB4tMZ24Bwhc601bvwPb9hXhh7Aq8pxtWpj+BFpWU4v0/83H+W9OjsgT8oKWK241/Nd6cadv47Nf/xMhfl6Ko2Hx/Wb3tAF4etxLjl2319QB3qLAEfR+zyTjQvdQHf+bHLUBlp2KFcLxZuziwGpJobN6MNTvxvU2Q4HBxCe7+Tu2EGrhRcl7O7AR3uLjUl89tlENxSrf2FhSh432/4fL3Z0YNsbF7r9pefMji5lelB8gNrRc6HrPKXP/JHFz49l9RwwNKpcQKQ+Fx4wW722EdxqFYmrNsLtaNYski1D/zzm8XYOnmvY5Dgs95fVpodj4rZseF93SBWeMwA0A/bDn8+mOXbLE81luRsC7QbRYM+9PQOWA3W5reeW+az8gXaoduE3w8fS2eG7scLxtuCEZ4GKZmnL5bVVTBZ5OvjmmWhu6Pt38zP+Lz0GZeNH7cm/YURNSo8nKs+2i6+U2D27191IJNuOUr78PJtZm+ZljcYN77wyJs2HUQd3wT/izNhic+ZDPcSPtszGZ78hJw12+jtyatDmU0mSlRuCb1MtRCP/R2+CtTo4YRvxa8yd51sNB1lpfTjE4i+D8gMKvngz8twv99MTciA99ul/TjtDLg8XE4+ukJsa8IgXPPVMNQKKsMFrugldWoWi81YfT7mFOgwGlWSy8e/Mn6+6S/5v585npc7VAs3Yx2bWE8tunPD0+MXhrKsrW7FvFynfLtnA3YdSDwHbo0mJWrH8Kuje6wOv9/+/dGfGAReInFK+OdM1SHKOz3BwsDtbP0x8p9uk6nWfk7HbMaI7KVHV8x0gt/rIjIvLeTSkOZ40UlONQYgD4PeEPwbxGEEDcIIVYhkDn0HzfPTWeq07RbBV0S7d7vF6LlXb8gb8So0NT0K3U1L6xSxq1uMLycdL0MWdFex3Zq0MMloZ6deNQH8uqNiatC05Tb3ac9N3Y5Ln9vpmNdm6Ht66HjfaOj/m7W0/nt3xsdennCH+D7f+YrXdTtOVQULqTtw1HWTcq8Wb0dlfHmKiTc3fS8q6up0f7e0RGZFF4ZZ5jwSvtcn9PdVEdNfxx8r0Umx7A/bGolAIF6B34KT9eq/hyrz0u/N+0/XGw5nLBUSpxpKPbuV++ZKrMZcty2we6ifvbaXZaBfT27DDbA/GJYf6F34gvRxci1h/XHPKteWjPa0w4cLra82TE7dszM34m8EaOww6GIpXHfcRO00oYXvzx+ZdTU3G7MWbcLxz5rXffJDdMsIZPLE5Wv2G5Djbqf5v1jORuMKrMhCgts9jur3XZ3jIFpY8DbbIi/MTPAqjfcysz8XRi/dKvpe4ilhg0APPpL9HBl/csUm6U1BC3ZtBcrt+73PQj/5azwtPEz1uzEkU+Ot1zW7JjtdIO688Dh0LZ8YvRSvDc1P6JWH2B/w55qGQVSAm9NjrwnuPID8ywou2Omdvy7/4eFOOWl6GOwmX6P/YEHfrTPyojlNKi6qYVI8Pk22C7j0HHt7/sKiiJqy+rfxyvjA5MpfDQtH7sPFppOmOLkz1U7lDNTY7He41Bmp5lKnXS87ze8PnF1xPlevz2tMoSN2fprdIHv0lLpapsZA67pzLeC1FLKV6SUrQDcAeAeN88VQlwthJglhJi1bZv3oRBlUSIzLcyGvPhxbNVnHO2xGIJg1UPxxSy16T/zRowKzb50m0Wvn9ficGbLJOKco3ISvP/HRXj81/A4cZXAmFlhX+PjZjN26Hs6VXm5Zur24JhQLS1/gkOxr8OKmywMKaWr4IS+YDoA/DDP36yfWOw5VIzHflmCg4fD+4nxs1q97QBWbNlnmnl1aww9815o+4BxKmk7f6/bjf2H7W9Y//v5XMvHSmX0jaIftZbcMMs0UWmC/kY9YlhZVJFi+wwHVabBIYcMhfANWbhNKpm2Rt/bDN02O3aYzarnh8d/XYo12w9g4cY9WKHrHV26yVvmD2CeWeKV1cxkk1dswwZddq9KYMAsA87Ld2PQE/ZD+k59eUrCA7KXvac2FMmOU8Huh39ejMvfn+lbDTuV89LE5dtw5JPjrGfzQiCIe+yzEy0f9+r2r+eHPke7Yy5gX4zXyvdz/wkdRcw6MwD74XqJiA25qUkmIVE5Jyvib24zKoHwd/KDaWtD03qreN+HjiwrXmqzmX0+w182n2TFq1JDVkvotYM//88w06W+SYeKSjB/wx7c+8Mi3Pb1fNtZzjzz6Th45JPjkxYk+WnePxHnCZX6Z8bPfniwYPn+w8XKNbWs1mW5nKu1lk0qwaGNAJrq/t0k+DcrnwM43c1zpZRvSil7Syl7161bV6FJqU95WJlNL00i2F0IeAlcWRXvtLrJfmX8KuVjmlZvyZj2r7GbnSVUv8LmxRLdOeTlJKi03gS+ES/px35zc9OhOkRE4+6CDTGdNRZs2IOPpq/Fo6MWOxY69dsNhlmAfpr3D96ctDrixtqYZbHvcDGOe26S515k45AsIzc3fdo+4DYLzOziQXV/cso6Sha7zEGtzWb1qMx6Ypds2otv5sQ+PXtU1hnsMxQA/zpO7IZLm33UWTYzmOl52e0f/2UJTnlpSqhIqFUbVDnVg3LD7O2USomL35mBQU+Mx47gud3r6cXLjKUqmahu6wclIpjk9BoqkwIA0YWGvVKprffgj4uwfuch0yGeRvHYhqrZ2saMH1XacX1mvvk1i+2wMk+v6M4qm6F+RqWlcGzUfz+f6zh828+OtX88ZtvH0gS7/dBuxj8vSiUiguQa7Zyqryd1ykuTo66LtCHmuw/aZ6S6onsNP7+SbjrZzBg7PlUZd+lY72XcDh9MtQzBZFK5CpoJoI0QooUQogICBaZ/1C8ghGij++fJALQz2o8AzhNC5AghWgBoA2AG0oBxJisrKnV+/HKwsCSqsOz/DNFuPbtaLH5SLbZ602d/e0551Ho67Mbr6w8LTsNi/MDjUPxmWXPja5ubC6ceXr2VW/fj14XepnYFAr0k936/EG9NXmNa6DSeRi0wb7c+mGZ1z6x6M230juI09So8NsGUahpySamM6NlaunlvXGoe+clsVpe/1ljP2PHJX2qZnUCgoKyVSSuig1HatptnMTuctmm93Ihe9cFMbPBYkDXbpMC9GS+ftN831bEcb4xNMYvF6f920ovBYSced/F4ZXjqZ8PSm2yyzwXakfxzTirz0luvF+vwwXhx+tTtgtGJuGn8j2GmQDtSSsfgOmA/yQwQv+9CLKsd4GICAI3fHa0vmgRlpZSoYHJuCJ+nwm964ca9UcfJeO9DK7fuxy8LNmHCstjvXWI9Vj8/1ltQW0rpep98xGHyGVevr7hcOpxBspwWkFIWCyFuBPAbgEwA70opFwkhHgIwS0r5I4AbhRDHAigCsAvApcHnLhJCfAlgMYBiADdIKVPzzOEz1SK2ibRh1yFcrjg7A4CIaddjZfd93+viBtxuLHp54HTQKW/XtU4zfCRb70fsp//WM9afKQ/01zNWJ+2sTG87pdssLjvJCDIaZ/Ab9vxkVMt1PKUmVamUyICA/jJo6spAcEggtmFxD9gUBDWjFb61mmpWC7R5uaa2KkivwuPurMRsP73ekLXnhlOtJzsqvbTfztEXPA1mDnm8EdMfPxIRQ7UaJuPHx6sVnS1PtI9EpSC1WY0wTVRNllTh8MHbBod8bkqs/tlToJSpYzWETlOWLyeLSqTjed/rccZsAgMJIDszOjgkZeBeKXrCDvMX9/PYp6+L98io6FpiXvnROe4lGCZlZGBKJcNSu37xg1lQ0EyqHQ/iQelKVkr5C4BfDH+7T/f7f22e+yiAR702sKxyOiink6d+W2paTDLVJDqlcNSCTTiuY33f1+v0NspbgOlvi8wD8pdVb5Ld7Ht2nOrHLHKYmlUvnnWn3HAT6E6GGfk78dBPi3FSl4ZRj705ebWnmZC8cjraall7fnZSGJkNUbbKRDHydPGbQpeVxiLJZjcz+np3APDD3I3o1bymp9dLlfNOKgwrS2V2tXdU+Hmzlkh+zy6VCpySixJdIy9eElITSkrT4NC7U9dg4+7o+xvj9Y22ra1Kb3jhJrPXjckrYq85tOeQ+8L/EjLi2OlHO9xQHe6bDlK7m7MMK5unkkhOhY1VqUx1mI6e+m1ZVJ2kNxxmpVOpwWA2G0l5ZjUsJdWVtQuzS94xHxFsN4WsnVKH4QtW04+b4VARNRe8FZhy3mzq50QGhgD1Gy6rGnN+MJtpSpWXc/xvi2Kb0SWeVD6O/34+F0e381YXMlXurxORZeg0tXqq0V93TInxhszNjIKJtNqhpo/9bGV+tyYxnI6xJrEOXzw9ZjmGdvC/49NIO+0n4uOREpYzd5rV8DMO+9M6E/LLQEe5H473UHQ7kDnEa7lUEKdDA5WHwlbpFtSRSK2e3XiJdSpfit1vizZ76llJJq8FJ60c8DMjhNcTriRwkkybNgQasdWnTgiKjeo+oVo4OFUlonfYatrlsuCj6WudFyqHPp5unYUR6+Hyw2n5Ma7BG6fvdDwDpXZDD/ViybYtCM4Wenpwhiojlc5UVXbb0uwhp9k4y7utNrNHWymVktdyKYKZQ3FSDmJDaWfkr0vTYjignydM8uYal7MolEdmvW1eOfUKU3K9NSm6WHVBUSkueGt6XDOD4qnnQ78nuwm+csrk0xQWeztHpkq9QJWZu4gixHg9PyZJGYNOndR+TuSQDMu3RE8tr/fZDP+GXd3wqXVtOLPtrFIwPBY3uShgXlZIpMYkNcTgUNwwNlT22M1aRURE3izeZF4/qqwGhgBg3+HUri/lluoU4au3xx6ITYcMXSo/Yt1fp6xMbO0UzbcOMw6XtaHtqcqsfuL01fEtzP7TvH/iuv5kWL3tQJmu11aelPG4ceoqD8PKiIiIiPw0e+2uZDeBSJnT5AllFe/DE2f2Oh7zyot0uL9ncChOUqGmAxEREVEqsavvEk9ep6BflyZFZCm9TF+9kzXfEuTJ0cucF6IyEbBMg9gQg0Pxkgb7DhEREVG5Fu/6IUTJsH3/YfR7/I9kN4Mo5NUJqT8RUjoMi2ZwKF7SIbRIREREVI4VFDE4ROUTb1WI3EmH7wyDQ3GSBvsOERERUbm262BhsptAREQpIB3KxjA4FCel6RBaJCIiIirHDhWWJLsJRESUAliQmjzjEHUiIiKisu3xX5ckuwlERJQCyn9oiMGhuEmHnYeIiIioPFu17UCym0BERCkgDRKHGByKl3RIOyMiIiIiIiIq79Lh/p7BISIiIiIiIiIiC+U/NMTgUNywIDURERERERFR2ZcOt/cMDsVJOuw8REREREREROVdOiR/MDgUJ+V/1yEiIiIiIiIq/9Lh/p7BoThJh4JVREREREREROVdOtzfMzgUJ2mw7xARERERERGVe+lwf8/gUJykw5hEIiIiIiIiovJOpsHAMgaH4qT87zpERERERERE5V865H4wOBQn6bDzEBEREREREZV3pWlwf8/gUJykQ9oZERERERERUXnHgtTkWWlpsltARETknzN7Nk52E4iIiIiSovyHhhgcIiIqV+49pWOym0DlVPsGVZPdBCIiIqLkSIPoEINDcZIOaWdElHq6NK6e7CZQOSUgkt0EIiJKAxk83VAKSofZyBkcipOSNNh5iCj1ZPKoTnEieLFOREQJcEKnBsluAlGUdLi7521EnBwuZtEhIko8wTt4IiIiKsMymDpEKSgdcj8YHIqTw0XlOzh0Yb9myW4CEZnIZHCIiIgSYOJtQ5LdBCqnshgcohTEYWXk2eHikmQ3Ia6O7VA/2U2gNHViZ6Ya28lgcIhcuuW4tkrLZfJinYh0KmTxNoLiI53ON/85pnWym0CKyn9oiMGhuDmjR5NkNyGueP9JyVKxQmaym5BQ/VrUcrV8Bo/q5NI1R7VSWq5/y9pxbglR8jWolpvsJpDB0oeHJbsJlGDplDlUq3KFZDeBVDFziLyqXaV8f9HL/1eDUlW6zZiU7bLCNDOHyC3V3v90ulin9JWbzUtjVYm6T8rNTq9OIQKGtKuX7CYQRSlNgxtgngHjhFPZJ17z2pWS3QRKgFSOfeSPPBnjbjnK13Wqvt/GNSoC8D84dNkReb6uj6yd1TO1M05T7buXwyEtFAfpNJwlVrzSpXjJq1052U0gipIO9/e8soqTNNh3Us5dJ3VIdhMA8IYl3lL9ur1l3SpJff1Um8q+U6NqyW5CmXDdkFYpF3wxSrWZ8Cql2RDTsi47M7X2HysDWnH4pKqy8YlSWcQh8rHp2qS68rJvX9I7ji0pX9Lh9p5fvTixSzv77X+DE9cQRf87tk2ymxCzVLlIGdy2rufn1q+W42NLyqd0G1amSuvNcHMD3yevpu3jn17VL+aAxU0stKikXf2qyW6Co1T75qVasMqroe3TY/hEX5f105KlS+PqePLsrsluRsr79voj0LB6etZnqpqTlewmuNaiTtnKxEnmzKvJ6OR9yudjTiIyII/vqD450XuX9/H0Guf0Sq2Mag4rI8+kTWyxXYP43AQ8eZb3A8uRbbwHNPww+n9Hun6OcbhLKtwonNmzseessQv7NcPUO47xt0EpoknNilF/++Lq/krPvefkyIww7WPu0axGrM0qV7Tdzs0F1SsX9rR9vGfzmsr788dX9rN4JPnfy7Ii1beU22Ns4xoV4zqzZapvL1UpcOpKiNLSZLfAhTS4AYhVz2Y1U+K6Kxlev7hXspvgWrxjBasfO8nX9SVr37p+SCt8cc2AhL/uCT7PxOvmXsTr4a6ti06toz3WkHrqnG6enndhv2b4/oaBjss9fHpnV+vlsDLyLNH7Trcm1fGvPk09P18I4D9D45s99PDwTpaPVa7gvhemVGEjt6mX2CE+1ynO+mMmK0MgK9XGBPnE7BzfWvGzuXxgC9N1ndK1UazNijutDlAsVGsIaV8HNzWHnJZ1c23WuGZFLHzwhJjW4aR9nALrqSLV77Pc3lx0b1ojru8p1bcXALxygX0AFgBqVqqAsTfHnlHsd70zK16/hyVl6KLaroNPFTswyq+yOKQ1M0OgVd34ZQ9lZAhcP8T7NXDU+pJ0fL99WHvUTvDsYRLmnR2x1CFMxNE20efg8/o0Vc6IevSMLuja2H5o3endG6FlGcuoS4TyeSeaAhIdWfz6uiMS+npenNPbe/DKjHETmx0uyuIJvDwyGwrmtVdIe15ZiN73am4/bEuF6syH2s2Mm3H6Tp+A2yF8VUxS7etUiRwqaZZFpvfeZdapx50aqY+h91u13PgOIxAi9YdMum2fhIzzO0rt7QUAJ3Vx7g1u16AqWteLLfBZp0pOwuqdVfX4XSgLx2yNH01V6Snv2LAafrzRuXe7LJh3//G+rOfb6xN3PTusk7dsjbKzJ4dlCIEx/3cUVj56YtRjdwxr7/j8EzrVxyldG9ouU15mtHR7eTriROft51blCpl45l/esmYAJCRLIdGf9sizuuIHhWwgjdPn+Px5PVy3QSUxoaxjcChOhiS4hoDb6a6NOjZ0VzS2QTXrceaJirgbv6BmB4HaVRJbw0cIYNv+wx6fm7yTarzHJpu9NdVXNC53zeCW6NeiVsrP7AS4u8CwCoo0r6XWq1ESHAjt5ljgtM8JESiUHIu29SNvWJ0CZlVsbjztmqsFpt65NFBYUbUWRo1K2UrLzX/ghKR8T5Klkcn2c9s+KeP7nk7tZn+jAgC3Ht82fg1QoHJcv8KQHRmLx8/s4tu6/FbiY7GGE30YgmGXARVLS0ee2QWLHzoBp3YLZLeaBc01r1zYE9Vy7Y9BrzkM/00V1SuqHUud9GwWe6eKqmYeZ7ktS4FOTWaGQKZFhnqzWs7boV39qnjZKRPSxwN+Mq+J3Xy8nRpV8xww1zO+3/G3DYlpfW72UM/7c4I+o2+uG4D7TukIwN39Sjz2oTL41XeNwaE4ObpdPVTMLjtZK7nZma4iwB0aVsMv/4muE9SyTmXMvvc4/xpmw/j9NBsic/uwdglpi9689bsT/pqxSkZvj9djdoPqufjimgGomeC0Xy9inVbeTbrrocISAEBFF9lyzplDQP1quTi5i/NNuNXxLlEXeO9e1gfXDWmFpsGL3Mo5WUoFEF2Ny0/wVcEx7evh30e6Dxz4cZOWmSmiAoPJDl4ZAz1nKxSqvOHocEF0u4vK6XcO9d6wGGUE23Vmz8Yxr8tNDQivvH4N/Czk2Tsv9uLWL1/QE0e2qWP6mJf3qBVOzRAClSpk+darnlFOsjFSkddjupTxy0yPV5aq3fFP5die6HviRO31U0fEVuvzm+uOiDnrV8ro91uvqloH16f/Nq/3aLx+PM6ieHQs574jEjSzY6/mtXDFoMC1UPsGVV1NouTUkeD2k0uD2BCDQ/H0y3+jgyf5I08G4H5KV/2B+7vrj4gYqnHD0f6N8QWAowyzbS19eJjpch1dTlGdk5WBzo39m9Y66qRusknzaid6LGnZvIiL901fBZOeKgGBj67s6/hcY9vMTsLXHNXScT1+1P9xy01wyKzGhbT4u5kalQLBMjdBaafmuQnsNLDI1PFz17JbV16dSrhjWPvIZRQ2nZubg3jPUmHc3FcPbokeCexFj2gLBDoZjvFm32M7fsfSTjbUGVNZv34ftjvemO2/1wx2Pq746dl/dfdhLeGN0r1pDR/WF2A101hLFzVMUi0dv0JmhukNmBDCU82h0DOizlmKzwu9fuS/Y+1kiId4x6vG3zokvi8Qo1IJXGqYFMWLrAwRlYn/ZpymFY91P1I63sb0CoZ1JWi3118bavdTbo5VudmZrtuqWnNThdUsdI+d2QXPn9s99O+L+zfH/Aeih35mZHgPeLidZbVl3coxJ04IIfBfFzVyX7vIvnh8NZedaSl2GosLBofiqEWdypYHjMm3u4tU61fTpn7ViEyP9g38Cbh0bRKo53FR/+YRf89180W2OUAKIfDTjYPwf8f6k+avUnOoLHFzcjnZYdy3HS1AmUiXDcyL+ltmpog5ffyyI/KQmSFw+wnO472bK6aPmxVUNOtxUwlsXdS/mdJrWrELXNwbTLHVfH51fzx9TjfHYWX6ixLHYWUKbXQrlnW6vQBLxM3owNaRPWcvnNc9hrWJqH+l0nHNOBzB+N7jzWlb1HGozzWgpXp7szIEbjsh/pmn73uc3jeRnjmnW8R1gNebtlQLDgHm7yU3O9PTDUDUNYmH7dSoei5ysyKvuZI1T0Wii/LqJWradbeZrdriUvpTT23lYyfhzpMir1/6+pAVZ8Yuc0gl2OdHkXY3/AyKqk5Oo2VexvtQZdzerq7/DZncVllLlSpkYUi7cGe/EEC13GxUyMyIGlJr9n67NXGu8eh2M427ZQheuyj2YbJ+ZqR3NhStvmqQfbZ2Kp7H/MbgUJw11PUI6L/QVr3sVvRfBAHg46ucb0zdGtqhPqaOOMYy9VCF1kqr760QAv81SQdU+Z4bI9TG+gXGcf0dGlZLQo9b/A4a8Rz6ZXVyedWnWge5WZlRPcyVFHtbjCcB/T/vP7Uj5t9/vK+1YFQ/QZWsNDdZH27TknOyIg/fTWtVUhpmo9+PnDOHXDUpbuvwSiXTR+Xz7meRNQEAn1zVP/T7xNuGoElN9zUshndvhBNMCqMKITxvv+HdY5vNz+x1swwZrx9e0Q8z7h6Ki4MdCsavoYT09SLb2CbjuotKIv9gHJ7h5oIyQ4i4nT/u1BUvHeJxel8r+m0y2JAF7NWRbepYHp1Ut9DJXRqixGQq+x9uGIgze7gbTvfi+T1cLW9FQpreFOdkZXg6k2s3z1GrFMDk24/GjLuih28IALV0QZjqlaIDMk774SUDmts+7lXb+lUx7z5/ikx74Wa/0Oo7xVPLOpXxYrCAbat6VVwfm+86Sa1osdkwQrejDczYXyf5c6xLwSQ3AMBzugwaO6EJT1yu35hl65aU6q/5iuG6XHWba8eR5Y+eiCfO6hp+vsVn/8ONgxRb5I7KedhquG8iZDp81/yod5fqGBxKoJqVvdeB0O+qQiCmmU2Mvav6gqxuh940rRVYXhtuoH3pv752gOf2WalguBk23viZ1SCI5UR1huKFiT4CL6W7YX76qL6TGpWysfqxk0xnmnBr8u1HR6SbWuniMA2kqowM4MMr+hr+JjyN0478LghUtij2Of7WIcjzWGzS7jU1ftc8shpWZsXrvl23anhIqmPNoThc6Tmt0y6Y4GZ/kVKq9fAoLHLJgDyl12xeO5wtqtLrpnnhvB7Izc6MCgB73fxSSl9myjPKNkyFlyECdRG076Cx3o2U7gMUVsub3VhJSPx04yDcPqwdTu7aEDcd0zri8ZiGZ4j43OjMf+B4X4ajOGlcoyL+N7QNvr9hIK4ODo/LzfZ4yecQlNPY3Xwe17G+aSZkt6Y18KzijZvGz4wWs2NK4xoVHVMH3rzYeqiCdozTr7tprUqoZzGRR/WK2Vj68DA8eFonvHtZ9D7rFBxyyoqYcsfRto9byciA5UkiEX3nR7m4PvIhduKoVEqc2q0R8keeHDULp4qrB3svAXF2r9hn+81MwLCyWBiP307NnXSbt/3ajvaSbrNDujapge8VZtGaeNsQvG6ROeO1/pV9yE8oLafyjW5Wq5KvpUHseNkU2v1o7C9u/3CiZgVNJgaHEsjNjY0xCq2/OPByQ32ubhr5P24ZEvFYpRjGf54UzIZ64LROwbYFeDlxuuWU4ioQW1pqvWpq72H0/waHLs5KJZCTpb49uzaujntO7gDA+XMdcWIHZFjMNKFRzS5qWqsSTlcMfl1q0it5zVEtQ7NCqRAQERkVY28+KvB3Dx+Pl4DF51f3d14oSH9Ssgve2c1A40Ujk8Cs3xdj5/dtFprxAUhMsWjjfh2vos7a6+jfUlmbUeaW49vi/L7NQsd/oft/t2J960K3jma1KuGKgS2iggvhXlYZ8e9QGwBc2M/d0Eqrdzu4bV2TfQno0qQ6rh/SGq9c0DPqvKO6DdwMN4vFbSe0c5yZKlbaW25QPRcZGQLdm9ZA05qBY8sZPbzN8Gh2bnopmL2j/8zteueFiM4888rqc/3hhoG43GQIsx2zQ2DnxtUdb5VMs7+DT3LKoDaTm52JS4/IQ8Pq0ecBp8xYp7Z6yWYMvG5GymaC+MXN2zN2SHq5Fv/03/3wzXVHuH6eH59DrMPKzMy+59iIf7vdJiN1syvecry7YbxeZpqzm6FQz8u1g3EiiKPa1sXk2yMDWM1rV8awzg1NOzU9n7J1m/yb62w65i1HdKi9TK3KFZCV4W/YoElN84COl6Fbfl3ula2rxvhgcChFGYMa1SqGDyReThL6A5Ff040CwK3Ht8MnV/UL9VJrbYu1cr8ZpyEFZsv73Yrxtw7Bb/8bHPX3DN1NUrwi61ZDho42jCn2wu55Dw7vHPW3O0/sgKEdzIcf9m9ZC90MhVCN66+vGHiLlbaLJLN2gqrqFbORP/LkiKK/fo/xf/zMLmijy+5IxHW/EEDPZjV8W5ee/juRY5IV0Vxh6J/VFm5Tr0oofdjT98rDk2pUqoDHz+wSypIUMWSv+BEYqxkc4jKscwPcd2pH62Bi8KXMbjD8DEBGnQOimiFt/20mf+TJ+MwkeJwhwm1vpDAM3GriBj1t5rRE3HBHvETwBTM87k9RkwIIoJ3uJkvLPq5XNSciM1GvQmYGXrvQvjCoG1ZZSPef2snFOvz9LPoHg4zGDDrbnn2F13fq5CqNU6V8P7NxVGu+eOXXcaaqTadPH0N2upeXPKJVHd+yOjs2dHetaTfrncr2M9vLasfYEWyXHRbPc4fTcnanz8oKs9Q1rJ6LD67oG5o91ei1C3thxIntQ/VtMgRQpUKW8iQCV1rUxenV3HoIvP6+TD/Tnt2meersrrj/1I6mjy168AT7RlrI1u2HZ/U0v6/xOzjkV1mOoe39HQaeqhgcSlHGL2NOVqblTDGtFFLcBre1Hr8ZywE4OzMDA1vX8XwT++5lvXHfKR3RqHpF19P4Ot38dGpULaYLP7MAV6MauREXxaFlg4uWlgLHtDcPmvxgknKa4aL+i1493QW4vjBxPIJybtW1mP1Fr2qw99zuPV97VCv8ajLjn+o7FAifYBI1nXo8WO3mVW0yEJrWqogjWtV2LD6eqJpc+no6dp+FU+Ba/9QjWtXGnSe2x796N8GNR7c2zcjo0LAapo44Bid0sq6jZqxdpvk9mN2WDFcNCgwDalW3SsT+7maWj1hvF4UQGNi6Nl67sCdudejR1V7LuD+Z7btO12hWu0drk/OcYwAsho2gHUtfv6gnvrneuaffzcQNKsdprwV57TaJNHn8HIU6ZfbDFoC2wWHuVstd1L8Zju/UwPJGyS23hy27STCsjkdu7k2eP7c7Pr+6P87p3QQz7h6KLi6GlJo51lD3sarD1OYqTX3Jpk6TVceT3fnBafsYO4jKCrvj/vl9I4d2JeLsqQ3tN3stt9OIxxrsM37mxu1hx8vsc16b+7guG0lTNxjEUh2qaLZ7f3xlP8y7//ioEhdmnNreoHourj2qFf5zbBtcdkQezuvbDBkZQmloWvRrWb+aPsiiDya2MRkCrvef4Gxg5/Ruiq5NagRex/AyViUdnOoF9ddl6lodRvxO+h79v8Ho31Kt0LuUMiKYpL+Ofuey1J9Awg8MDiWQmwCKsX7OdUNaWR5tVKaUt5qG1g27CxTti2w2tMPOMe3r44pBLZCRIVxP4+t0kH9oeOeEBwWMn7H+IKm/WNJmscrOzPB0ELxjWLj+RsR79Jo55O1plloY0n2t1m93UuvetAY6mPSMqX6k+huhWHeDVIwtndLFesa6ybcfg0//7TyULtXelzFwYFf479N/90ftKjl48uxuuNVmVil9HbXXLuyJk7pErrO41KRKrg9i2bQnd22I/JEno0alChHf70mGFHU7Xi+unjgrfGEthMCJXRpGXAybTSErQ0HYqEeilnUKSFo9ajac1uktxnJ9qTVzWOeGpkN9YqHyvfvxRvubhFWPnWT6d23olv7C3bZn+JxujkFks8Lo+v1Lf94z2+/+O7Str5MGuHXVkea97BLW28ZN5t3pPRqjf8vaEEKgnkLnCGBf3/GZc7ph2p3hGW2dAo8qiUN2x9LHzoi+mQbsM02c/HDDwIjJTfw418y7P/7FsRtUz0XrelVMs12Tcb48p3cTy9e+48T2roIJdt9BlbdmvL51c31tFez2q0Ozb14tzLrnWMx/4Hic3zd6KHO9armYducxUTPbzrg7skh8aBSA4Tv1440DMahNHeWRF6rbplpuNh44rZO7WaER+XnZvZT2NmpWykb1SuZtN2vroNbmAR4/YjYZGQL/M5mYSHNx/+Yxv46x7EPrelXwr95qwUwpgS+uUS9FUR4xOFQGVKqQGTW9vFtWB+BjO9THSxdY9yhppt85FFNuP8bycb9uwu38+8iWod8XPniCY/0E48G2bX13ac1m78VqOwqLE8qdJ3YwXV6f5WCc4eQUhWnqc7MzQ1Oz+7HJ2yqOw1YZXiEAPHZmF/yrd/jz0U64z53bLWJ6aLv9xeo6xumk20zXO619HlY3pFcMNL9pSIZKFUzGoNucIWO5cHfLmOLrNNWnHWOrj2pbF12bVEeLOpXx+JldIm7Kom/Knd9z4xqVkJ0pTLNdhHCe4QoIvz+/e6/cZP5o9O/YzUfudbpVrWPC6qVqmFxgho//KplDzm/Cqpc5OkBh/x61bTDj7qGuivJmZQjcc7J5Kr0fVD5Gu8xAOz2a1sBtJ7TDM//qFvWY12FUqsOiymKGpsNISUsqXy/b81vw6tvsmqJCVobh2Gf/YiqBrKzMDMsZr6wyIWItYOw34425Vb2SWI29+Sh8e3100CVq/07g9jHbT7IzMyKGIf3HJHCvZ1dnUuWtmGVvelmP6vIqGTqaL67pjzpVcmxrujWsXhGZGQKLHwoPhzIGc7XmtKlXJWIWQC17Bgh/G71md/pBtU843HETW2AwbkyOXV2aVFc6puln/YxaRwyT6UgEhvinMwaH4ky/e2fbFBIGIguz6Wkn6Bo+1gp64bzueOG87nj70t7oqTDddoPquZZRZztuZz+z0rNZjYipSrWo8Nibo+v/WHHb+2t2wLQ6vqretH1z3RH4wqI4srZu1ZsCP29c37usDz65qp/teHsA+PV/g6MK7JmpVCELx+jG5mrv7YweTUJ1NwCnmw5vpyz9ZxE6Mbp4vt91flT837FtQxe+Tr3wZmp5rKkkRPQFttmMRvee0hHXDA4HZ+85xb8b5w+u6IsfbxyE8bcOwbDOzoFRJxUrZGLFoyfhRF1mlZvvyl0ntY96f8b9x206PxCYre93F8crM4m4+XYcqWXyuPYnY+vMVuU0Y5aE9U2fU/DJ6t/1qua6Ksq78rGTcIFNIe2HT4+uw+aG9j5iie9aZmMKgRuObh1RnNvvzMkezWraHKf8P376WSdRT0oZuuk2FrL24/zq1/c1TiWFHNWpqnZe+VfvJujhoa6c02ytTjd4Y28+Cg8aJkPxkzGYZnyNwQ7DZ07t1si0nEC83Hyc9fBJADilayPLx5zuT366cZDl8MNY2H1udark4GSTztLnzu2GzwyZ0W6+a2YdcUYZGQIPmdTb1Pvkqn74665w5pFfx6mZdx/r2JmhWhLA6txstZzGcvWGA6NZEMdYm8uMXcaYiH4ZU9ccFTn7n9s6i+f1Mc8kKmPzmMQFg0MJckrXho4HbqsLAO1L+tW1A/DQcPfph2aGd2+M4d3d1fixYzVbTby1DtY6MOvNNvLak64iVHPI4TV6Na+Jfi1r+3r5rN/kXm82alSqgIGt62DMzYNNg1fjbjkKn17VD9UrZnuqG2F3I2PFj8QYfS2Uu07qEDVMLYkjHULyR56M/+pSbL3sphNuG+LptQUE+gZP5NrNUYs60b2DjWpUxJ0nmWfBufFvi+EdKro3reFDR21sK7j/1I6oVzVHacie8WMc3Laup1mD/Ph+uxMaI2z3aOTfgn9s7jB7zG0ntMMwm+Et2rpU36bTVyWZQ5nsaK2KpXfTm9jPPCNObI/b9dmf+qmS47S533YxM6ZRyzqVI4qvGmm7SHPDec3PIYtm20XlOP/eZX3wzDndHJe1uu549cKeuPX48HWnm3PLg6d1wt0nqXUCPHl2N3xnkmnjJNdhZlenKaNzszMjhk86DY80ZbPPjjBkfhuvV5xqSz1/bveYay95zcp/4bzueOrsrqHZIh+wKCqsaeZwXdelSXWl63u7JW44ulX0Hx1WeVq36IDWUW3rYYBDB83bl/R2NUttqDkK21n7TCpmZ6J+tXDmUa3KFfDlNTazhSmqWzXH+VpBMWMzXHfTZlUO7zknmMGlEvx65YKejkFfPdPrCcT3fg0IHCtGntU19O9vrhuAz/7dH8d2qB8o45LmlIJDQohhQohlQoiVQogRJo/fLIRYLISYL4T4QwjRXPdYiRBibvC/H/1sfFly50kdlDNCjmob+cXSho80r10ZlwzI8/T68Y7ZSPv7CU+eP7d7uBifzRv46cZBGPN/kT3ybmssvXBe94j6QL2a1zSt5WTVCqtxyirsnjPqP4PcrzAGDatXRD+TaZ1b1q2CIyzGIKuw+vzs9hevaZ3axWKGiLyw6ty4elSBa9Nm6WtpmHw2jarneiquaObve4+L+pv+pKjaU+d1euwMARQHo9L6nkO7jD9taNTJNjWPrMQSPG5bP1yc+T6XmUv6j9H4mfZ2MXPM5QNbYMbdxzovaMNt1lFk+rh6b+HJXRuibtUcPHl2V8flTV/XxbJa54A+9R6I7sm74ejWSj2exmW0i9KozCSH4228zntNYxzOkpEh8M11R+DDK/pFPTbixPb4zzGtTZ4Vyc17iwjgqD/N9Plt61eJqv8U747Wyoaefjfn2XG3DoEQwjLjVTsmGTvnGioMoXaisq3tPsej29fDWb2aOGazmm2Pk7s0xEldGuLGY+yHGlm5uH9zVFSYlcnIarY6s7+3qJuYYTl9bTIZKmVbZ5EYR+EZY81Ox2M/Dj9esp+BQNHfc3o3VZ6lycux0u317gX93JfH6JtXK6reqV2wV3Nsx/oRhY+ddFKo3Wpkts38OG4ovbbiMV1bzqp4NGBeokAfLOzUqBruP7VjRF3Y9ywKM+s/q5Y2w+7O7t0EjarnWtYBOiYOs4LZ7a+9mtfCgFa18falvS2PYenEMTgkhMgE8AqAEwF0BHC+EMJ4Zf43gN5Syq4AvgbwpO6xQ1LK7sH/TvOp3WWG3ZfWmKGhXQA0MtyYpdq4bxV+NPnYjvUtp1DU69KketS44U+vir7otjO8e+OIjJgG1XNNU3Cdghzajf0Jnerj4eHqU+pGrjt8BOvUyLpnSkYs5/7EpvGSDm4nvImce5TN/t6vRS28dUnviOle3fQIvnVJb9wxrD2a1aqky2izXt5pJhgg8qRSu0qO6VjzHIfx8cZCyABQ02Q4mPZSs+45Fv87tk1cb7yEEKHZurQbcgHg+xsG4qtro3vAvri6P/645SjPryfdpIUE3XNydMaSVe0M69cN/BQiurf3/Sv6YpzFe4rHEEPXNRkiugjVn9ewekXMvPtY5SKMGudhZYEF9LNcmXUOCAFcPTjQA/fe5X0iCl3brh/R20ibndGp5pCx7WaBKLfTP5sZ0s7+wlWbcMBOr+Y1TYdqX3tUK9xsMTuc2XfBDc+dsUKtrkW8ErVyDEMRVWed0Wtaq1JUz7d+XzNumhM7N8CHV/R1/TpmzDZLj+BwfpUAjNPndnavJsjOFOjdvGaoGK8+o8ELr9dwU+44GkseGhb195fO7xnx7xEntnfMprej7QNSoWfS7r1cc1TLUDucyiAYg0HG9c68+1gse2SY5eNeqH5ljTOHuf2ueysMHf0iMZ3fAPTJC3wvtNmLa1augAUPnIB3L+uN587thqUPD4sYPTHtzmMiird7Fb7vcn4DbocvxZvdNq9bNQd3DGuPj0w6IjTZGZET49SrmhNx/BBC4PKBLVCrcoVwgWuLUgb6LaMPSE26LTI437hGRfx551DzkQgSuH6IcweJ3WvbXbu5Gf6XzLpSyaSSOdQXwEop5WopZSGAzwEM1y8gpRwvpTwY/Od0AP4PTC2j7A4hbS2mEjR+0b0WntWnE3ulGuAwtt2vY6eWWmk304YZs5lttEDLlYNa4JULekY9ri8+Z9W7bflJaJlDwX++cXFvXOwxy0uV1puaIQQuOyLwWm5P8H/dNRSfXuVvVX6zFrgZVpaTnRkxy4lbjWpUxHVDWkEIYVkoV9OsdmX88p8j8cbFvUJ/87rrGr/PRqrDQbU252Znxn2YpkB4ti59D2Pdqjmm48b7tawdFby20qlRNVw/pFXU+3Yqymzc/lp9MYHoGZPcEgCuO6pVRAZZlZwsx+ELKq+rFZL3/ROLSB/3e+XAu5d5G7Kjz4Q1m61szeMnh1L/j25XD+f2cQ6YaOtS3e+9DCv78toBmOhxGKYV403lA6e66xhQdWnwOO+W1eZUrVXmNCwq/Kvw7dyv10r3/TyuY31k6Wb51M7bXmdk1a4LWhmyWIQQGNzWeniEyg2i3W781Nld8fNNgyJqQ1m/lvVjj57RGbWr5GDFoyfh6+uOQN8WgZvrWHu/te9g5QpZrm6mcrIyQwEv7e0/c063iH1txIntcc3glo51boBAkMCs4+XtSyMzF2xrmNh8DrnZmfjP0DZY9siwqIxg43HI6bBUt2oOcnRD5eyOY24zTOzWlT/yZDx+ZmSGqHZzrPJ1vPm4tmhaKz7FvfWqmNT6Mb6tD67oi0m3HR01e/Ex7evjjB5Noq4nGlav6Gk2SeM9hZvhe+E6PsnruHdzLXDdkFZoZjPkOztTRARTVIZja0sYM5Ksjot2r2/Gy32vPps9K8P62DLURVZS2UvN8IdKcKgxgPW6f28I/s3KlQB+1f07VwgxSwgxXQhxuvsmlg9mO1j1itkRM1NZHcSTWTbBbYDDz5sXgUAGz8IHT8CVMcyOdH7fZnjv8j6httWqXMG0yF37BtXwwnndQ6/thvYZRRVEtfhUtUyf9g2qeg5GvHNZH9x5YvuI4q3GHlYn9avlmvZc+lVIXKNaPA/w92AcrjkU/lvLOpUxsHVtfHxlP1zUrxma1qqEEzqFLxTMhktZNf/Ubo1wdLu6ePPiXvjAoZfZ7YVEIjIGhUAoc8jv+iynd2+M24dFziYhhAgMm3GYVcWJ++9M+BkZGSKq9pSelwu+/JEn4+VgwNnve2Nh8btfjmnvPRCrCV8sOzPbre2G2lXNyQrtR8bPxnFYmcnfquRkoXnt2HsD9ZlTjxsmk0hE7T0vr2HcXqq1ymyHLQirQJEzpzonKrIzM7Dy0RPxuUodMJNMs7N6Nsbv/zfYMRvMC7tjSW52Jjor1pwyXkfcoTuuGj/T07s3xgvndTet73bfqR1dH+czMwTm3X98TMV2jTeQ1x7VynL/nWAI0Dz7r+5Y9siJUcsZp6q2o3JMz8nKtJwd6+h2dXHN4Ja+ZB1qfrhxoFJNHK0TUCXD2Y3fdaUY/jO0DYQQuElhOKsTu+OSWaakcelKFbJcBxK8eO2iXoaMdA/D95J4b9ZecYZhFW6OCcbjTW52JpY9MixUliOW659zDGUU3r6kt/I++dm/++POkzqE7uHczHRH0XzdekKIiwD0BvCU7s/NpZS9AVwA4HkhRFSlJyHE1cEA0qxt27b52aSUlpEh8PIFPfHxlf3w3fVHhL51xq+pmxtrPf1BOu41h0IHVv9fqEpOVkwX24+f2QVHm1z4VTM52RoPfMZZaSyHR4XXoNSmU7o2wrhbjsLQDuEbM+OqT+9uPbMEEAjgXBO8yNK2z9Ht6uHm49qifYOq+OCKvt4KNML//cVVfQwfX/twUQmAyHo6424dgk+u6o9BbeqY7lfdmtZABYUeTQB46fweeO/yvji+UwPHXviWhp5pp94Lu+2gWkfAiRAiVGtDu0BQ3f5eh1xVzc22HU6gTzMWIvIbFevxpWxOtR19dIn1bVzc37n2g9W2MuthdcrQM3u+ZvqdQ/H+5eaB1ZqVsrHgwRNCmYRRw8oM+6Bxj4zn5/3UOd1Cv2c5zGrkF6/rtXqe6nFOCBEartSxYSCgYRaYc7O5f/nPkZ5mctI6LvSvlZWZ4TnDWgiBNg5Zn2Yk1G/YY90Pjdv6uiGtQrPpme3zw7s3Ns2evmRAHlY9dpLta31+dX+lYZF+MRZlz4vDMI4+HrPKtI+tSc1KuPOkDp73MTP1quYq1cQ5q1cT3HNyB9xwdGvcc3IHvH2JWqan02i7VibZsrdYDGc1euwMtSHCKlLtnOxXe/zsbPvoyr549IzIexF9oW4BgX4tauGETt46evRZ9urPCf+ek5WJkWd1xVk9m2BgK++1SY2O7VhfeZ+sVjELmRnCNmjs5i3WCtY8dZpAo7xSuTLYCEA/mLVJ8G8RhBDHArgbwGlSysPa36WUG4M/VwOYAKCH8blSyjellL2llL3r1lWvcl6W2O2Ug9rUCU4NGxA1rMzjwcpNMbZYaVNDJiLi75Xx4Gd2EtDq75zeI3DgbVUnOtXcjPZ3NwdYbSiLliGiXXho6+jbwv3nJxDoBRr9v8FRhc0TTZ/Wnojzv1m20xk9AkmOtRSKW186oHlUAT0/s0CuNUy76ZQNZ3dxMXVE7GPsNY+e0RkX9GuGQcGeH78+K5X1GKePBgKfo74grzbsQp8h5/ZCxutQF7vnvXFxL9PCwW4238DWtU1rUdm5+bi2+OnGcKF60zo0Du/XbCp27fuquqn071OltpeVGpWyTXv5XjivO3680aEgvzFT0/CBJSzrNviyb1zcCw+c2jEhxzsv9MG0ESe2Vx7qKgAc36kB8keejAbB4TD1qwX2l1O7NTIUfFfbgzo2qmZZt8LOiBPbB1/H9VNDwp9PbEf4P24+yjbA5dd+YBqI82fVUfq3rI1HTvfv5j/Aejv/dNMg2+F73tccMP7WIfhfjJmqKvSTSxzbwb8stMwMgauObInc7ExcdWRLHOtyyP1xHQPnF2OAzJd6SCYbXzUbLtVowwGNMdVuZjPS2ex0TWpWxDWDW+K9y80LNntxZJu6UUEXIUREB/cX1wzAGxe7GyKuXR+7YZUI0LhGRTzzr26m5/LaLo/z8aj1GEFh369ZuQLm3Xc8blUMTpU3Kt0eMwG0EUK0QCAodB4CWUAhQogeAN4AMExKuVX395oADkopDwsh6gAYiMhi1aQTjvS7G+dspbtuCk23Pe6j/jMIew4WKS/ful4VvHVJb9cz8dgxG4/+/uV9kL/9QEzrtduezWtXjsi2UT1EtalXBbPX7lKekU7vov7NsWHXQdxwtPeUXu0tpUKZPC1Q1qt5TeRkZeBwcamr3hirJV+/qBe+nr3e4tHAReY/uw9F/O3+UzthxIlqPX4PDo++YTbj9cTltifJLigca7FRvYbVK+KxM7pgwYY9vq1ThV1WWzXdEIZjO9TDW5f0xtHt6uKhnxcDcF8Q0s2wJz1tKEWOyU30CZ0aRAxHjH5R8zbqj8WfKNT7ytbtN1IiakjeyV0b4pFRSxzXY/TNdUdEZD5MvG0IikoktuwtCLbTnNn+r++lfvSMzlETBOh1a1oDn8+0/h5r6xrePfrC1dgmp73AbvhgPNjuDx6MvfkoHPvsxNC/7Y6jdodYs8cudTFk3Oz5tavkYMlDw5CbnYExi7cEllNeo3fGgJbZaz51dlfc9vV8y3X8q1dTfDFrPepWie04Wq9aLuopHItj3S7671zULEEpVhxXT7UG5cBWtTFpucdRA4ZMxkbVc7H9QCEKiwO19KwKytapkoORZ6oWyTd/A1kZAi3rVsZ/h7aJCHS+cmFPV9fP8aC1eFCbOpbn2suOyMOp3bzMOmr92FFt66JGpWzsVnz/qRJHf2h4JzSrXQlHtY0M7H10VT9s3BV5Xdk7rybGL9tmmnkphMCdJ0V22My4eyj6PvpHTO0zrfsW0xoDtcCeOCt6NtNXLoyuyRp6TRe1mQDgnUt7K52HezarkXKdKmbDINOFY3BISlkshLgRwG8AMgG8K6VcJIR4CMAsKeWPCAwjqwLgq+DFy7rgzGQdALwhhChFIEtppJRycZzeS0pT2efNinoCMQwr8/SsALtZsqxYFRFuUacy2tSzL/hqtOCB400j0EPa1QM8BnK93NSrXnc9cFonnNy1Idp5GAdcsUKmcmDCSs3KgYNYvWqpNQVj3xa1MHnFdlf7otUN0LDODWxTPGtVrhA1tCsjQ3iajvfeUzviwR8XhW7MAzf0sV+E33JcWzzz+3KlZUMxgQRd/PvdW3NOL3ezZOnp37IQIjysKNZGKdDvfved2hHtGlTFEB+z8E7v0RhTVm5XXj4iA9THj0g/GyAQzv7cvKdA6fkRw8pCfxO40GG64vP6NEW/FrUw9NmJkNJl54dxWJnN9mhaq6JjZoxqQWa924clriextYvzpsphIvJ7pd4Oq86l0LFVt2JjM7Izhad6NcZaRkZ2x6tzeje1DQ5dO6QVHjmjs1JB5EsGNMfc9bsx30Pw3K/jlTZ190ldGuDo4HBkq1nW3Lj5uLZYu+Og84I6j53RBXd9tyCGV42vyXccAyklWt/9q+1yx7Sv65iF43QKFkJg3C1Dov6ek5WJetXcX3f0bFYjqhBzvAgh8MBp8Smc36BarnpwKEUCArWr5ETU8tJUy81GtYaRx69XLuyJNdsPKF9b1quaiy6Nq8c00Yotwza8bkgrzFm7y/FpGRkCFbQRC8G/ndy1IXo2q2n9JJf0pTOsLH14GDIzBB4Odv7ZFZN2Eg5Ip27QvCxQGjAtpfwFwC+Gv92n+/1Yi+f9CcDv/NQypUWdyti0p8BVcSzjsfKB05ync3dcZxIPwMYZIFR4ycBxcmaPJnhj4mqc2Dk4q5AP20Sb9Sw3OxNHtvF3GJfWvq+vHYBlW/bZLnt0u3p44bzuofeWTGabNVkFqb26uH9zXNy/Oa76YKav671paJtwcMjhjfo9Fn+5SVFP09f16RMw63Xx64Qdz9O+volVc7Nx1ZEtva3I4vM7u1cT3PrVPOXVZGQE0sf3FhTHP91ax2r3M/sIuzetgU//WqfUCSCEQMu6VULrMR4b7N5jNcN5wa7m0Mgzo3tE9V67sCe6mA0ZcOBlit1Y3HZCOzz12zIAsdQcCj9TOEVdzJ7v8MLhwv/RC6541L7GjZXTuzfGd39HVTCI4uUwKWCemWzmoWDHTd6IURF/V9qEPh3CW9eritcv6olBumsMP47TbiYG0LbzsM4NfA8O+XlUC2ToClTIyghlD5UVjWtWikvNpURzlymeCld87lSqkOW6A/2nmxyGSDsw3U4WXxyzIJf669iLR8xF68S57YR2qF4xG8Md6q16JUI/3e9z95/aET18DJqlOn/L31OU1y7shdnrdqpNVxr8aTyw+jGbDAHtGlSNSK9VyuZyuGwxm/Us4vkeDqTG5/TOq4XeJtOK62lFKP3y+kW98Pbk1crTllsptciGM9OtaQ3MW787pteLl5uOaa2c8ROrOlUqYPv+Qt/Xm6jZG54/tzvqxTyNstXfY7uQTGiQXErkjzw56qbSi0QW7XQ65pmdp87p1QT9WtTyNAtY1FAxm5fPzc7EikdPxOXvzcSUldstlz27VxMMbG1fGPNEk1kJU9ENR7cOBYdiFa/Qon6oQaI7bN28nl0Qyy03M3r68fUd5mPHz8Ond8aO/YedFzThLtHP/+PWhFuHYI2utEC4BkqkSbcdjU17DsFKooMSscz25kUiMid8eYmyFxtKCrvzcqpkX2m87hdVc7OVC1B7cUq3hpi1dhduPd56QhQrlw9sEYcWpS4Gh+KseqVs5eCOn8fywW3r+jqzQnmUarMkpJLOjavj+fN6xLweqzpaZm4Y0gpXfzQ7pU50WvvbN6yGH28ciNNenhr31/zhxkEJr/8D+Hf8Od2myKHqd86pLe4LUqs/IZX2P031itnYc6jI8w3NGT0a47TujXD5e86ZcFq9IDfBZiGE5+nhjTfqfRwC4dmZGTi1W0NMWbk9agZArc6e1/T9h4d3UurISRbP+6bJ86zWZTb01TlzyL+Ll3N6NUHlnCzsPRS/mi2xfsc/uKJvqDC3Wyd2buCpELcVL8dtldkKE8nNe8irU1kpu6ZB9Vzbz+i6IVETJ0fTJhpRbp25BQ8cr1Rz0M+ATjxjQ07vxF0ZgVha4qxX85qYrTDEKt2p7ntudqtkXEtp10hm7czJysTjinXG0h2DQynEj2N5+wZVsXTzvqgaGSl4v1MmlKdhq+9d1gcNa/hXyNiSvhZJaOiI89P6taiN+tVycNMx8Z9ZxK1Aorq3b9HPNw1SruMCBHqk3fRKe1ElJwt98sxTZN2e0Pu3rKUcSEj2sLKEXqz4+GKfXNUPfyzZYjFUz/n5z53bXfm1alWugKUPD0NOgjLNjJvpvwpDXf7VuynO7NkkamhQ2/pVseqxkzxPI3yxiyLNAFJq6IrK7ua0r+RkZeCmoW2wff9hfDBtbXjdDse+Lo2rY+HGvahRsULM3/GnzukGALj5i7lKyyfjBqSy6zp24Ua+dlEvX9qQ7CC21fmjLDimfb2EDt+KR5mEVFaaQhfOn1zVD3sLklscvCxR7cCzW6xhMCjr9wQNlDgMDqUQ7aIqlpTni/o3xz3fL0TTWqk7pXw8vXtZb4xbutV5QUXJPMX5fe2nFbJMpKuObIFpq3egvc1sBQ2r5+KKgS1QvVI2/rrLtHxZ0ug//zpVA729g1q7qy3VuXF10+ldkznl68IHT4j6m9t9Xbv+u7h/nuPwSr94PTQm5Xsc3EDHtK8X8zGpaa1KuMxFWvNFA2LLDLAr5KyfmSwW2hAk48WoSsarEALZmebLeQ0MedGrWU1MW70jYfuX3YW73f2Y2bOMAZ+xNw9G9YqBY5xxVU7fuwdO64R/9W6KZrUrpcSMmU6s3s/1Q1phy15vw62SJdGFVyWA3/9vMBq66MBwaqEfX1mnffTzq/vjwZ8WY8mmvbG/WJyUl2x2Y8D8KJsJHeL9jnOzMx0nJiirUvVYW69aLubdfzyq5cY/xNChYTUs2bS3XHXkpwIGh1JQLOeHC/s1Q4eG1aJmoikvJx0nx7SvrzyMr6LCCcPtLGtGrerG9vyySH/TMbRDfdspywFg2p1D492kmAkRmPJ96ohj0EBh6uKPr+yHRg5ZWsbiula0c97/HdsWz42Nf92jeB4pVI9DWuCsd3Pz3mnXU9m7GN7ot3eNU0/7ZMz/DcZWkxvZVY+d5MvNlpUL+jXD7LW7cPVgj4W6g366cRDGLtniU6tS0w1HKwxdSSD98C/jV7F1PeuZNp12p5yszIQX6/R0MxCqjWT+jm63KeR60zGt8dK4lcHnu3u9eNCa4NdLfHJVP2zYZT1zmf4tt6nvflZWOxcPaI7Hf13q6bmq+0H/lrVx6/FtceUHs5TPH6FtnIQbz4m3DcHqbQecFzTok1cTM/N3IcsieJ4IhSXh4ND0O4eihs2U4OlybxIrs30wNMN1Qtuh9mVIdI2tkODGYLAoNgwOpZCs4BW96iwaZoQQUYEhMvfhlX0x9JmJtss0qlExpqKyFStkom7VHGzbV7Z6IynAeCJUHe41qI19MdxjO7jP4ioP11CqFxYDWtXGjLuHhurfaFoGg61NanrMjFTYhoN9nLY+ntrWr4q29avin92RRVfjnTlTvWI23r60d8zrscqoKy+cguJ+szs+aDNpXtS/OX6Y+4/juoxfU1c3cAm+KPdyc+nlK3LL8e0wdeV2zFm32/Vz43Hs9vum2qmAezxVqhD7rUg8Av9axkkl18MIY9e8dmVPddxev6gXJq/YjobV4zs0HbCuNVakCw451eYqB5c15YLbjOCyOMscqWNwKIWc368ZNu0twE3HBKbKfeG87vhIN+4/Fn59jV84rzs6NbIeIlSW5HksoOrW+FuHuKpLkcjpqinxlj48zFUA+JIBeZixZicu6NcMzxoKxU6+/Wjf2pXo4QlOjIEhALioXzO0rVcF/VrWdrUuN++sbYy94uwJpVTToHpuKFilsncaCyYnZY+OS0BFW7W3lbseeuvpVVy+RmodtiOIMt6Lf3r3Rti0+xCuPLLszFRUu0qO7aQQbp3YuUHE7HCAc7CzUY2K2LL3MAYpBBx5ulQT7+2U6UNyQjJw94kPBodSSE5WJu48sUPo38O7N/Z1enI/pFp7YpGog0qVnCzAwwQ4PGmWT27Hv9etmoMvrhlg+lhcaovFcceLfSp64TowFPH8mF7dP+NvHYKDhcXJbga58MXV/ZPdBF8c2aYOxi/bZvtduOHoVqhXNQePjlqCQ0UlrtafqDjABf2aYcHGPbjuKPOhe69e2BM1KxmCXMHjT+yHOHcriOdxp4zGXXzj5v1rRai7BWc1dJKVGSjQno5+vHEgVm7djzN7Nol6TMvgtdqOdYMzPl7Uv5nSa3VqVA21fJzBrzwyC7Ae3b4efp6/yZeAzrDODXDN4JZqs/ghde9PUrRZZQ6DQ2XYp1f1w+SV222X6d28Jmat3ZWyX2SK1iFYvLmp16EzCTTxtiHYf7gYJ784JfS38rSv3XJ8O+TvOIg+Leyn105nrkacJKkLOdWyolokcKYc8odZUDKZdT2MVHfxVy/shc17C5Blc0ORk5WJi/o3R/+WtTBh2TalIuGJVjU3Gy9f0NPy8ZO6RBfJzwhlDnmTYocRgm44jMKH2qpuFYy/dQia1Iz/kKuy6rEzuqCgqARdm9RA1yY1TJfpk1cL4245KnQee/PiXhZBZLVv2qj/HOmxtemnee3wfcEz/+qGESe296XgdnZmBu48qYPzgpQWGBwqw45oXQdHOKRtfvLvfigoDA9palqLJ0VNqgYxLjsiD33yapWJehxexsSXJZ0bV8f4W4ckuxkJkQ73PfrMpVH/GYTalT2k9JVhFTIzIoqFUmyeOrsbXp+4Cv1jyGZT8c6lvSNuCmJRsUKmcnCydb2qtoWqzfgViNWGfrUOTgrx3mV9XGcxGWWIGKNDQarXDukeTErFaywG5u1d0E8t26elbrKV4w1Tlp/ctSHGLN6C9g2ijx13ndQej/3irfg4RcrJyvRef9GjVDumVQnOiGastZhizSxzGBwq53KyMpGTFYgqv3FxL3Sz6AmIl1S8OEh1QogyERii8ikVZitLhE6N4vsdS8bFSceG9vXg5t1/fIJakh4aVM/FA6d1ivvrDO3gPANnCn21PPnu+iOw51BR1N+vCc6Kd3R790X8jZK1jeJTkDrwM9WyIomGd2+Mk7s0NM1OvLh/Hiav2I7JK+xHPVBqS5Xzzcvn98A3czaGApHaLGnMDoxN2ao8RTE5oVMDx5kD0tHZvaLHVBMRlTUPDbcPVFSskImKSZh5h/xlNzV0qjimfT08eXZX5eV7NKuJIe1iDwDZ0yIq3p6dSmEYLVvNauhPvMQrGHVGjEWUU+Vm1YuWdQJZOH3yys9Mw1bDVitWyMRHV/ZDRR+GQqWTVDn2pNqEOfWq5eK6Ia1CHY89mtXEGxf3wn2ndExyy8o2Zg5R2hJCYOGDJ/Ak5bMyfI2W1tKhAzqR+ya/BxQPL53fA90VC+om04vn9whMxhAjPw9LIrbYUOggqfrdjueN1AmdGuDve4+LmlkuFalsh+fO7Y7nzu0e/8akoC5NqmPKHUejcQ1mO5C9VLmuSOWp7E8wDHMk95g5RGmtSk5W1FhVik1Z7sFLb8Ebn3JYTyMZbU3US5a1qWcpNqd2a2Q6S2G13NTo6/Nrv4/HeSQjNBQrtvW4HR4brxupVA8MJeIGMtUyGbxqUrOS8n511aAWaFnGayfddVJ7AAiVvaCyITMjcL3B6/zyLTWuJojKmAdO7RiaFpUolf104yDMzN+pvHx5POdLl4GvWCR6+9Wtml5FtSna/ad2xHEdnesSJUJo9qjkNsOUVpC6NEHR4rIUQHeSSvXi9LTacUe0sp+cpTy555SOuKeMD5u5eEAeLh6Ql+xmkEtvXNQLn/y1Fm3qVXFemMosBocoLioH08n7ltMpwC8b2CLZTVCWIYDTYxzPT2VXlybV0aUJC5wDierJTryezWpgzrrdSXhl/9SvloPmtRhw9+LyFDwf+RZL8PELpTXJa3DI7bNqBTN7/ntsG0+vl0qeOrsrnvptWajgqxva5s7NzkBBkb+zJXZvWgPz7jse1ctAHS6isq5Z7Uqc8j4NMDhEcVGrcgWMvXmwafo7Jdbqx09OdhOoDPDay61yDzioTR3gVyQtu6E89eCbSdVefTf+uuvYZDchbjo2rIadBwqT3YyESOVhPk//qxue/m0Z6lfzNjHHJQPycOtX89Bc8bomNzsT+SPLx/l3aIf6SrPmRTAclqbccQz2FRT716ggBoaovOOshJRIDA5R3LSuV9X2cbNilW3rV0Hb+vbPo9Rz+cA8vDc1H0BqF6ojZ6qBhgeHd0KV3Cwc08F5hqFOjaon9Sapd/Oa+HPVDjSobj8E67RujWJ+rWTs/bxwTG2//PfIZDch4VLxPHBEqzr49nrvw4/O7tWEs5vGoE6VHNSpwmGwRESpjMEhSopxtxxlmp485v+OSkJrKFb3n9oJbetXxZ3fLkh2U8ijzo2ro2ezGnjgVPvp0DX1q+Xi6XO6xblV/vjvsW1xWvdGtgHrFY+eiEwfMnC0qeLrVfWWnUBUlvkVp9S+iamciUT2LujbDKPmb0Kv5uVninYiovKOwSFKipZ1WcysvKkUvClminfZlJudiW+vH5jsZsRFZoZwzGT0a9avtvWr4rlzu+GY9okbQlcehpVR+XBy14b4ds5GZGVyn0x3A1vXKTfD6ohSAc/1lAgMDhGRL07t2gg79hfign7Nkt0UoqQ6oweHnlB6euKsrrjrpA6+BVuJiNId8ycpkXj2JlJwIQMejjIyBK4Y1AK52ZnJbgoRESVBdmaGL3Vl2jUIZPo1rsFJLYiIiBKFmUNECh49owsePaNLsptBRERU7l0xsAV6Nq+Jns1Yr4aI0hsHk1EiMXOIiIiIiFJGRoZgYIiICBxWRonFzCEiojLi30e2QLVcFvwmorLpxxsHoqSUtzpERKpysgK5HI1qcBZUij8Gh4iIyoi7T+6Y7CZQCpJ+zR9OFGddm9RIdhOIiMqUJjUr4aXze+DINnWS3RRKAwwOEREREZUxt53QDgNa1U52M4iIKM5O7dYo2U2gNMHgEBERURkmBMtVpqMbjm6d7CYQERFROcKC1ERERGVY++C03zUqsR4VEREREXnDzCEiIqIy7L5TO2J498ZoXa9qsptCRERERGUUM4eIiIjKsJysTPRtUSvZzSAiIiKiMozBISIiIiIiIiKiNMbgEBERERERERFRGmNwiIiIiIiIiIgojTE4RERERERERESUxhgcIiIiIiIiIiJKYwwOERERERERERGlMQaHiIiIiIiIiIjSGINDRERERERERERpjMEhIiIiIiIiIqI0xuAQEREREREREVEaY3CIiIiIiIiIiCiNMThERERERERERJTGGBwiIiIiIiIiIkpjDA4REREREREREaUxBoeIiIiIiIiIiNIYg0NERERERERERGmMwSEiIiIiIiIiojTG4BARERERERERURpjcIiIiIiIiIiIKI0xOERERERERERElMYYHCIiIiIiIiIiSmNKwSEhxDAhxDIhxEohxAiTx28WQiwWQswXQvwhhGiue+xSIcSK4H+X+tl4IiIiIiIiIiKKjWNwSAiRCeAVACcC6AjgfCFER8NifwPoLaXsCuBrAE8Gn1sLwP0A+gHoC+B+IURN/5pPRERERERERESxUMkc6gtgpZRytZSyEMDnAIbrF5BSjpdSHgz+czqAJsHfTwDwu5Ryp5RyF4DfAQzzp+lERERERERERBQrleBQYwDrdf/eEPyblSsB/OrmuUKIq4UQs4QQs7Zt26bQJCIiIiIiIiIi8oOvBamFEBcB6A3gKTfPk1K+KaXsLaXsXbduXT+bRERERERERERENlSCQxsBNNX9u0nwbxGEEMcCuBvAaVLKw26eS0REREREREREyaESHJoJoI0QooUQogKA8wD8qF9ACNEDwBsIBIa26h76DcDxQoiawULUxwf/RkREREREREREKSDLaQEpZbEQ4kYEgjqZAN6VUi4SQjwEYJaU8kcEhpFVAfCVEAIA1kkpT5NS7hRCPIxAgAkAHpJS7ozLOyEiIiIiIiIiIteElDLZbYjQu3dvOWvWrGQ3g4iIiIiIiIio3BBCzJZS9jZ7zNeC1EREREREREREVLYwOERERERERERElMYYHCIiIiIiIiIiSmMMDhERERERERERpTEGh4iIiIiIiIiI0hiDQ0REREREREREaYzBISIiIiIiIiKiNMbgEBERERERERFRGmNwiIiIiIiIiIgojTE4RERERERERESUxhgcIiIiIiIiIiJKYwwOERERERERERGlMQaHiIiIiIiIiIjSGINDRERERERERERpLCvZDSAiIiIiIiIiKioqwoYNG1BQUJDsppRpubm5aNKkCbKzs5Wfw+AQERERERERESXdhg0bULVqVeTl5UEIkezmlElSSuzYsQMbNmxAixYtlJ/HYWVERERERERElHQFBQWoXbs2A0MxEEKgdu3arrOvGBwiIiIiIiIiopTAwFDsvGxDBoeIiIiIiIiIiNIYg0NERERERERERD6bMGECTjnlFADAjz/+iJEjR1ouu3v3brz66quuX+OBBx7A008/7bmNGgaHiIiIiIiIiIgUlZSUuH7OaaedhhEjRlg+7jU45BcGh4iIiIiIiIiIAOTn56N9+/a48MIL0aFDB5x99tk4ePAg8vLycMcdd6Bnz5746quvMGbMGAwYMAA9e/bEOeecg/379wMARo8ejfbt26Nnz5749ttvQ+t9//33ceONNwIAtmzZgjPOOAPdunVDt27d8Oeff2LEiBFYtWoVunfvjttuuw0A8NRTT6FPnz7o2rUr7r///tC6Hn30UbRt2xaDBg3CsmXLfHnfnMqeiIiIiIiIiFLKgz8twuJ/9vq6zo6NquH+Uzs5Lrds2TK88847GDhwIK644opQRk/t2rUxZ84cbN++HWeeeSbGjh2LypUr44knnsCzzz6L22+/Hf/+978xbtw4tG7dGueee67p+v/zn//gqKOOwnfffYeSkhLs378fI0eOxMKFCzF37lwAwJgxY7BixQrMmDEDUkqcdtppmDRpEipXrozPP/8cc+fORXFxMXr27IlevXrFvG0YHCIiIiIiIiIiCmratCkGDhwIALjooovw4osvAkAo2DN9+nQsXrw4tExhYSEGDBiApUuXokWLFmjTpk3ouW+++WbU+seNG4cPP/wQAJCZmYnq1atj165dEcuMGTMGY8aMQY8ePQAA+/fvx4oVK7Bv3z6cccYZqFSpEoDAcDU/MDhERERERERERClFJcMnXoxTwWv/rly5MgBASonjjjsOn332WcRyWtaPH6SUuPPOO3HNNddE/P3555/37TX0WHOIiIiIiIiIiCho3bp1mDZtGgDg008/xaBBgyIe79+/P6ZOnYqVK1cCAA4cOIDly5ejffv2yM/Px6pVqwAgKnikGTp0KF577TUAgeLWe/bsQdWqVbFv377QMieccALefffdUC2jjRs3YuvWrRg8eDC+//57HDp0CPv27cNPP/3ky3tmcIiIiIiIiIiIKKhdu3Z45ZVX0KFDB+zatQvXXXddxON169bF+++/j/PPPx9du3YNDSnLzc3Fm2++iZNPPhk9e/ZEvXr1TNf/wgsvYPz48ejSpQt69eqFxYsXo3bt2hg4cCA6d+6M2267DccffzwuuOACDBgwAF26dMHZZ5+Nffv2oWfPnjj33HPRrVs3nHjiiejTp48v71lIKX1ZkV969+4tZ82alexmEBEREREREVECLVmyBB06dEhqG/Lz83HKKadg4cKFSW1HrMy2pRBitpSyt9nyzBwiIiIiIiIiIkpjDA4REREREREREQHIy8sr81lDXjA4RERERERERESUxhgcIiIiIiIiIiJKYwwOERERERERERGlMQaHiIiIiIiIiIjSGINDREREREREREQ+yMvLw/bt25PdDNcYHCIiIiIiIiIiMpBSorS0NNnNSAgGh4iIiIiIiIiIAOTn56Ndu3a45JJL0LlzZzz88MPo06cPunbtivvvvz+03Omnn45evXqhU6dOePPNN5PYYn9kJbsBREREREREREQRfh0BbF7g7zobdAFOHOm42IoVK/DBBx9g7969+PrrrzFjxgxIKXHaaadh0qRJGDx4MN59913UqlULhw4dQp8+fXDWWWehdu3a/rY3gZg5REREREREREQU1Lx5c/Tv3x9jxozBmDFj0KNHD/Ts2RNLly7FihUrAAAvvvgiunXrhv79+2P9+vWhv5dVzBwiIiIiIiIiotSikOETL5UrVwYQqDl055134pprrol4fMKECRg7diymTZuGSpUqYciQISgoKEhGU33DzCEiIiIiIiIiIoMTTjgB7777Lvbv3w8A2LhxI7Zu3Yo9e/agZs2aqFSpEpYuXYrp06cnuaWxY+YQEREREREREZHB8ccfjyVLlmDAgAEAgCpVquDjjz/GsGHD8Prrr6NDhw5o164d+vfvn+SWxk5IKZPdhgi9e/eWs2bNSnYziIiIiIiIiCiBlixZgg4dOiS7GeWC2bYUQsyWUvY2W57DyoiIiIiIiIiI0hiDQ0REREREREREaYzBISIiIiIiIiKiNMbgEBERERERERFRGmNwiIiIiIiIiIgojTE4RERERERERESUxhgcIiIiIiIiIqK0t3v3brz66qsAgAkTJuCUU07x/TUuu+wyfP3118rL5+fno3PnzqaPDRkyBLNmzfKlXQwOEREREREREVHa0weHVJWUlMSpNYmlFBwSQgwTQiwTQqwUQowweXywEGKOEKJYCHG24bESIcTc4H8/+tVwIiIiIiIiIiK/jBgxAqtWrUL37t1x2223Yf/+/Tj77LPRvn17XHjhhZBSAgDy8vJwxx13oGfPnvjqq68wZswYDBgwAD179sQ555yD/fv3h9bXsWNHdO3aFbfeemvodSZNmoQjjjgCLVu2DGURSSlx2223oXPnzujSpQu++OKLqPYdOnQI5513Hjp06IAzzjgDhw4d8u29ZzktIITIBPAKgOMAbAAwUwjxo5RysW6xdQAuA3Br9BpwSErZPfamEhEREREREVFa+N//gLlz/V1n9+7A889bPjxy5EgsXLgQc+fOxYQJEzB8+HAsWrQIjRo1wsCBAzF16lQMGjQIAFC7dm3MmTMH27dvx5lnnomxY8eicuXKeOKJJ/Dss8/ihhtuwHfffYelS5dCCIHdu3eHXmfTpk2YMmUKli5ditNOOw1nn302vv32W8ydOxfz5s3D9u3b0adPHwwePDiifa+99hoqVaqEJUuWYP78+ejZs6dvm0Ylc6gvgJVSytVSykIAnwMYrl9ASpkvpZwPoNS3lhERERERERERJUnfvn3RpEkTZGRkoHv37sjPzw89du655wIApk+fjsWLF2PgwIHo3r07PvjgA6xduxbVq1dHbm4urrzySnz77beoVKlS6Lmnn346MjIy0LFjR2zZsgUAMGXKFJx//vnIzMxE/fr1cdRRR2HmzJkR7Zk0aRIuuugiAEDXrl3RtWtX396rY+YQgMYA1uv+vQFAPxevkSuEmAWgGMBIKeX3Lp5LREREREREROnGJsMnUXJyckK/Z2Zmori4OPTvypUrAwgMBzvuuOPw2WefRT1/xowZ+OOPP/D111/j5Zdfxrhx46LWqw1VS7ZEFKRuLqXsDeACAM8LIVoZFxBCXC2EmCWEmLVt27YENImIiIiIiIiIKKxq1arYt2+fq+f0798fU6dOxcqVKwEABw4cwPLly7F//37s2bMHJ510Ep577jnMmzfPdj1HHnkkvvjiC5SUlGDbtm2YNGkS+vbtG7HM4MGD8emnnwIAFi5ciPnz57tqqx2VzKGNAJrq/t0k+DclUsqNwZ+rhRATAPQAsMqwzJsA3gSA3r17p0bYjIiIiIiIiIjSRu3atTFw4EB07twZFStWRP369R2fU7duXbz//vs4//zzcfjwYQDAI488gqpVq2L48OEoKCiAlBLPPvus7XrOOOMMTJs2Dd26dYMQAk8++SQaNGgQMZTtuuuuw+WXX44OHTqgQ4cO6NWrV0zvV084pTAJIbIALAcwFIGg0EwAF0gpF5ks+z6An6WUXwf/XRPAQSnlYSFEHQDTAAw3FLOO0Lt3bzlr1iyPb4eIiIiIiIiIyqIlS5agQ4cOyW5GuWC2LYUQs4Mju6I4DiuTUhYDuBHAbwCWAPhSSrlICPGQEOK04Av0EUJsAHAOgDeEEFrgqAOAWUKIeQDGI1BzyDIwREREREREREREiaUyrAxSyl8A/GL4232632ciMNzM+Lw/AXSJsY1ERERERERERBQniShITURERERERETkKFVm7yrLvGxDBoeIiIiIiIiIKOlyc3OxY8cOBohiIKXEjh07kJub6+p5SsPKiIiIiIiIiIjiqUmTJtiwYQO2bduW7KaUabm5uWjSJKryjy0Gh4iIiIiIiIgo6bKzs9GiRYtkNyMtcVgZEREREREREVEaY3CIiIiIiIiIiCiNMThERERERERERJTGGBwiIiIiIiIiIkpjDA4REREREREREaUxBoeIiIiIiIiIiNIYg0NERERERERERGmMwSEiIiIiIiIiojTG4BARERERERERURpjcIiIiIiIiIiIKI0xOERERERERERElMYYHCIiIiIiIiIiSmMMDhERERERERERpTEGh4iIiIiIiIiI0hiDQ0REREREREREaYzBISIiIiIiIiKiNMbgEBERERERERFRGmNwiIiIiIiIiIgojTE4RERERERERESUxhgcIiIiIiIiIiJKYwwOERERERERERGlMQaHiIiIiIiIiIjSGINDRERERERERERpjMEhIiIiIiIiIqI0xuAQEREREREREVEaY3CIiIiIiIiIiCiNMThERERERERERJTGGBwiIiIiIiIiIkpjDA4REREREREREaUxBoeIiIiIiIiIiNIYg0NERERERERERGmMwSEiIiIiIiIiojTG4BARERERERERURpjcIiIiIiIiIiIKI0xOERERERERERElMYYHCIiIiIiIiIiSmMMDhERERERERERpTEGh4iIiIiIiIiI0hiDQ0REREREREREaYzBISIiIiIiIiKiNMbgEBERERERERFRGmNwiIiIiIiIiIgojTE4RERERERERESUxhgcIiIiIiIiIiJKYwwOERERERERERGlMQaHiIiIiIiIiIjSGINDRERERERERERpTCk4JIQYJoRYJoRYKYQYYfL4YCHEHCFEsRDibMNjlwohVgT/u9SvhhMRERERERERUewcg0NCiEwArwA4EUBHAOcLIToaFlsH4DIAnxqeWwvA/QD6AegL4H4hRM3Ym01ERERERERERH5QyRzqC2CllHK1lLIQwOcAhusXkFLmSynnAyg1PPcEAL9LKXdKKXcB+B3AMB/aTUREREREREREPlAJDjUGsF737w3Bv6mI5blERERERERERBRnKVGQWghxtRBilhBi1rZt25LdHCIiIiIiIiKitKESHNoIoKnu302Cf1Oh9Fwp5ZtSyt5Syt5169ZVXDUREREREREREcVKJTg0E0AbIUQLIUQFAOcB+FFx/b8BOF4IUTNYiPr44N+IiIiIiIiIiCgFOAaHpJTFAG5EIKizBMCXUspFQoiHhBCnAYAQoo8QYgOAcwC8IYRYFHzuTgAPIxBgmgngoeDfiIiIiIiIiIgoBQgpZbLbEKF3795y1qxZyW4GEREREREREVG5IYSYLaXsbfZYShSkJiIiIiIiIiKi5GBwiIiIiIiIiIgojTE4RERERERERESUxhgcIiIiIiIiIiJKYwwOERERERERERGlMQaHiIiIiIiIiIjSGINDRERERERERERpjMEhIiIiIiIiIqI0xuAQEREREREREVEaY3CIiIiIiIiIiCiNMThERERERERERJTGGBwiIiIiIiIiIkpjDA4REREREREREaUxBoeIiIiIiIiIiNIYg0NERERERERERGmMwSEiIiIiIiIiojTG4BARERERERERURpjcIiIiIiIiIiIKI0xOERERERERERElMYYHCIiIiIiIiIiSmMMDhERERERERERpTEGh4iIiIiIiIiI0hiDQ0REREREREREaYzBISIiIiIiIiKiNMbgEBERERERERFRGmNwiIiIiIiIiIgojTE4RERERERERESUxhgcIiIiIiIiIiJKYwwOERERERERERGlMQaHiIiIiIiIiIjSGINDRERERERERERpjMEhIiIiIiIiIqI0xuAQEREREREREVEaY3CIiIiIiIiIiCiNMThERERERERERJTGGBwiIiIiIiIiIkpjDA4REREREREREaUxBoeIiIiIiIiIiNIYg0NERERERERERGmMwSEiIiIiIiIiojTG4BARERERERERURpjcIiIiIiIiIiIKI0xOERERERERERElMYYHCIiIiIiIiIiSmMMDhERERERERERpTEGh4iIiIiIiIiI0hiDQ0REREREREREaYzBISIiIiIiIiKiNMbgEBERERERERFRGmNwiIiIiIiIiIgojTE4RERERERERESUxhgcIiIiIiIiIiJKY0rBISHEMCHEMiHESiHECJPHc4QQXwQf/0sIkRf8e54Q4pAQYm7wv9d9bj8REREREREREcUgy2kBIUQmgFcAHAdgA4CZQogfpZSLdYtdCWCXlLK1EOI8AE8AODf42CopZXd/m01ERERERERERH5QyRzqC2CllHK1lLIQwOcAhhuWGQ7gg+DvXwMYKoQQ/jWTiIiIiIiIiIjiQSU41BjAet2/NwT/ZrqMlLIYwB4AtYOPtRBC/C2EmCiEODLG9hIRERERERERkY8ch5XFaBOAZlLKHUKIXgC+F0J0klLu1S8khLgawNUA0KxZszg3iYiIiIiIiIiINCqZQxsBNNX9u0nwb6bLCCGyAFQHsENKeVhKuQMApJSzAawC0Nb4AlLKN6WUvaWUvevWrev+XRARERERERERkScqwaGZANoIIVoIISoAOA/Aj4ZlfgRwafD3swGMk1JKIUTdYEFrCCFaAmgDYLU/TSciIiIiIiIiolg5DiuTUhYLIW4E8BuATADvSikXCSEeAjBLSvkjgHcAfCSEWAlgJwIBJAAYDOAhIUQRgFIA10opd8bjjRARERERERERkXtCSpnsNkTo3bu3nDVrVrKbQURERERERERUbgghZkspe5s9pjKsjIiIiIiIiIiIyikGh4iIiIiIiIiI0hiDQ0REREREREREaYzBISIiIiIiIiKiNMbgEBERERERERFRGmNwiIiIiIiIiIgojTE4RERERERERESUxhgcIiIiIiIiIiJKYwwOERERERERERGlMQaHiIiIiIiIiIjSGINDRERERERERERpjMEhIiIiIiIiIqI0xuAQEREREREREVEaY3CIiIiIiIiIiCiNMThERERERERERJTGGBwiIiIiIiIiIkpjDA4REREREREREaUxBoeIiIiIiIiIiNIYg0NERERERERERGmMwSEiIiIiIiIiojTG4BARERERERERURpjcIiIiIiIiIiIKI0xOERERERERERElMYYHCIiIiIiSqbD+4Df7wcKDyS7JURE/jq4E3hrKDDznWS3hBwwOERERERElEzzvwCmPg9MfDLZLSEi8tcPNwAbZwGjbk52S8gBg0NERERERMlUuV7g5/blyW2H3/ZtBqa/Dkjp73q3LQN2rPJ3nUQUHwV7kt0CUsTgEBERERFRMmVkBn6q3ESVFAGbF8S3PX755ipg9B3AjpX+rveVvsBLPf1dJxHFh2DIoazgJ0VERERElEylxYGfKsGhyc8Arw8CtiyKb5v8cGhX4Ofhff6ts7TEv3URUfwxOFRm8JMiIqL0VVoCfHcd8M/cZLeEVJSW+HuTSaljwdfA0lHJbkXyuAkOrRoX+Hlge/za45esnMDPQzv9W+eGWeHfS4r8W2+8bFsG7MpPdiuIkkfLjKSUx+AQERGlr7mfAvM+Bd48KtktIRW/3AY83iQw8wmVL99cCXx+QbJbkTx/PBT4qRIc0rJxykJgJDMYHFryk3/r3LUm/Pvudf6tN15e6Qu80C3ZrSBVRQVAwd5kt6J84fYsMxgcIiKi9LXwG/VlpQQO749fW8jZrOA0uJ/+K7ntoLKhtBQoOpTsVqjRMksOK9xEacGhooNxa44vpATW/Rn4XcuMsrNqHPDtNc7LFeqOwwW7nZcf/xgw4Qnn5dw4tAtYOdbdc4oP+9sGio/XBwIjmya7FeVLSWH4d34PUhqDQ0RElL6KC9SXnfo88HhjYP+2uDWHFG2YmewWUFnwxwPAow0CmQBOSoqAPRvi3iRfHAgeg+Z8mLw2HNrtnLm0f0v4dy2DyMryMcBHZwDzPwc2L7RftlAXFFP5bCc+AUx4zHk5N766HPj4LHdZjIUH/G0DxYffxdMp0v6tyW4B2WBwiIiIyp95XwCzP3BeLrtS+HenqZYX/xD4ydoRZUPhAWDcI/73Us77Ati61N91Unz8/XHgZ6FCxt+3/wae65ScoVpuCkvv1A2pWvm7f20oPAg8XBdY+K3zskWHgCeaB2Yic1pO4xSIn/9F+PetS+yX1QdZih0yw5yO615tXx74uW+T+uu76Yyg8mXvpjQPiui+B2m9HVIfg0NERFT+fHc18NN/nJfL0vVmO124V6gS+FnIgshlwqSnAv/N/cTf9X53NfBqP3/XSeomPQ08UF2tMHlGVuCnSsbGou8CP5Mx5CEi4Czsl3Wb3TSyeWB7OdX82L85MPTj68ud1znh8cDPxd/bL6fflk5D4ITufR/cYb9ske7zdMocerKF/eNeZVcM/NzrEByK2AZlZIgj+e/Z9sDTbdSX374ifm1JCt33e//m5DWDHDE4RERE5ZdTr7G+B6vQ4eYlf3LgJ2fLKhv2BS9AMysktx2qigudlyFg3MOBn1NfcF5WG9bkJhtI+54nkj6DERIosanPo8+CatDVed1aTR6noTJugmLabGG51e2XK3ETGNHdPDplA0VkDjkEh7T6TH7Lyg381A+dM6MPiqls48U/AO+fEqiXRamt6BDwYk9g5jvqz7H7bmtWjgVe7g3M+9x721KZStF9ShoGh4iIqPxyunHQ32gVKdaD4LAyNTtWJXdWsXmfBX6WheDQtmXAI3XD2St2/ngYeHdY/NuU6lQKHIe4GFo00efCxSoq1Yr8t91xS3usQhWgch311yhxCD66CXqvnRr46RSc0gc8nYJD+swhp2ygwoOAyFBbb7xogR6njCj9Z6kyBO7LSwIBStXzESXPng3AzlXAqJvtl9MH+pyCiUA4a2jjHOdld65WyyYsPAjsU3jteIn4fjODLpUxOEREROWXUzZQ4QEgq6LasjnVAj9/vy/2dpV3h/cDL/WM35AON4SPlzpu6pfs3aReS0abxnzxj87LTn4aWDdNvR3JtGZyoGhvaYn6c/YpDjmQLjIr3CyrDUVLJON+ZZdhogVOKtZyPmZFrNMh4OIlI9IpE0bLHBKZPmcO7QcqBQNjbur4+FlPSstIctoG+vftFPT65bbw705tXTcdeKY9pwhPpkO7Az8rVLVfTr+PHtzuvF7tGKQSAH+xR6BWmpMPTwOeaeu8XLy0OCr8O2tvpTQGh4iIqPxyKkR7eB9QpV5wWYeeWjc3mOlu09xktyDM1+CQi33gxe7Aa0cAqyc4L7v058BPN0N74lVo10+fnR8YInFA4YZIM+V5teXiFRyqUFl9Wd8Yg0M2wRHtxqpSLedjlr5wutOwRTfBobodAj+dhodoba1Yw9/MoaKD4awpN1kITlk+bmjnFqdAlpvMoZlv6ZZ1OBaMeyRQDPufv+2Xo/jJnxT4WaGS/XL6fXTPRuf1hoJDPgYztRk+k3Xe0DrXAH+/h+Q7BoeIiKj8croIKToIVKod+N2pN0t7vM3xsbervNOKtaaCjEz/1qXPgHGqCaLtLyrZQJoDLmZxSea02GPuCdRFcaIVb3ca0qRnt6z+xsbNTY7Tspvmh39v2l99vX5xkzmkPVa5jnPwWx+kLXEINmxeEP7dKTijBTmcXl8LSOXWULghdFlzSDtu2wWHpAxkLaksCwQyQSY/6/zdljL8OTgFcaa/Gv7dabvqOX1eWtBbusjK89uaSe6yAssbbT/ROpis6PdnlSFgmdnB9Tts293rnNcV1ZYkFNwHEBEAd/M9oIRjcIiIiMqOJT+7q2NjdwMtZeBGNDfYo2UXHCopDqd4p2tK9IbZgVmP1k13XjbboSc1kYSfwSFdmv9hh6yJvCMDP93sL24CPqrDr+Lhz5fcFW522gbfXh3+3S44pA9GOAV8IgJJDjdZ25aFf3cKTMRDVHDIruZQsH0VazkHXCrX1T3P4aZw8tPh3w87DFXSbu6cXl8LcORWc359tzWHcqoCGdn2n1dJUeCzDwWHHNo75h7gjwfD9cqsFB1E6GbXKeC0elL4dz+HwIWCQ0nKaF37J/DBqcCEkcl5fSCwjeIZaHDsAAh+9vqsGDP6NqrUktIyh5z2gQ9Pd16XUbL2F/3rpus1VBnB4BAREZUNB3YAX1wIfH6B+nPsbra1G/2cYL0A29563Q3AmknWy5Vnq8cHfi7/zXnZGW85L+PWur+8Pc/PYWX64JBWb8KKNuTGzcwsbi6aDyWx2LdbTjfQ878I/65aF8bpJkcfZJr2qvVyAPDz/4V/T0qvtiE4ZNcG7ThVqbZzMFErHA04Z29pARTAuY6Ntp86fa5a5lBOVedMGH3mkFMQp3B/YPhfdkX7baWtRzlzKFhH6IfrHV5fP1uaw/vqeo6uPS4Cj47BNC04lKRhQlonzZaF/q53wdfhGmxO3j4WeLS+v6+v5zSs67A2tNDhs9JfP6h0AKjWHPJ0DkjS/qI/XnPG15TG4BAREZUN2sXF9uXqz7G7ENNulrReP9s6H8lKxU4h2vAslSKZ+pt9v+z7R31ZfY+vn8PK9BkoTjd62qx2boJDO1erL6uSwVV0CNjrYrvFi5vvj+oQNKdsIH2AYd6n9ssW6m5WkjFLlKvMoYLADHw5VQLHN7vgwORn1NYJAA27hX93yorT1lVcYJ9doQWEcqo51zzSxYYc21p0MBAYysq1P257DQ450d/cOmWa6YNXrjKHVINDCpkgJUXAtFfUhoCt+0utAyQ7N/DT75mnvrkycr+1E+/adk7HLS2T0emz0u8DSsEhxXOtl46PVMgcUv2eUVIwOEREyTP/K2BeHG4iqZwK3gQ5ZRbo64fY9UBHBYfsZgji1Kvh3kyFGwz98B+/prN3U9siInAgLBeLqQ12+5aU4aE5Bbv9e329sQ84L/NoA+DZDvF5fSf674yboVp2vfX6QIjTccDNTF76oYdJqeXkIjhUVBAIimTlBp6nmmnlFJwpOhQYpgXYZw6Vlgbapw0dVQmq51Z3mTnkFBwKvn52rkPmkDYEr2bw3w77RPUmgZ/akFAr+uOb42xlB8MzYroJDjkG04LbSyVzaNrLwG93AbPfc1723eMDw8WcuJlRq6xa4lAvTgsSOn1WrjOHFGsOldXgEIeVpTQGh4goeb69CvjuaufliIDwRahTZsGsd8O/2xVM1W6qQsPKHHrr9crCTFF+024G9irMtqL3zVX+vL7xJsQuYyFidiwfP6vf7wv/brcf6m8Y3WQOqagXnLZ48G32yyU7223/lvDvhx0KF+vZBjt0n6XTTambgK4+mJiM4JDxhs3pWJSVE8geAuyDLj0vCf/umN1wEKgSHKJjN+xDu9FVycbR9sGcqoHf7Y6b+iCqYzaOljlUUX1mN6e2AuF9tnYr++X0+4jTOosO6YJTbjKHXBRxd7JjZeCnm1kDVQkfg+/6/UOlcHO8Vaxl//imeYGfjplubjOHVGcr87DtUyE45Oe+Tb5jcIicrfsr8oLYzrdXA7Pfj2tziChNhYJDDhdM+hs9u5tSY3DIts6HMTiUhtPaa7UlFn/v7gZ6yyJ/Xt8YDLC7KdT3fPsZyJv7Sfh3u/0wYvpqn4M0oYwBh17lBV/7+7pu6W+a3QwjsNte+u+d03fQ6/CwpASH3AwrOxwIimTlBP5ttx/K0nDAxylIWXRILYhSZAwO2WVn6oJDkPYBvSU/6V7D5v2XlgRumrMqKmQOBR/TbvKdModWTwj8dPrO6s8rjsGhg4GC3CLTXQad0w20VvNsokJB6L2bAj/d7NtO31kt69vP4Lf+PSez4L5GGzpnZffawE+nz0rb70Sm82ewez3w90dq6/USmHttkPvn+EF/jHPKtKKkYnCInL17PDD1Bee6BQe2B+pM/PTf9OxVJ6L40m4snG6K9Rcedj3gJbpiqYDzUA69dAwO/f1x+Hc3WRl+DTswrsdu2NCOFbp/xOl8ZPe+tH1JZKoP+1Glrc/pBja7or+v61ZEcMjF0ELb4FCchpVl6W4Ck1Is1c1U9oeCmUPZCsseDg//cqrjUlSgCw7ZbDvtscp1Aj/ttrN2LK5QxbmtEc9TCE4pZQ5pgSzFzCHNtqX2j2s1qqo1AQ7usF+26GBwCJxD8Wwjp221YUbg5z9/O69LC+A4Bcf0369tDrX9tDpeWuDJD/pzcDLr0miZO6qfl1PmkPZ45Tr22cwA8PFZwNKfg89zCg7pbuPzp1ovp1/P3iRlZDFzqMxgcIjU6VPEzehnbkmFdFAiKl+0Ar9O6rYL/27Xqxk1rExxtjIgPYNDXjmmxquuxxAUdLrR0cTrs1IJDuVU9b8mh7Y9nd6Xfvif05TM8aC/YXKa/UpPteaQ47Ayw/5hV7+jx8WBn53OSG7m0LEPBn46DdXKygUytcwhmxutokPh4JCTooNApTrOr+8mc0grnq0F31SDQyp1hLIVMoe0fVC15pB2s+0UcNH2keqNnYOJ2mfgVDzbyLFGk6L8qcDGWYHfnYIY+7eGf1etlZblkF3jhv6zPOzimOE37btl93npg6Iq9byAwPfLaR88qBv65/R56YND/8yxXs5tMGbbcmDSU+6e4yQiOKSwbz/dFvjoTH/bQEoYHCJnWgTdKcUzYrYPFz12RERO5n8FfHSG2rKV64Z/tw0OBS+YtJsXN7OVabUGyFz7U8K/uykkbcdzcChOmUN2mSvaTU6FKv7f5BwIZio4bdc9uuBQMoKZ+gCDmwCZ3bL69+E2OGQX8CgtBirXCwTzkhEc0mZg1Grd2AaqCwJBkVDNIafgUPDmvU476+W0ZVUCPtpjFRWycUoKAzfaWQr1kfRU6ghl5TpnDkUNK7NZVkr1Y4U2rKxyXfWZ1dxmDvmVcbhWl1Hi9Pq714V/Vw3oOmXyuqHflsmcBELLyrPbXlrwrFJt5/06lDlU293xxSmYqg8O2R0H3H5G758EjHvE3yxKN1mfQCAhYdUf/r0+KWNwiJxlVw78dAoO6cdg+12AkyjZpAQmPpka00KnIy2FXoV205hb3aEgtT44lONutjKVacTLMzcBF79SyI3BANXg0MI41d6xy3DRbli1FH6/6mdIGZ5m3HF4pe7GJhnBoYjXd3Fz4ltwyPCddazjkxsI5jkN+4iHn/8X+Kkyq1VotjLV4FAloEFXoFYL6+WkjKyNY5s5pBV5VixInZWjy8TwIXNIW0dWrnrmkMqwspIihIb3aRlUVrR9pFJttYLUFbTMIR+HlanSPifA+fW1GjpA+DjjxM9hs6kSHMpSyBzS7nkq1Qkci+yC9fpsO1fBIYdtoBpwcdtBE49tHzFbWZInSyBbDA6RM+0g6Zg5xOAQlWM7VgHjHwU+vyDZLSEn2k1jVkX7m2LtYiozy/nC3Xgxk+7DytwEJuIVHFKtKbPoO39e38juglvbX5oNCPzc43KGNyv6bel0wZ/s4FBE5pCb4JBNW90Eh4w3YbZBjIJAsKVClcDzkjEMDwgci7T2WDHOVuZYnygXyMhU2F9lsDZOJYfgUPB7V0lhqFZJMDgUKp6teCwoPmQdgA5lDuUE2mobSAu+jwqVA9tLpXi2/nlWCvcHAl651dQKUmdXCgSy3ASH/DpuVtYFupxef+Ps8O+qmUN+DptNleCQ9t2yHd4YPL5ogUenbD8gGBxyEXx2CqLoz8O2mUMej2duMt2cRAwr87kOH/mKwaHyZv5XwJrJ/q5TO0Dvcyg6p88c0qemEpUH2kU7M4eSw1WmSvDCIzvX/iLETeYQaw5Fcnr/8egZjMocSuLNA+AwrCzYttZDAz/9ykZxk42j3z5+Dv1QpS/U6yo4ZLNd9+mOv26HlSllDlUGIN3VhvGTLFULVGfph5U57IfZFQPZQHb7gLatQoWTHeoIAWpDtYoPh4+v2r9VWW0DLYiTlRPYDiptzdKGdSkMHc6tEVin3Tnn8P7AvqINa7NbVvsMshxeP2rGOp+Oodp6K9Z0vtnfvxWo1SpQTkJ1OGz1JrG1Ty+ijo+PgQm3QsXe7WoOBYND2vfAKUiZVTEYfHZRdsNpG5QqBofcZg5p+8zTrd09z3adwWuG1sf6V0+L4kIpOCSEGCaEWCaEWCmEGGHyeI4Q4ovg438JIfJ0j90Z/PsyIcQJPradzHx7FfDBKc7LqSotDdcScjpR6GsOJWPMPpVdfvWqx1NJ8EakvKXD/v0x8OWlyW6Fv/SZQ3Y3kNpNaKjmEGcrU+Z0sRmPwE1UzaEkn2dsh5UFjxOqhXBV6Y8/qZ45dGA7AAHkVLMPTBizdLQhSGZ+/G/4d6fe56iaQwoZLhWCw+iTdQ1TWqoWqI4IDjkMh82uFJj5aavNDFwRRZ4dghihITIKM5sZh5W5yYaxakOxLjjkVMdH+w5k5wa2g93nuj44dLlgNwIBQpvtWrgfyKkSnhHQ7txRGMwcysqxX27+l5H/tvtc3WS2ad+9ClWcg55FBwND4HKq2WcO6V+/9bHqbXGi/3ySWbtUZAbbYPe5asGh4DHednhnsO5UhcqBz1U1c8YpmKe/vrEriu22cyAu51YZyJyq3ZqzlaU4IR16Y4UQmQCWAzgOwAYAMwGcL6VcrFvmegBdpZTXCiHOA3CGlPJcIURHAJ8B6AugEYCxANpKab2X9q5aVc7q1SvGt5XG8qcEfuYN8md9pcXh2hqVagP1Olgvu2cjsGtN4PcazYEaTf1pQ7JJGdgOWk8C+UfbXwH/9tl4KToIbJwTKADY/Ihkt8Y/fh8z4mXb0uDNZpBde/esB3atDVwMZ2QBDTqbL3doJ7BlMdCwG7BjZeDivV5Hi3Xqjm9A+TrGqdJ/X+u0Af6/vXuPkas87zj+fXZ9n734vmuvb2tw7BjbBGJudWiKaYGSFuOWpFQokMqSaURaGlVqIY2UlkY0laKmrdSQEkhFKK2LTFCs3ijiprQJBoMpMXYt1te18f0WFl/Wu/v0j/cdz9nNnjOz9qzHu/P7SNbOOfP6zHvOnPd9zzznfd9T15Se9oO3e98hLcf5dXx3716pkz8GdVP7T5vMa7k+v+92J10O9c39p/vocDhnpyyIf+f3nig9a7tZee06A3veDK/rmsL3kObApsJTRGddX3i4RDmUUm8caQvHwSzMy5GfbLkv74FdPy4sj64PZbI/e98qBA3GNIS5dNIc2xme2DZlARzcAtOWhB+9/dn/0/C3rilMDt3yycIP/wtx6njYv3wgJU3+eDYthMNtIf2klLv2e94MvVvqm8PE+E1XFH6g9tW+Ply75acFSPu+zp4Kx3bKfDjeHvY97Xqv4wAcfh+mXxWe6jWxFRpa+k97cHP40TpxDuzfBM2Lw1xw/elbZmdeWwiAJZ06Hs7t5iUh6HWiPX2/jreHeXRmLwv7N7q+99Msk04eCefJqFz44Z9VZg5uCYGeuiY4uj077c7/Dm3FmY4QFJj+if7THdvR+0bZ+FnhX3+8G3b9pLCcVQ4/OgSHtob98p5wbqfZvymk6e7MPlbuhYmuG2fAhDnp2xyIk4cLQcyG6TBxbnb6wbp+ydczDS3pc3Xl6/jGlvC9zVia/uS2Q1vD5M4N0+DojuzzpX19InhkMGdZej53v14IENU3p9cZyXYDih+vwbg2P9IWyljdVPjZvuzr6GSbcKlfmw5R9tprb7n70v7eK6Xn0LVAm7tvd/dOYA2wok+aFcBT8fVa4GYzs7h+jbufcfcdQFvcngwVyTuTxe48JmN+pT4Ccyg4uj1U1tXeU6Da5QPpg/XkI8k2kG7R+e+opoZzE4z2u81Ypq0m/Msq4yr/vR1+P7tX1mDMKdC37FViqFSvz884t/LnS344arnOn+R2Og5kp63UvDl53WfDTZXus9k9NvqW7VLr2GLpvCf0AKiJN3aKzWVkFubmyS+Xw4FNIUBSKifko1hdZDUhHRQ/D62ES/18WbKaUG+WUhfWlHBu549r/ufGQNrPtDrfE/V2TZHteg9g8bstsl/E4zm6Pvvz8+/l241knlLzWhvOraw6q+8uDKS3XZb8sakdWXwopneH41RsjiqSn1/Ga6LkflWyzT13vZd1bsfjk69fss7tnu5wTK2E+qXXdood28T7WZ/ft/4t5zxRAxLrgmL1QKWHjFe5Um4jtQDtieU9wHVpady9y8xOAJPi+tf7/N+U2wvR/Pnw6qslZOsS98Z3Yet/XPzP3Ra7RF9W5C4VhChuw/TQ3TXN2ZOwO25zbA6mZ2x320YYPS0MP8tNgOYieUiOcb+UbdsI5GBOQ/93seT85c9XKO2craQzI2HPAMrXUHGuzpjAuYvjS9G2TqCU88ULZbb1U7DvHWhJSfvhGTiYgy8/D8//bug9kJb26DE40Zi4qDoCl6X0bhiukuUVYHZjYS6RvraPAU/Ul+UoMwc+gI5EHibVw/iU7fbNa1nOb++93Un10Diecxfo7uG1O3zYAUdycP/34e9vhMn10JjVfia2OzuXflw7O6C9xHqzfSx0xh8wzQa5MtZbpVxrfDAOplwNu38MdKd/B11nYFdin0blYGba91pDoR7oyf78bRuhcR7c80/w7euguS79GLTnwh3q61bD0yuhpSG9h8tAlHpNlk/3W4/CS4+EHjFp11DbRsEN98In7oHHboCmBqjrJ21PF+wYA5/+Irz2jex8nK6BvTn4wnfglUdDUCutLjz+YTi3v/oaPDoNGutgYkravbnQW+hX/gy+uxymNfTfi6rnLOzoU2Znji8M80v6qAv25+CLT8G2V+C//gRaG/vviXHkCJycCl95FR6/KfQimpaS144uOJCDG++HH30TZo1P7z22d1zogbPkbnh+Ncxq7D9tdyfszMGvfzX0PP7fNTAn7XuN7RaEnmGj6mBSQwgkeE8IMHh3eH3WQ9uVl3V+5du5xSvDkxvnZtUZOZj76fBQmaz2sKer8H01NsDkMtUtu7ZAVy48JXlMHUwtsdyU+5psVw66aqE+Iw8nTsLhHKz8Ojx/P0zshAkpP3H3joXma+GTX4DnVqWfLwDbRgOJ+j+zjkv0VBpTm/59Jc8tyG67YXCuzQ/Wg+fgmlXh4S5zGwvBsp/7/ER+L7Vr06vugUW/WelcXDhLP6Zl7GN8/sxsNbAaYNaslC6UQ03X6co+sauUz+44GBqZrKEBAHNuhJarYef/ZG+3ZSlcdlPoOv/us8Xz0N0ZulkmH7N5KRrTGPbl7ClFsweL1Q6dJ9zVTx86eR2IUp9MUilNi8IPFghDv4p9B02L4Io7wx2ztLQjx8HH74C6ZrhiZSjfaWnHTYa5N8FVn4cnlodhL8PxPBiIrtPpc2jMuCb8sNv2chj+UY5j1dBS6C0z79bQRT1tu40zw3CTvHKd32Mnhqf/NM4MN0I6OzjXMwEKr3NTwo/iqR8P59bx9uxj0DgLTsQhc1nHFUKbfHBLGIKRtc0pC2DuL4UfulYzOOdr1jYnzoUlnw13zHf+KPs7qGuGjv2w7MHsa43xswuP225aXPyaZMHtYVjI/NvDNU9a+inzYeEd0HxlOGZnOsp7vIpta2QuzPMxZxks+Vy4uZj2f2b9Aiy8MwzlmXcLnDyakfYGmH8bnNgD7/xjRj4MLrs5nLOLVmZfw44ZH37kjhgFV98L+95NTzvp8lAPT/5Y2P7pE8WPxZ3fgXeeSa+Pa0eH/Z7QCq1dYR/TeqblpobvHmDxZ2HTc+mfP2J0qFcW3xWG63SdTp8bZdK8UK5nXgOzP5Wddvay8K9+euhFlvb5+TZu3q1huGj7GyFwWlMbyu+IEbG3VC2Mq4k90mJgOuuYjhwH8z8Tvqvju4rUGfNDm9h1Krs9hHDce2KwqlxlpWlRyEPjDGh7qfTtlrtua1oUtpm//u/P6Hq44jfC8dr8w+z6ZdLlIaAwY2mov8+eSj9fxk4MQ96heB037cowtBTCROLFzq28Ym1Mcr/LdWwbpoehZJctD0Hd5EOMfi5tSxgSDJfetelwm3e0H6XMOXQD8KfufmtcfhjA3f8ikeaFmOYnZjYC2A9MAR5Kpk2mS/u8pUuX+oYNGy5op0REREREREREpMDMLmjOoTeBeWbWamajgLuBdX3SrAPyj7u5C3jZQ9RpHXB3fJpZKzAPeON8dkJERERERERERMqv6LCyOIfQl4AXgFrge+7+npk9Amxw93XAk8DTZtYGHCUEkIjpngU2A13AA1lPKhMRERERERERkYur6LCyi03DykREREREREREyutCh5WJiIiIiIiIiMgwpeCQiIiIiIiIiEgVU3BIRERERERERKSKKTgkIiIiIiIiIlLFFBwSEREREREREaliCg6JiIiIiIiIiFQxBYdERERERERERKqYgkMiIiIiIiIiIlVMwSERERERERERkSqm4JCIiIiIiIiISBVTcEhEREREREREpIopOCQiIiIiIiIiUsUUHBIRERERERERqWIKDomIiIiIiIiIVDEFh0REREREREREqpi5e6Xz0IuZHQJ2VTofZTIZOFzpTIhUMZVBkcpR+ROpHJU/kcpSGZRL1Wx3n9LfG5dccGg4MbMN7r600vkQqVYqgyKVo/InUjkqfyKVpTIoQ5GGlYmIiIiIiIiIVDEFh0REREREREREqpiCQ4Pr8UpnQKTKqQyKVI7Kn0jlqPyJVJbKoAw5mnNIRERERERERKSKqeeQiIiIiIiIiEgVU3BokJjZbWa21czazOyhSudHZDgws5lm9oqZbTaz98zswbh+opm9aGbvx78T4nozs7+N5fBdM7s6sa37Yvr3zey+Su2TyFBjZrVmttHM/jUut5rZ+ljO/sXMRsX1o+NyW3x/TmIbD8f1W83s1grtisiQY2bjzWytmf2fmW0xsxvUBopcHGb25Xj9ucnM/tnMxqgNlOFEwaFBYGa1wN8BvwosBH7bzBZWNlciw0IX8IfuvhC4Hngglq2HgJfcfR7wUlyGUAbnxX+rgccgBJOArwHXAdcCX8tfTItIUQ8CWxLLfwl8y90vB44Bq+L6VcCxuP5bMR2xzN4NXAHcBnw7tpsiUtzfAP/p7guAKwllUW2gyCAzsxbg94Gl7r4IqCW0ZWoDZdhQcGhwXAu0uft2d+8E1gArKpwnkSHP3fe5+9vx9YeEi+IWQvl6KiZ7Crgzvl4BfN+D14HxZjYNuBV40d2Puvsx4EVCAy0iGcxsBvAZ4Im4bMByYG1M0rf85cvlWuDmmH4FsMbdz7j7DqCN0G6KSAYzawR+EXgSwN073f04agNFLpYRwFgzGwGMA/ahNlCGEQWHBkcL0J5Y3hPXiUiZxO65VwHrgSZ33xff2g80xddpZVFlVOT8/DXwR0BPXJ4EHHf3rricLEvnyll8/0RMr/Incn5agUPAP8ShnU+YWQ61gSKDzt33At8EdhOCQieAt1AbKMOIgkMiMuSYWR3wHPAH7v6z5HseHsGoxzCKlJmZ/Rpw0N3fqnReRKrUCOBq4DF3vwr4iMIQMkBtoMhgiUMvVxCCtNOBHOpxJ8OMgkODYy8wM7E8I64TkQtkZiMJgaFn3P0HcfWB2FWe+PdgXJ9WFlVGRQZuGXCHme0kDJdeTpj/ZHzsYg+9y9K5chbfbwSOoPIncr72AHvcfX1cXksIFqkNFBl8vwzscPdD7n4W+AGhXVQbKMOGgkOD401gXpy9fhRh0rF1Fc6TyJAXx2o/CWxx979KvLUOyD9t5T7gh4n198YntlwPnIhd718AbjGzCfFO0C1xnYikcPeH3X2Gu88htGsvu/s9wCvAXTFZ3/KXL5d3xfQe198dn+TSSpgs942LtBsiQ5a77wfazWx+XHUzsBm1gSIXw27gejMbF69H8+VPbaAMGyOKJ5GBcvcuM/sSoaGtBb7n7u9VOFsiw8Ey4PPAT83snbjuK8A3gGfNbBWwC/hcfO/fgdsJk/2dBH4HwN2PmtmfEwK5AI+4+9GLsgciw88fA2vM7OvARuJkufHv02bWBhwlBJRw9/fM7FnCRXUX8IC7d1/8bIsMSb8HPBNvPm4ntGs1qA0UGVTuvt7M1gJvE9qujcDjwL+hNlCGCQsBTBERERERERERqUYaViYiIiIiIiIiUsUUHBIRERERERERqWIKDomIiIiIiIiIVDEFh0REREREREREqpiCQyIiIiIiIiIiVUzBIRERERERERGRKqbgkIiIiIiIiIhIFVNwSERERERERESkiv0/gTOGw5Fc40gAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# figure size of the plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 8))\n",
    "testing_data['max_u_filtered_regressor']['mlp']['predicted']['bus_15'].plot()\n",
    "testing_data['max_u_filtered_regressor']['mlp']['real']['bus_15'].plot()\n",
    "# plot a line with the threshold\n",
    "plt.axhline(y=threshold, color='r', linestyle='-')\n",
    "# Add legend\n",
    "plt.legend(['predicted', 'real', 'threshold'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
