{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Target Dataset Hyperparameters Tunning\n",
    "The objective of this notebook is to tune the hyperparameters of the model to obtain the best performance on the sparse dataset. \n",
    "\n",
    "**Summary of the Article**\n",
    "- Description of the dataset.\n",
    "- Hyperparameters tunning:\n",
    "    - Gradient Boosting Regressor.\n",
    "    - SVRegressor.\n",
    "    - Multi-Layer Perceptron.\n",
    "    - Long-Short Term Memory.\n",
    "- Training Models.\n",
    "- Next Steps.\n",
    "\n",
    "## Description of the Sparse Dataset \n",
    "The objective of this master thesis is to forecast the occurance and amplitude of constraints in the electrical grid. Using historical values of active and reactive power it is possible to compute the voltage and current values in the network, thus obtaining the occurance and amplitude of constraints. Since not every timestep containts a constraint, not every time step is woth to be predicted, so it is usefull transform the target features into a sequece of values that better represent the constraints. One way to obtain this target dataset is to set all time steps that do not characterise a constraint to 0, a positive value (with the amplitude of the constraint) otherwise. The following formula states the transformation:\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\text{Target} &= \\begin{cases}\n",
    "            0 & \\text{if} \\; \\text{constraint} \\; \\text{is not violated} \\\\\n",
    "            \\text{amplitude of constraint} & \\text{if} \\; \\text{constraint} \\; \\text{is violated} \\\\\n",
    "        \\end{cases}\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "The resulting dataset is a sparse dataset, since constraints are not as common as regular values. Bellow the dataset for maximum voltage constraints is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from thesis_package import utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns='timestamps')\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "X_max_u_train, X_max_u_test, y_max_u_train, y_max_u_test = utils.split_and_suffle(exogenous_data, y_max_u)\n",
    "data = {'X_train': X_max_u_train, 'X_test': X_max_u_test, 'y_train': y_max_u_train, 'y_test': y_max_u_test}\n",
    "threshold_value = y_max_u_train.loc[:, y_max_u_train.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "threshold_signal = pd.Series(np.ones([len(y_max_u_test)]) * threshold_value)\n",
    "# Plot prediction_gb_max_u\n",
    "sns.set(style='whitegrid')\n",
    "fig, axs = plt.subplots(1, 1, figsize=(30, 10))\n",
    "axs.plot(y_max_u_test.reset_index(drop=True))\n",
    "axs.plot(threshold_signal)\n",
    "axs.set_title('Dataset Sample of Maximum Voltage Constraint', fontsize=30, fontweight='bold')\n",
    "axs.set_xlabel('Timestep', fontsize=18, fontweight='bold')\n",
    "axs.set_ylabel('Constraint Value', fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse boolean dataset is derived from the one above. It represents the time steps with constraints as of the class 1 and the rest as class 0. This datset is used to train the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns='timestamps')\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "X_max_u_bool_train, X_max_u_bool_test, y_max_u_bool_train, y_max_u_bool_test = utils.split_and_suffle(exogenous_data, y_max_u_bool)\n",
    "data = {'X_train': X_max_u_bool_train,\n",
    "        'X_test': X_max_u_bool_test,\n",
    "        'y_train': utils.convert_df_to_bool(y_max_u_bool_train),\n",
    "        'y_test': utils.convert_df_to_bool(y_max_u_bool_test)}\n",
    "# Plot prediction_gb_max_u\n",
    "sns.set(style='whitegrid')\n",
    "fig, axs = plt.subplots(1, 1, figsize=(30, 10))\n",
    "axs.plot(y_max_u_bool_test.reset_index(drop=True))\n",
    "axs.plot(threshold_signal)\n",
    "axs.set_title('Dataset Sample of Maximum Voltage Constraint Occurrences', fontsize=30, fontweight='bold')\n",
    "axs.set_xlabel('Timestep', fontsize=18, fontweight='bold')\n",
    "axs.set_ylabel('Constraint Value', fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, not all busses have constraints, so it is only possible train classification models for those busses that have positive values in the target dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.cols_with_positive_values(y_max_u_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from thesis_package import aimodels as my_ai, utils, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Tunning of Models\n",
    "The objective of this section is to tune the hyperparameters of the models to obtain the best performance on the sparse dataset. In order to perfom the hyperparameters tunning, we are going to use the optuna library presented in the `optuna_introduction.ipynb` notebook. The models are the ones implemented in the `aimodel.py` file in the `thesis_package`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters\n",
    "num_trials = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y = utils.split_and_suffle(exogenous_data, y_max_u)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = train_y.loc[:, train_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.true_positives_rmse\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_xgboost_regression_sparse.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Gradient Boosting Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y = utils.split_and_suffle(exogenous_data, y_max_u)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = train_y.loc[:, train_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.true_positives_rmse\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_gradient_boost_regression_sparse.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement as above but for multi-output classification grandient boost classifier.\n",
    "def objective(trial):\n",
    "    # import data0 \n",
    "    y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    y_max_u_bool = utils.convert_df_to_bool(y_max_u_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y = utils.split_and_suffle(exogenous_data, y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)])\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'deviance', 'exponential']) \n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    # evaluate the classification performance f1 score\n",
    "    f1_score = sklearn.metrics.f1_score(valid_y, prediction, average='micro')\n",
    "    return f1_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_gradient_boost_classifier_sparse.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Support Vector Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y = utils.split_and_suffle(exogenous_data, y_max_u)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    threshold = train_y.loc[:, train_y.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold)\n",
    "    return metric.true_positives_rmse\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_support_vector_regression_sparse.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Support Vector Classifier.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    y_max_u_bool = utils.convert_df_to_bool(y_max_u_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y = utils.split_and_suffle(exogenous_data, y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)])\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    # classification performance\n",
    "    f1_score = sklearn.metrics.f1_score(valid_y, prediction, average='macro')\n",
    "    return f1_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_support_vector_classifier_sparse.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but Extreme Gradient Boosting Classifier.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    y_max_u_bool = utils.convert_df_to_bool(y_max_u_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y = utils.split_and_suffle(exogenous_data, y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)])\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'loss': trial.suggest_categorical('loss', ['deviance', 'exponential']),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-8, 1.0, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 1e-8, 1.0, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.XGBoostClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    # classification performance\n",
    "    f1_score = sklearn.metrics.f1_score(valid_y, prediction, average='macro')\n",
    "    return f1_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_xgboost_classifier_sparse.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
