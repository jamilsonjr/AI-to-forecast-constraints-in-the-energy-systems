{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Concepts\n",
    "In order the be able to use Machine Learning Algorithms it is necessary to understand the preliminary concept definitions of Loss Function, Cost Function and Gradient Descent.\n",
    "#### Preliminary Concepts: Loss Function\n",
    "The loss function is the function that is used to evaluate the performance of individual prediction of a model. The loss function takes as input the predicted and real value of the output and revaluates the difference between them.\n",
    "Some common loss functions are:\n",
    "- Mean Squared Error (MSE): $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - y_i^*)^2 $$\n",
    "- Mean Absolute Error (MAE): $$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - y_i^*| $$\n",
    "- Root Mean Squared Error (RMSE): $$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - y_i^*)^2} $$\n",
    "- Mean Squared Log Error (MSLE): $$ \\text{MSLE} = \\frac{1}{n} \\sum_{i=1}^n (\\log(y_i) - \\log(y_i^*))^2 $$\n",
    "#### Preliminary Concepts: Cost Function\n",
    "The cost function is the function that is used to evaluate the performance of the model. It is defined from the loss function.\n",
    "#### Preliminary Concepts: Objective Function\n",
    "The objective function is the function we want to optimize in order to find the best model. It is defined from the cost function. The cost function, that is, the loss over a whole set of data, is not necessarily the one weâ€™ll minimize, although it can be. For instance, we can fit a model without regularization, in which case the objective function is the cost function.\n",
    "#### Preliminary Concepts: Regularization Term\n",
    "The regularization term is a term that is added to the cost function (in the Objective Function) in order to penalize the model. It is used to avoid overfitting.\n",
    "Some common regularization terms are:\n",
    "- L1: $$ J = \\text{MSE} + \\lambda \\sum_{i=1}^n |w_i| $$\n",
    "- L2: $$ J = \\text{MSE} + \\lambda \\sum_{i=1}^n w_i^2 $$\n",
    "#### Preliminary Concepts: Gradient Descent\n",
    "The gradient descent is a method that is used to find the minimum of the objective function. It is formulated as follow:\n",
    "$$\\begin{aligned}\n",
    "    \\theta^{(i+1)} = \\theta^{(i)} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "\\end{aligned}$$\n",
    "Note that, Stochastic Gradient Descent is updating the parameters of the model at each iteration. Batch Gradient Descent is updating the parameters of the model at the end of each iteration over oa batch of training examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
