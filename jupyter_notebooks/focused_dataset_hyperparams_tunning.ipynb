{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Target Dataset Hyperparameters Tunning\n",
    "The objective of this notebook is to tune the hyperparameters of the model to obtain the best performance on the Focused data set. \n",
    "\n",
    "**Summary of the Article**\n",
    "- Description of the dataset.\n",
    "- Hyperparameters tunning:\n",
    "    - Gradient Boosting Regressor.\n",
    "    - SVRegressor.\n",
    "    - Multi-Layer Perceptron.\n",
    "    - Long-Short Term Memory.\n",
    "- Training Models.\n",
    "- Next Steps.\n",
    "\n",
    "## Description of the Focused Dataset \n",
    "The objective of this master thesis is to forecast the occurance and amplitude of constraints in the electrical grid. Using historical values of active and reactive power it is possible to compute the voltage and current values in the network, thus obtaining the occurance and amplitude of constraints. Since not every timestep containts a constraint, not every time step is woth to be predicted, so it is usefull transform the target features into a sequece of values that better represent the constraints. One way to obtain this target dataset is to set all time steps that do not characterise a constraint to 0, a positive value (with the amplitude of the constraint) otherwise. The following formula states the transformation:\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\text{Target} &= \\begin{cases}\n",
    "            0 & \\text{if} \\; \\text{constraint} \\; \\text{is not violated} \\\\\n",
    "            \\text{amplitude of constraint} & \\text{if} \\; \\text{constraint} \\; \\text{is violated} \\\\\n",
    "        \\end{cases}\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "The resulting dataset is a sparse dataset, since constraints are not as common as regular values. The Focused dataset consists on the compilation of all data points that characterize as constraints. Since the objective of this transformation is to study the effect of a more focused dataset on the model performance, after training, the model performance is evaluated on the previous sparse data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from thesis_package import utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max_u_focused = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_focused_constr.csv')\n",
    "exogenous_data_vm_pu_max_focused = pd.read_csv('..\\data\\ground_truth\\\\exogenous_data_vm_pu_max_focused.csv').drop(columns=['date'])\n",
    "X_max_u_train, X_max_u_test, y_max_u_train, y_max_u_test = utils.split_and_suffle(exogenous_data_vm_pu_max_focused, y_max_u_focused)\n",
    "data = {'X_train': X_max_u_train, 'X_test': X_max_u_test, 'y_train': y_max_u_train, 'y_test': y_max_u_test}\n",
    "threshold_value = y_max_u_train.loc[:, y_max_u_train.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "threshold_signal = pd.Series(np.ones([len(y_max_u_test)]) * threshold_value)\n",
    "# Plot prediction_gb_max_u\n",
    "sns.set(style='whitegrid')\n",
    "fig, axs = plt.subplots(1, 1, figsize=(30, 10))\n",
    "axs.plot(y_max_u_test.reset_index(drop=True))\n",
    "axs.plot(threshold_signal)\n",
    "axs.set_title('Dataset Sample of Maximum Voltage Constraint', fontsize=30, fontweight='bold')\n",
    "axs.set_xlabel('Timestep', fontsize=18, fontweight='bold')\n",
    "axs.set_ylabel('Constraint Value', fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from thesis_package import aimodels as my_ai, utils, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Tunning of Models\n",
    "The objective of this section is to tune the hyperparameters of the models to obtain the best performance on the sparse dataset. In order to perfom the hyperparameters tunning, we are going to use the optuna library presented in the `optuna_introduction.ipynb` notebook. The models are the ones implemented in the `aimodel.py` file in the `thesis_package`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters\n",
    "num_trials = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_focused_constr.csv')\n",
    "    exogenous_data = pd.read_csv('..\\data\\ground_truth\\\\exogenous_data_vm_pu_max_focused.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    return mean_squared_error(valid_y, prediction, squared=False)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_xgboost_regression_focused_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_focused_constr.csv')\n",
    "    exogenous_data = pd.read_csv('..\\data\\ground_truth\\\\exogenous_data_vm_pu_min_focused.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    return mean_squared_error(valid_y, prediction, squared=False)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_xgboost_regression_focused_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Gradient Boosting Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_focused_constr.csv')\n",
    "    exogenous_data = pd.read_csv('..\\data\\ground_truth\\\\exogenous_data_vm_pu_max_focused.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    return mean_squared_error(valid_y, prediction, squared=False)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_gradient_boost_regression_focused_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Gradient Boosting Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_focused_constr.csv')\n",
    "    exogenous_data = pd.read_csv('..\\data\\ground_truth\\\\exogenous_data_vm_pu_min_focused.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    return mean_squared_error(valid_y, prediction, squared=False)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_gradient_boost_regression_focused_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Support Vector Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_focused_constr.csv')\n",
    "    exogenous_data = pd.read_csv('..\\data\\ground_truth\\\\exogenous_data_vm_pu_max_focused.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=y_max_u.columns)\n",
    "    valid_y = pd.DataFrame(valid_y, columns=y_max_u.columns)\n",
    "    return mean_squared_error(valid_y, prediction, squared=False)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_support_vector_regression_focused_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Support Vector Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_focused_constr.csv')\n",
    "    exogenous_data = pd.read_csv('..\\data\\ground_truth\\\\exogenous_data_vm_pu_min_focused.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=y_min_u.columns)\n",
    "    valid_y = pd.DataFrame(valid_y, columns=y_min_u.columns)\n",
    "    return mean_squared_error(valid_y, prediction, squared=False)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results/params_support_vector_regression_focused_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
