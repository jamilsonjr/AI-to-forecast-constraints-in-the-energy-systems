{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Intro\n",
    "\n",
    "**Summary of Article**\n",
    "- Theoretical Introduction to Neural Networks.\n",
    "- FeedForward Neural Network Implementation for Regression.\n",
    "- FeedForward Neural Network Implementation for Classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Intro\n",
    "### Theoretical Introduction to Neural Networks\n",
    "Neural Networks (NN)  are a class of ML models that are based on the connections of layers of artificial neurons. The connections between the layers are made up of weights and biases, that are updated during the training process. Activation functions are used to determine the output of a neuron. Different activation functions are what allow the NN to learn and generalize expressive results. The following illustration represents the architecture of a neural network. (Only on thesis)\n",
    "** Figure here **.\n",
    "### Training Process \n",
    "The training process of a NN is the process of updating the weights and biases of the neural network to make it better at predicting the output of the input. Backpropagation is a method of updating the weights and biases, where the derivative of the loss fuction with respect to the weights and biases, is used to update the respective values. The training process takes the following steps:\n",
    "\n",
    "- Take a batch of training data.\n",
    "- Forward propagate the batch of data through the neural network.\n",
    "- Compute the loss function for the batch of data.\n",
    "- Backpropagate the loss function to get the gradients.\n",
    "- Update the weights and biases using the gradients.\n",
    "- Repeat the above steps until the loss function is less than a determined threshold.\n",
    "\n",
    "The most common activation function are: \n",
    "- Sigmoid function: $$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "- Tanh: $$ g(z)= \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$$\n",
    "- ReLu: $$ g(z) = \\max(0,z)$$\n",
    "\n",
    "The most common Loss functions for Regression is:\n",
    "- RMSE: $$L(z,y) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - z_i)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network with PyTorch \n",
    "Now Pytorch will be used to train a neural network. The data will be the sparse dataset normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sys; sys.path.append('..')\n",
    "from thesis_package import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create the data set class. Our data set class will extend `torch.utils.data.Dataset`. Afterwards, we will create the data loader class. The data loader class will extend `torch.utils.data.DataLoader`. The data loader is used to separate the data set in batches, shuffle the data set and create an iterator.\n",
    "\n",
    "First the dataset is loaded and prepared in tha same fashion as for the other ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns='timestamps')\n",
    "y_max_u = y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)]\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "X_max_u_bool_train, X_max_u_bool_test, y_max_u_bool_train, y_max_u_bool_test = utils.split_and_suffle(exogenous_data, y_max_u_bool, scaling=True)\n",
    "data = {'X_train':X_max_u_bool_train,\n",
    "        'X_test': X_max_u_bool_test,\n",
    "        'y_train':y_max_u_bool_train.astype(bool),\n",
    "        'y_test': y_max_u_bool_test.astype(bool)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the dataset class is declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9610, 0.9391, 0.2836, 0.0202, 0.6667, 0.1429, 0.2948, 0.0231, 0.9624,\n",
       "         0.2588, 0.9390], dtype=torch.float64),\n",
       " tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ThesisDataset(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        train_X, train_y = data['X_train'], data['y_train']\n",
    "        test_X, test_y = data['X_test'], data['y_test']\n",
    "        self.X = torch.tensor(train_X.values)\n",
    "        self.y = torch.tensor(train_y.values)\n",
    "        self.X_test = torch.tensor(test_X.values)\n",
    "        self.y_test = torch.tensor(test_y.values)\n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        return self.X[index], self.y[index]\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "dataset = ThesisDataset(data)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our data loader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 36172, total interations: 1131\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "from math import ceil\n",
    "total_samples = len(dataset)\n",
    "n_iterations = ceil(total_samples / 32)\n",
    "print('Total samples: {}, total interations: {}'.format(total_samples, n_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our training loop will look like the follwing:\n",
    "\n",
    "```python\t\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(dataloader):\n",
    "        # Zero grads, Forward, Backwards and Update\n",
    "    # Compute and print loss\n",
    "    # Evaluate model on validation set\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is necessary to decide on hyperparameters for the neural network. These hyper parameters will later be tunned using optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'input_size': dataset.X.shape[1],\n",
    "    'hidden_size': 32,\n",
    "    'output_size': dataset.y.shape[1],\n",
    "    'num_epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to configure the device once it is faster to train the models oh the GPU, if one is available. Later it is necessary to push th tensors into device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device # In my case I don't have a GPU, so I use the CPU ... Sad, so if anyone wants to give me a GPU, hit me up on LinkedIn :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class FeedforwardNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self, output_size, input_size, hidden_size, layers,\n",
    "            activation_type, dropout, **kwargs):\n",
    "        \"\"\"\n",
    "        n_classes (int)\n",
    "        n_features (int)\n",
    "        hidden_size (int)\n",
    "        layers (int)\n",
    "        activation_type (str)\n",
    "        dropout (float): dropout probability\n",
    "        As in logistic regression, the __init__ here defines a bunch of\n",
    "        attributes that each FeedforwardNetwork instance has. Note that nn\n",
    "        includes modules for several activation functions and dropout as well.\n",
    "        \"\"\"\n",
    "        super(FeedforwardNetwork, self).__init__()\n",
    "        self.feedforward_nn = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                            nn.Tanh(),\n",
    "                                            nn.Dropout(dropout),\n",
    "                                            nn.Linear(hidden_size, hidden_size),\n",
    "                                            nn.Tanh(),\n",
    "                                            nn.Dropout(dropout),\n",
    "                                            nn.Linear(hidden_size, hidden_size),\n",
    "                                            nn.Tanh(),\n",
    "                                            nn.Dropout(dropout),\n",
    "                                            nn.Linear(hidden_size, output_size))\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        x (batch_size x n_features): a batch of training examples\n",
    "        This method needs to perform all the computation needed to compute\n",
    "        the output logits from x. This will include using various hidden\n",
    "        layers, pointwise nonlinear functions, and dropout.\n",
    "        \"\"\"\n",
    "        return self.feedforward_nn(x)\n",
    "\n",
    "\n",
    "def train_batch(X, y, model, optimizer, criterion, **kwargs):\n",
    "    \"\"\"\n",
    "    X (n_examples x n_features)\n",
    "    y (n_examples): gold labels\n",
    "    model: a PyTorch defined model\n",
    "    optimizer: optimizer used in gradient step\n",
    "    criterion: loss function\n",
    "    To train a batch, the model needs to predict outputs for X, compute the\n",
    "    loss between these predictions and the \"gold\" labels y using the criterion,\n",
    "    and compute the gradient of the loss with respect to the model parameters.\n",
    "    Check out https://pytorch.org/docs/stable/optim.html for examples of how\n",
    "    to use an optimizer object to update the parameters.\n",
    "    This function should return the loss (tip: call loss.item()) to get the\n",
    "    loss as a numerical value that is not part of the computation graph.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()  # Setting our stored gradients equal to zero\n",
    "    output = model(X)  # Computes the gradient of the given tensor w.r.t. the weights/bias\n",
    "    loss = criterion(output, y) # cross entropy in this case\n",
    "    loss.backward() # Computes the gradient of the given tensor w.r.t. graph leaves \n",
    "    optimizer.step() # Updates weights and biases with the optimizer (SGD of ADAM)\n",
    "    return loss.item()\n",
    "    \n",
    "\n",
    "\n",
    "def predict(model, X):\n",
    "    \"\"\"X (n_examples x n_features)\"\"\"\n",
    "    scores = model(X)  # (n_examples x n_classes)\n",
    "    predicted_labels = scores.argmax(dim=-1)  # (n_examples)\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    \"\"\"\n",
    "    X (n_examples x n_features)\n",
    "    y (n_examples): gold labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_hat = predict(model, X)\n",
    "    n_correct = (y == y_hat).sum().item()\n",
    "    n_possible = float(y.shape[0])\n",
    "    model.train()\n",
    "    return n_correct / n_possible\n",
    "\n",
    "\n",
    "def plot(epochs, plottable, ylabel='', name='', title=''):\n",
    "    plt.clf()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.plot(epochs, plottable)\n",
    "    plt.grid()\n",
    "    plt.title(title)\n",
    "    plt.savefig('%s.pdf' % (name), bbox_inches='tight')\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('model',\n",
    "                        choices=['logistic_regression', 'mlp'],\n",
    "                        help=\"Which model should the script run?\")\n",
    "    parser.add_argument('-epochs', default=20, type=int,\n",
    "                        help=\"\"\"Number of epochs to train for. You should not\n",
    "                        need to change this value for your plots.\"\"\")\n",
    "    parser.add_argument('-batch_size', default=1, type=int,\n",
    "                        help=\"Size of training batch.\")\n",
    "    parser.add_argument('-learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('-l2_decay', type=float, default=0)\n",
    "    parser.add_argument('-hidden_sizes', type=int, default=200)\n",
    "    parser.add_argument('-layers', type=int, default=1)\n",
    "    parser.add_argument('-dropout', type=float, default=0.3)\n",
    "    parser.add_argument('-activation', choices=['tanh', 'relu'], default='relu')\n",
    "    parser.add_argument('-optimizer', choices=['sgd', 'adam'], default='sgd')\n",
    "    opt = parser.parse_args()\n",
    "    print(opt)\n",
    "    utils.configure_seed(seed=42)\n",
    "\n",
    "    data = utils.load_classification_data()\n",
    "    dataset = utils.ClassificationDataset(data)\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "\n",
    "    dev_X, dev_y = dataset.dev_X, dataset.dev_y\n",
    "    test_X, test_y = dataset.test_X, dataset.test_y\n",
    "\n",
    "    n_classes = torch.unique(dataset.y).shape[0]  # 10\n",
    "    n_feats = dataset.X.shape[1]\n",
    "\n",
    "    # initialize the model    \n",
    "    model = FeedforwardNetwork(\n",
    "        n_classes, n_feats,\n",
    "        opt.hidden_sizes, opt.layers,\n",
    "        opt.activation, opt.dropout)\n",
    "\n",
    "    # get an optimizer\n",
    "    optims = {\"adam\": torch.optim.Adam, \"sgd\": torch.optim.SGD}\n",
    "\n",
    "    optim_cls = optims[opt.optimizer]\n",
    "    optimizer = optim_cls(\n",
    "        model.parameters(),\n",
    "        lr=opt.learning_rate,\n",
    "        weight_decay=opt.l2_decay)\n",
    "\n",
    "    # get a loss criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # training loop\n",
    "    epochs = torch.arange(1, opt.epochs + 1)\n",
    "    train_mean_losses = []\n",
    "    valid_accs = []\n",
    "    train_losses = []\n",
    "    for ii in epochs:\n",
    "        print('Training epoch {}'.format(ii))\n",
    "        for X_batch, y_batch in train_dataloader:\n",
    "            loss = train_batch(\n",
    "                X_batch, y_batch, model, optimizer, criterion)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "        mean_loss = torch.tensor(train_losses).mean().item()\n",
    "        print('Training loss: %.4f' % (mean_loss))\n",
    "\n",
    "        train_mean_losses.append(mean_loss)\n",
    "        valid_accs.append(evaluate(model, dev_X, dev_y))\n",
    "        print('Valid acc: %.4f' % (valid_accs[-1]))\n",
    "\n",
    "    final_acc = evaluate(model, test_X, test_y)\n",
    "    print('Final Test acc: %.4f' % (evaluate(model, test_X, test_y)))\n",
    "    # plot\n",
    "    file_name = '.\\q4_1_results\\q4_1_loss_' + str(opt.learning_rate) + '_' + str(opt.hidden_sizes) + '_' + str(opt.dropout) + '_' + str(opt.activation) + '_' + str(opt.optimizer) +  '_' + str(opt.layers)\n",
    "    plot(epochs, train_mean_losses, ylabel='Loss', name=file_name, title='Loss(Epoch)')\n",
    "    \n",
    "    file_name = '.\\q4_1_results\\q4_1_acc' + str(opt.learning_rate) + '_' + str(opt.hidden_sizes) + '_' + str(opt.dropout) + '_' + str(opt.activation) + '_' + str(opt.optimizer) + '_' + str(opt.layers) + '_' + str(final_acc) \n",
    "    plot(epochs, valid_accs, ylabel='Accuracy', name=file_name, title='Accuracy(Epoch); Final Accuracy=' + str(final_acc))\n",
    "#%%\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
