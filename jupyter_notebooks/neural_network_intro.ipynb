{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Intro\n",
    "\n",
    "**Summary of Article**\n",
    "- Theoretical Introduction to Neural Networks.\n",
    "- FeedForward Neural Network Implementation for Regression.\n",
    "- FeedForward Neural Network Implementation for Classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Intro\n",
    "### Theoretical Introduction to Neural Networks\n",
    "Neural Networks (NN)  are a class of ML models that are based on the connections of layers of artificial neurons. The connections between the layers are made up of weights and biases, that are updated during the training process. Activation functions are used to determine the output of a neuron. Different activation functions are what allow the NN to learn and generalize expressive results. The following illustration represents the architecture of a neural network. (Only on thesis)\n",
    "** Figure here **.\n",
    "### Training Process \n",
    "The training process of a NN is the process of updating the weights and biases of the neural network to make it better at predicting the output of the input. Backpropagation is a method of updating the weights and biases, where the derivative of the loss fuction with respect to the weights and biases, is used to update the respective values. The training process takes the following steps:\n",
    "\n",
    "- Take a batch of training data.\n",
    "- Forward propagate the batch of data through the neural network.\n",
    "- Compute the loss function for the batch of data.\n",
    "- Backpropagate the loss function to get the gradients.\n",
    "- Update the weights and biases using the gradients.\n",
    "- Repeat the above steps until the loss function is less than a determined threshold.\n",
    "\n",
    "The most common activation function are: \n",
    "- Sigmoid function: $$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "- Tanh: $$ g(z)= \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$$\n",
    "- ReLu: $$ g(z) = \\max(0,z)$$\n",
    "\n",
    "The most common Loss functions for Regression is:\n",
    "- RMSE: $$L(z,y) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - z_i)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network with PyTorch \n",
    "Now Pytorch will be used to train a neural network. The data will be the sparse dataset normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sys; sys.path.append('..')\n",
    "from thesis_package import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create the data set class. Our data set class will extend `torch.utils.data.Dataset`. Afterwards, we will create the data loader class. The data loader class will extend `torch.utils.data.DataLoader`. The data loader is used to separate the data set in batches, shuffle the data set and create an iterator.\n",
    "\n",
    "First the dataset is loaded and prepared in tha same fashion as for the other ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max_u_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_bool_constr.csv').drop(columns='timestamps')\n",
    "y_max_u = y_max_u_bool[utils.cols_with_positive_values(y_max_u_bool)]\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "X_max_u_bool_train, X_max_u_bool_test, y_max_u_bool_train, y_max_u_bool_test = utils.split_and_suffle(exogenous_data, y_max_u_bool, scaling=True)\n",
    "data = {'X_train':X_max_u_bool_train.astype(float),\n",
    "        'X_test': X_max_u_bool_test.astype(float),\n",
    "        'y_train':y_max_u_bool_train.astype(float),\n",
    "        'y_test': y_max_u_bool_test.astype(float)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the dataset class is declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 9.3792e-01,  2.1600e-01,  1.3364e-01,  5.4339e-01,  1.0000e+00,\n",
       "          4.2857e-01,  1.2878e-01,  5.7269e-01,  9.3824e-01, -1.8370e-16,\n",
       "          2.1688e-01]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ThesisDataset(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        train_X, train_y = data['X_train'], data['y_train']\n",
    "        test_X, test_y = data['X_test'], data['y_test']\n",
    "        self.X = torch.from_numpy(train_X.values).float()\n",
    "        self.y = torch.from_numpy(train_y.values).float()\n",
    "        self.X_test = torch.from_numpy(test_X.values).float()\n",
    "        self.y_test = torch.from_numpy(test_y.values).float()\n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        return self.X[index], self.y[index]\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "dataset = ThesisDataset(data)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our data loader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 36172, total interations: 1131\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "from math import ceil\n",
    "total_samples = len(dataset)\n",
    "n_iterations = ceil(total_samples / 32)\n",
    "print('Total samples: {}, total interations: {}'.format(total_samples, n_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our training loop will look like the follwing:\n",
    "\n",
    "```python\t\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(dataloader):\n",
    "        # Zero grads, Forward, Backwards and Update\n",
    "    # Compute and print loss\n",
    "    # Evaluate model on validation set\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is necessary to decide on hyperparameters for the neural network. These hyper parameters will later be tunned using optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'input_size': dataset.X.shape[1],\n",
    "    'hidden_size': 32,\n",
    "    'output_size': dataset.y.shape[1],\n",
    "    'dropout': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to configure the device once it is faster to train the models oh the GPU, if one is available. Later it is necessary to push th tensors into device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device # In my case I don't have a GPU, so I use the CPU ... Sad, so if anyone wants to give me a GPU, hit me up on LinkedIn :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class FeedforwardNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self, model_params, **kwargs):\n",
    "        \"\"\"\n",
    "        n_classes (int)\n",
    "        n_features (int)\n",
    "        hidden_size (int)   \n",
    "        layers (int)\n",
    "        activation_type (str)\n",
    "        dropout (float): dropout probability\n",
    "        As in logistic regression, the __init__ here defines a bunch of\n",
    "        attributes that each FeedforwardNetwork instance has. Note that nn\n",
    "        includes modules for several activation functions and dropout as well.\n",
    "        \"\"\"\n",
    "        super(FeedforwardNetwork, self).__init__()\n",
    "        output_size, input_size, hidden_size, dropout = model_params.values()\n",
    "        self.feedforward_nn = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                            nn.Tanh(),\n",
    "                                            # nn.Dropout(dropout),\n",
    "                                            nn.Linear(hidden_size, hidden_size),\n",
    "                                            nn.Tanh(),\n",
    "                                            # nn.Dropout(dropout),\n",
    "                                            nn.Linear(hidden_size, hidden_size),\n",
    "                                            nn.Tanh(),\n",
    "                                            # nn.Dropout(dropout),\n",
    "                                            nn.Linear(hidden_size, output_size))\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        x (batch_size x n_features): a batch of training examples\n",
    "        This method needs to perform all the computation needed to compute\n",
    "        the output logits from x. This will include using various hidden\n",
    "        layers, pointwise nonlinear functions, and dropout.\n",
    "        \"\"\"\n",
    "        return self.feedforward_nn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the `train_batch()` function. This function will be used to train the model on a batch of data. The train_batch has he following steps:\n",
    "- Setting the stored gradient values to zero.\n",
    "- Computes the gradient of the given tensor with respect to the weights and biases.\n",
    "-  Computes the gradient of the given tensor w.r.t. graph leaves.\n",
    "- Updates weights and biases with the optimizer (SGD of ADAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, y, model, optimizer, criterion, **kwargs):\n",
    "    \"\"\"\n",
    "    X (n_examples x n_features)\n",
    "    y (n_examples): gold labels\n",
    "    model: a PyTorch defined model\n",
    "    optimizer: optimizer used in gradient step\n",
    "    criterion: loss function\n",
    "    To train a batch, the model needs to predict outputs for X, compute the\n",
    "    loss between these predictions and the \"gold\" labels y using the criterion,\n",
    "    and compute the gradient of the loss with respect to the model parameters.\n",
    "    Check out https://pytorch.org/docs/stable/optim.html for examples of how\n",
    "    to use an optimizer object to update the parameters.\n",
    "    This function should return the loss (tip: call loss.item()) to get the\n",
    "    loss as a numerical value that is not part of the computation graph.\n",
    "    \"\"\"\n",
    "    # Forward\n",
    "    print('X shape: {}'.format(X.shape))\n",
    "    output = model(X)  # Computes the gradient of the given tensor w.r.t. the weights/bias\n",
    "    print('output shape: {}, y_shape: {}'.format(output.shape, y.shape))\n",
    "    loss = criterion(output, y) # cross entropy in this case\n",
    "    # Backwards\n",
    "    optimizer.zero_grad()  # Setting our stored gradients equal to zero\n",
    "    loss.backward() # Computes the gradient of the given tensor w.r.t. graph leaves \n",
    "    optimizer.step() # Updates weights and biases with the optimizer (SGD of ADAM)\n",
    "    return loss.item()\n",
    "    \n",
    "def predict(model, X):\n",
    "    \"\"\"X (n_examples x n_features)\"\"\"\n",
    "    scores = model(X)  # (n_examples x n_classes)\n",
    "    predicted_labels = scores.argmax(dim=-1)  # (n_examples)\n",
    "    return predicted_labels\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    \"\"\"\n",
    "    X (n_examples x n_features)\n",
    "    y (n_examples): gold labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_hat = predict(model, X)\n",
    "    n_correct = (y == y_hat).sum().item()\n",
    "    n_possible = float(y.shape[0])\n",
    "    model.train()\n",
    "    return n_correct / n_possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot(epochs, plottable, ylabel='', title=''):\n",
    "    plt.clf()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.plot(epochs, plottable)\n",
    "    plt.grid()\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "X shape: torch.Size([11, 32])\n",
      "y_shape: torch.Size([32, 34]), output shape: torch.Size([11, 11])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (11) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\neural_network_intro.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m X_batch, y_batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# X = batch_size x 11, y = batch_size x 34\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     X_batch \u001b[39m=\u001b[39m X_batch\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m32\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_batch(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         X_batch, y_batch, model, optimizer, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m mean_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_losses)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\neural_network_intro.ipynb Cell 22\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(X, y, model, optimizer, criterion, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m output \u001b[39m=\u001b[39m model(X)  \u001b[39m# Computes the gradient of the given tensor w.r.t. the weights/bias\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39my_shape: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, output shape: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(y\u001b[39m.\u001b[39mshape, output\u001b[39m.\u001b[39mshape))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y) \u001b[39m# cross entropy in this case\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Backwards\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/neural_network_intro.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Setting our stored gradients equal to zero\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (11) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "# Data\n",
    "test_X, test_y = dataset.X, dataset.y\n",
    "# initialize the model    \n",
    "model = FeedforwardNetwork(model_params)\n",
    "# Hyperparameters\n",
    "hyper_params = {'optimizer': 'adam', 'lr': 0.001, 'epochs': 100, 'batch_size': 32}\n",
    "# get an optimizer\n",
    "optims = {\"adam\": torch.optim.Adam, \"sgd\": torch.optim.SGD}\n",
    "optim_cls = optims[\"sgd\"]\n",
    "optimizer = optim_cls(\n",
    "    model.parameters(),\n",
    "    lr=hyper_params[\"lr\"],\n",
    "    weight_decay=0.0)\n",
    "\n",
    "# get a loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training loop\n",
    "epochs = torch.arange(1, 10 + 1)\n",
    "train_mean_losses = []\n",
    "valid_accs = []\n",
    "train_losses = []\n",
    "for ii in epochs:\n",
    "    print('Training epoch {}'.format(ii))\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        # X = batch_size x 11, y = batch_size x 34\n",
    "        X_batch = X_batch.reshape(-1, 32)\n",
    "        loss = train_batch(\n",
    "            X_batch, y_batch, model, optimizer, criterion)\n",
    "        train_losses.append(loss)\n",
    "    mean_loss = torch.tensor(train_losses).mean().item()\n",
    "    print('Training loss: %.4f' % (mean_loss))\n",
    "\n",
    "    train_mean_losses.append(mean_loss)\n",
    "final_acc = evaluate(model, test_X, test_y)\n",
    "print('Final Test acc: %.4f' % (evaluate(model, test_X, test_y)))\n",
    "# plot\n",
    "plot(epochs, train_mean_losses, ylabel='Loss', title='Loss(Epoch)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
