{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models with most Important  Features\n",
    "The aim of this notebooks is to evaluate the impact of training models only with the most important features. The most important features are considered to be the ones that obtain the highest MI scores addressed in the `feature_engineering.ipynb` notebook. For now, only the 4 most important features are considered. The dataset considered will be the one created in the `feature_engineering.ipynb` notebook.\n",
    "\n",
    "The best features for high voltage constraints are:\n",
    "- $cos(\\text{hour})$: Cosine of hour of day.\n",
    "- $R$: Forecasted Irradince, measure in $W/m^2$.\n",
    "- $\\text{last hour mean irradiance}$: Mean irradiance of the last hour.\n",
    "- $T$: Temperatura in Kelvins.\n",
    "\n",
    "After training Linear Regression and Gradient Boost Regression models, the evaluation will be perfomed by using the metric implemented in `ml_hybrid_metrics.ipynb`.\n",
    "\n",
    "**Summary of this article**\n",
    "- Input the exogneous data previously treated and explored.\n",
    "- Train the Linear Regression and Gradient Boost Regression models with\n",
    "    - All the datasets features \n",
    "    - Only with the most relevant features.\n",
    "- Evaluate the models with the metric implemented in `ml_hybrid_metrics.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is the one created in the `feature_engineering.ipynb` notebook. It contains features that are external to the network, such as Temperature and $cos(hour)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "import pandas as pd\n",
    "output = pd.read_csv('..\\data\\ground_truth\\pf_res_bus_vm_pu.csv')\n",
    "# create a timestamps variable and convert it to datetime\n",
    "timestamps = output['timestamps'].apply(lambda x: pd.to_datetime(x))\n",
    "output.drop(['timestamps'], axis=1, inplace=True)\n",
    "output = output.apply(lambda x: (0.95 - x).apply(lambda y: max(0, y)))\n",
    "#Training data\n",
    "_exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_exogenous_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `feature_engineering.ipynb` notebook it was possible to identify the most important features to be used in the model, by using calculateing the *MI scores* of the features. In this present article we will use the 4 most important features to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exogenous_data_most_relevant = _exogenous_data.loc[:, ['T', 'R', 'last_hour_mean_irradiance', 'cos_hour_day']]\n",
    "exogenous_data_complete = _exogenous_data.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning Models\n",
    "In this section both models, Linear Regression and Gradient Boost Regression, will be trained with the different datasets. In terms of train test splitting the dataset, the full dataset will be split into train and test datasets with a 80% and 20% respectively. The train dataset will be suffled for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from thesis_package import aimodels as my_ai, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(exogenous_data_complete, output, test_size=0.2, shuffle=False)\n",
    "# Shuffle the data\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "# Linear Regression\n",
    "X_train['season'] = le.fit_transform(X_train['season'])\n",
    "X_test['season'] = le.fit_transform(X_test['season'])   \n",
    "if 'regressor_all_features.pickle' not in os.listdir():\n",
    "    regressor_all_features = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_all_features.fit(data={'X_train': X_train, 'y_train': y_train})\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = {'n_estimators': 1000, 'learning_rate': 0.1, 'loss': 'squared_error'}\n",
    "    regressor_all_features.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_all_features.fit(data={'X_train': X_train.values, 'y_train': y_train.values})\n",
    "    utils.serialize_object('regressor_all_features', regressor_all_features)\n",
    "else: \n",
    "    regressor_all_features = utils.deserialize_object('regressor_all_features')\n",
    "prediction_lr_all_features = regressor_all_features.predict(data={'X_test': X_test})\n",
    "prediction_lr_all_features = pd.DataFrame(prediction_lr_all_features , columns=y_test.columns)\n",
    "prediction_gb_all_features =  regressor_all_features.predict(data={'X_test': X_test})\n",
    "prediction_gb_all_features = pd.DataFrame(prediction_gb_all_features, columns=y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_all_features._strategy.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Relevant Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(exogenous_data_most_relevant, output, test_size=0.2, shuffle=False)\n",
    "# Shuffle the data\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "if 'regressor_most_important_features.pickle' not in os.listdir():\n",
    "    # Linear Regression\n",
    "    regressor_most_important_features = my_ai.Context(strategy=my_ai.LinearRegressionStrategy())\n",
    "    regressor_most_important_features.fit(data={'X_train': X_train, 'y_train': y_train})\n",
    "    # Gradient Boost Regression\n",
    "    hyper_params = {'n_estimators': 1000, 'learning_rate': 0.1, 'loss': 'squared_error'}\n",
    "    regressor_most_important_features.strategy = my_ai.GradientBoostRegressorStrategy(hyper_params)\n",
    "    regressor_most_important_features.fit(data={'X_train': X_train.values, 'y_train': y_train.values})\n",
    "    utils.serialize_object('regressor_most_important_features', regressor_most_important_features)\n",
    "else:\n",
    "    regressor_most_important_features = utils.deserialize_object('regressor_most_important_features')\n",
    "prediction_lr_most_important_features = regressor_most_important_features.predict(data={'X_test': X_test})\n",
    "prediction_lr_most_important_features = pd.DataFrame(prediction_lr_most_important_features , columns=y_test.columns)\n",
    "prediction_gb_most_important_features =  regressor_most_important_features.predict(data={'X_test': X_test.values})\n",
    "prediction_gb_most_important_features = pd.DataFrame(prediction_gb_most_important_features, columns=y_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Models\n",
    "In this section the results using the two datasets will be compared for both models trained. The TP, TN, FP, FN, Accuracy, Precision, Recall, F1 Score will be plot and compared.\n",
    "\n",
    "The evaluation in done by using the metric implemented in `ml_hybrid_metrics.ipynb`. The threshold percentage for the metrics will be set to 10%, as concluded in the ml_hybrid_metrics.ipynb notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from thesis_package import metrics \n",
    "metric = metrics.Metrics()\n",
    "threshold = output.loc[:, output.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "# Signal the same length of the index of y_test, with a constant value of threshold.\n",
    "threshold_signal = pd.Series(np.ones([2000]) * threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "all_features_prediction = 1\n",
    "best_features_prediction = 1\n",
    "# Function\n",
    "metric.get_prediction_scores(all_features_prediction, y_test, threshold=threshold)\n",
    "tp_all_features, fp_all_features, fn_all_features, tn_all_features = metric.true_positives_ctr, metric.false_positives_ctr, metric.false_negatives_ctr, metric.true_negatives_ctr\n",
    "metric.get_report()\n",
    "tp_rmse_all_features, fp_rmse_all_features, fn_rmse_all_features, tn_rmse_all_features = metric.true_positives_rmse, metric.false_positives_rmse, metric.false_negatives_rmse, metric.true_negatives_rmse\n",
    "# same for best features\n",
    "metric.get_prediction_scores(best_features_prediction, y_test, threshold=threshold)\n",
    "tp_best_features, fp_best_features, fn_best_features, tn_best_features = metric.true_positives_ctr, metric.false_positives_ctr, metric.false_negatives_ctr, metric.true_negatives_ctr\n",
    "metric.get_report()\n",
    "tp_rmse_best_features, fp_rmse_best_features, fn_rmse_best_features, tn_rmse_best_features = metric.true_positives_rmse, metric.false_positives_rmse, metric.false_negatives_rmse, metric.true_negatives_rmse\n",
    "# Create dataframe with the results\n",
    "index = ['all_features', 'best_features']\n",
    "columns = ['tp', 'fp', 'fn', 'tn', 'tp_rmse', 'fp_rmse', 'fn_rmse', 'tn_rmse']\n",
    "results = pd.DataFrame(index=index, columns=columns)\n",
    "results.loc['all_features', 'tp'] = tp_all_features\n",
    "results.loc['all_features', 'fp'] = fp_all_features\n",
    "results.loc['all_features', 'fn'] = fn_all_features\n",
    "results.loc['all_features', 'tn'] = tn_all_features\n",
    "results.loc['all_features', 'tp_rmse'] = tp_rmse_all_features\n",
    "results.loc['all_features', 'fp_rmse'] = fp_rmse_all_features\n",
    "results.loc['all_features', 'fn_rmse'] = fn_rmse_all_features\n",
    "results.loc['all_features', 'tn_rmse'] = tn_rmse_all_features\n",
    "results.loc['best_features', 'tp'] = tp_best_features\n",
    "results.loc['best_features', 'fp'] = fp_best_features\n",
    "results.loc['best_features', 'fn'] = fn_best_features\n",
    "results.loc['best_features', 'tn'] = tn_best_features\n",
    "results.loc['best_features', 'tp_rmse'] = tp_rmse_best_features\n",
    "results.loc['best_features', 'fp_rmse'] = fp_rmse_best_features\n",
    "results.loc['best_features', 'fn_rmse'] = fn_rmse_best_features\n",
    "results.loc['best_features', 'tn_rmse'] = tn_rmse_best_features\n",
    "# Plot the results in a bar chart using seaborn. One subplot with the tp, fp, fn, tn and the other with the rmse.\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "sns.barplot(x='index', y='tp', data=results, ax=axs[0])\n",
    "sns.barplot(x='index', y='fp', data=results, ax=axs[0])\n",
    "sns.barplot(x='index', y='fn', data=results, ax=axs[0])\n",
    "sns.barplot(x='index', y='tn', data=results, ax=axs[0])\n",
    "sns.barplot(x='index', y='tp_rmse', data=results, ax=axs[1])\n",
    "sns.barplot(x='index', y='fp_rmse', data=results, ax=axs[1])\n",
    "sns.barplot(x='index', y='fn_rmse', data=results, ax=axs[1])\n",
    "sns.barplot(x='index', y='tn_rmse', data=results, ax=axs[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
