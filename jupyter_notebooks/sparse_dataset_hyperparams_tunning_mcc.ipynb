{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Target Dataset Hyperparameters Tunning\n",
    "The objective of this notebook is to tune the hyperparameters of the model to obtain the best performance on the sparse dataset. \n",
    "\n",
    "**Summary of the Article**\n",
    "- Description of the dataset.\n",
    "- Hyperparameters tunning:\n",
    "    - Gradient Boosting Regressor.\n",
    "    - SVRegressor.\n",
    "    - Multi-Layer Perceptron.\n",
    "    - Long-Short Term Memory.\n",
    "- Training Models.\n",
    "- Next Steps.\n",
    "\n",
    "## Description of the Sparse Dataset \n",
    "The objective of this master thesis is to forecast the occurance and amplitude of constraints in the electrical grid. Using historical values of active and reactive power it is possible to compute the voltage and current values in the network, thus obtaining the occurance and amplitude of constraints. Since not every timestep containts a constraint, not every time step is woth to be predicted, so it is usefull transform the target features into a sequece of values that better represent the constraints. One way to obtain this target dataset is to set all time steps that do not characterise a constraint to 0, a positive value (with the amplitude of the constraint) otherwise. The following formula states the transformation:\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\text{Target} &= \\begin{cases}\n",
    "            0 & \\text{if} \\; \\text{constraint} \\; \\text{is not violated} \\\\\n",
    "            \\text{amplitude of constraint} & \\text{if} \\; \\text{constraint} \\; \\text{is violated} \\\\\n",
    "        \\end{cases}\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "The resulting dataset is a sparse dataset, since constraints are not as common as regular values. Bellow the dataset for maximum voltage constraints is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy' has no attribute '_lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\sparse_dataset_hyperparams_tunning_mcc.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/sparse_dataset_hyperparams_tunning_mcc.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m; sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/sparse_dataset_hyperparams_tunning_mcc.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthesis_package\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/sparse_dataset_hyperparams_tunning_mcc.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jamil/Documents/IST/Thesis/new_thesis/code/AI-to-forecast-constraints-in-the-energy-systems/jupyter_notebooks/sparse_dataset_hyperparams_tunning_mcc.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\jupyter_notebooks\\..\\thesis_package\\utils.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m shuffle\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\__init__.py:82\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __check_build  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     85\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     86\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\base.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_tags\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     _DEFAULT_TAGS,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m check_X_y\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\utils\\__init__.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_version, threadpool_info\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     as_float_array,\n\u001b[0;32m     33\u001b[0m     assert_all_finite,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     check_scalar,\n\u001b[0;32m     42\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\utils\\fixes.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreadpoolctl\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\stats\\__init__.py:453\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    450\u001b[0m \n\u001b[0;32m    451\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 453\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    454\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    455\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:44\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[1;32m---> 44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     47\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\stats\\distributions.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m (comb, chndtr, entr, xlogy, ive)\n\u001b[0;32m     22\u001b[0m \u001b[39m# for root finding for continuous distribution ppf, and max likelihood\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# estimation\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m optimize\n\u001b[0;32m     26\u001b[0m \u001b[39m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m integrate\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\optimize\\__init__.py:399\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m=====================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mOptimization and root finding (:mod:`scipy.optimize`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \n\u001b[0;32m    397\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    400\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_minimize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    401\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_root\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\optimize\\_optimize.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m (atleast_1d, eye, argmin, zeros, shape, squeeze,\n\u001b[0;32m     31\u001b[0m                    asarray, sqrt, Inf, asfarray)\n\u001b[0;32m     32\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearOperator\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_linesearch\u001b[39;00m \u001b[39mimport\u001b[39;00m (line_search_wolfe1, line_search_wolfe2,\n\u001b[0;32m     35\u001b[0m                           line_search_wolfe2 \u001b[39mas\u001b[39;00m line_search,\n\u001b[0;32m     36\u001b[0m                           LineSearchWarning)\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_numdiff\u001b[39;00m \u001b[39mimport\u001b[39;00m approx_derivative\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py:120\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mSparse linear algebra (:mod:`scipy.sparse.linalg`)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m==================================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_isolve\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_dsolve\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    122\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_interface\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mIterative Solvers for Sparse Linear Systems\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39m#from info import __doc__\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39miterative\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mminres\u001b[39;00m \u001b[39mimport\u001b[39;00m minres\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlgmres\u001b[39;00m \u001b[39mimport\u001b[39;00m lgmres\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\iterative.py:150\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[39mreturn\u001b[39;00m fn\n\u001b[0;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m combine\n\u001b[0;32m    128\u001b[0m \u001b[39m@set_docstring\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mUse BIConjugate Gradient iteration to solve ``Ax = b``.\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    129\u001b[0m                \u001b[39m'\u001b[39;49m\u001b[39mThe real or complex N-by-N matrix of the linear system.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[0;32m    130\u001b[0m                \u001b[39m'\u001b[39;49m\u001b[39mAlternatively, ``A`` can be a linear operator which can\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[0;32m    131\u001b[0m                \u001b[39m'\u001b[39;49m\u001b[39mproduce ``Ax`` and ``A^T x`` using, e.g.,\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[0;32m    132\u001b[0m                \u001b[39m'\u001b[39;49m\u001b[39m``scipy.sparse.linalg.LinearOperator``.\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    133\u001b[0m                footer\u001b[39m=\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[39m               Examples\u001b[39;49m\n\u001b[0;32m    136\u001b[0m \u001b[39m               --------\u001b[39;49m\n\u001b[0;32m    137\u001b[0m \u001b[39m               >>> from scipy.sparse import csc_matrix\u001b[39;49m\n\u001b[0;32m    138\u001b[0m \u001b[39m               >>> from scipy.sparse.linalg import bicg\u001b[39;49m\n\u001b[0;32m    139\u001b[0m \u001b[39m               >>> A = csc_matrix([[3, 2, 0], [1, -1, 0], [0, 5, 1]], dtype=float)\u001b[39;49m\n\u001b[0;32m    140\u001b[0m \u001b[39m               >>> b = np.array([2, 4, -1], dtype=float)\u001b[39;49m\n\u001b[0;32m    141\u001b[0m \u001b[39m               >>> x, exitCode = bicg(A, b)\u001b[39;49m\n\u001b[0;32m    142\u001b[0m \u001b[39m               >>> print(exitCode)            # 0 indicates successful convergence\u001b[39;49m\n\u001b[0;32m    143\u001b[0m \u001b[39m               0\u001b[39;49m\n\u001b[0;32m    144\u001b[0m \u001b[39m               >>> np.allclose(A.dot(x), b)\u001b[39;49m\n\u001b[0;32m    145\u001b[0m \u001b[39m               True\u001b[39;49m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[39m               \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\n\u001b[0;32m    148\u001b[0m                )\n\u001b[0;32m    149\u001b[0m \u001b[39m@non_reentrant\u001b[39;49m()\n\u001b[1;32m--> 150\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mbicg\u001b[39;49m(A, b, x0\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, tol\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m, maxiter\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, M\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, callback\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, atol\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m):\n\u001b[0;32m    151\u001b[0m     A,M,x,b,postprocess \u001b[39m=\u001b[39;49m make_system(A, M, x0, b)\n\u001b[0;32m    153\u001b[0m     n \u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(b)\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\_lib\\_threadsafety.py:57\u001b[0m, in \u001b[0;36mnon_reentrant.<locals>.decorator\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m     55\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not re-entrant\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m     56\u001b[0m lock \u001b[39m=\u001b[39m ReentrancyLock(msg)\n\u001b[1;32m---> 57\u001b[0m \u001b[39mreturn\u001b[39;00m lock\u001b[39m.\u001b[39;49mdecorate(func)\n",
      "File \u001b[1;32mc:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\scipy\\_lib\\_threadsafety.py:45\u001b[0m, in \u001b[0;36mReentrancyLock.decorate\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m     44\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m scipy\u001b[39m.\u001b[39;49m_lib\u001b[39m.\u001b[39mdecorator\u001b[39m.\u001b[39mdecorate(func, caller)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy' has no attribute '_lib'"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from thesis_package import utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns='timestamps')\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "X_max_u_train, X_max_u_test, y_max_u_train, y_max_u_test = utils.split_and_suffle(exogenous_data, y_max_u)\n",
    "data = {'X_train': X_max_u_train, 'X_test': X_max_u_test, 'y_train': y_max_u_train, 'y_test': y_max_u_test}\n",
    "threshold_value = y_max_u_train.loc[:, y_max_u_train.max(axis=0) != 0].max(axis=0).mean() * 0.1 \n",
    "threshold_signal = pd.Series(np.ones([len(y_max_u_test)]) * threshold_value)\n",
    "# Plot prediction_gb_max_u\n",
    "sns.set(style='whitegrid')\n",
    "fig, axs = plt.subplots(1, 1, figsize=(30, 10))\n",
    "axs.plot(y_max_u_test.reset_index(drop=True))\n",
    "axs.plot(threshold_signal)\n",
    "axs.set_title('Dataset Sample of Maximum Voltage Constraint', fontsize=30, fontweight='bold')\n",
    "axs.set_xlabel('Timestep', fontsize=18, fontweight='bold')\n",
    "axs.set_ylabel('Constraint Value', fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse boolean dataset is derived from the one above. It represents the time steps with constraints as of the class 1 and the rest as class 0. This datset is used to train the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABscAAAJzCAYAAABTdM3PAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAADiOElEQVR4nOzdd5wURd7H8W9vICw5IyA5SpAsYkaUJCgIomJAfVAMp55nRs9T70S9Oz3TnWIOGBARAzmJICgIkkUk513SAgtsmunnj2XH7ZmeZXp6dmdn5/N+Xo/H9E51V3dXV/XUr6vaME3TFAAAAAAAAAAAABAHEqKdAQAAAAAAAAAAAKC4EBwDAAAAAAAAAABA3CA4BgAAAAAAAAAAgLhBcAwAAAAAAAAAAABxg+AYAAAAAAAAAAAA4gbBMQAAAAAAAAAAAMSNpGhnAACAkqpXr17atWtX0L8nJCSobNmyqlatmho3bqxevXrpyiuvVEpKSrHl0TRNLV++XF26dCm2bUbbzz//rK5du7pez6pVqzRjxgz9+OOPSktL06FDh5ScnKxKlSqpRYsW6tKliwYPHqzTTjstArkuXVq1amX5PGfOHDVo0CBKuYmM7Oxsffjhh5o6daq2b9+uzMxMVatWTS1atNCwYcPUt2/fU67j+uuv15IlSwKWN27cWDNmzAia7tFHH9UXX3xh+7fffvst9J0oJq+88opeffVV3+fBgwfr2WefjWKOSo9+/fpp8+bNvs+nKjsFDRs2TKtWrfJ9btOmjSZPnhyRfPmX7bFjx2rIkCFBvx+pero0Sk9P19SpU/X9999r48aNOnDggDwej6pWraoWLVqoR48euvLKK1W9evVoZzWqfvvtN9WrV0+VKlWKdlYcl/9I2blzpxITEyN2H7Jq1SrNnTtXP/74o1JTU3Xw4EElJCSoevXqaty4sXr27Kk+ffrEfHsOAAAQKoJjAACEyev16sSJEzpx4oR2796tRYsW6dVXX9Vzzz2nCy+8sMi3v3LlSj399NMqX768PvzwwyLfXrSlpqbq+eef17fffusqYHDkyBH97W9/05QpUwL+lpOTo+PHjys1NVULFy7Uf//7X91666266667lJDAgPvSyuPx6Oabb9bSpUsty1NTU5WamqpOnTq5Wv/WrVuVmpqqOnXq2P79xx9/dLV+lB4DBgzQK6+84vu8detWrV+/Xq1bty403Z49e7R69WrLsssuu6xI8liYSNXTpZHX69Xbb7+tN954Q0ePHg34e359s3DhQr366qsaPXq0brvttrhrew4fPqyXX35Zn3zyiWbOnFkigmPFLTMzU+PGjdNbb72lN99803VwbPPmzXrqqae0ePFi278fP35cO3fu1MKFC/Xiiy9qyJAhuv/++1W5cmVX2wUAACjp4utOGwCAIpaenq7bb79dX375ZZFt4+DBg3rkkUc0fPjwgM7Q0ig3N1dvvPGG+vbtq2+//dbVurKysnT77bfbBsbs5OTk6LXXXtPTTz/tarso2ebOnRsQGCvoVIGJUATrlNyxY0ehI1QRX+wCWtOmTTtlupkzZ8o0Td9nwzA0YMCAiOatMJGsp0ujEydO6E9/+pP+9a9/2QbG/GVmZuo///mP7rjjDuXk5BRDDkuGTz/9VH369NFHH30kj8cT7exExaxZs9S/f3+99tprysrKcr2+H3/8UcOHDw/aBvnLycnRZ599pmHDhmnLli2utw8AAFCSMXIMAIAQXXzxxerdu7ekvJEmJ06cUFpamn744QetW7fO9z2v16u//e1vatu2rVq2bBnxfHz33XeaNGlSxNdbUu3du1cvvPBCRNb14Ycf6ueff7Ysa9mypXr16qU6deooIyND69at08yZMy0dcx9//LH69OmjHj16RCQfKFlWrlxp+VyxYkXdcsstqlq1qlJTU9WxY0fX21i8eLGuuOKKgOWxOGqsd+/eql+/vu9zw4YNo5ib0qVx48Zq27at1q5d61s2ffp0/fnPfy403axZsyyfu3TpUqxTwkayni6NxowZo9mzZ1uW1apVS3379lXTpk115MgR35R3BYOc8+bN08svv6y//OUvxZ3lqHjiiSeinQVbo0aN0uDBg32fi3Iq67Fjx0bsgYkNGzbojjvu0LFjxyzL69Wrp969e6tx48byer3asmWLZs6cqX379vm+s3XrVt10002aNGlS3E/xCQAASi+CYwAAhKhNmza275i4//77NXXqVD344IO+J7wzMzP19NNPx8V0h7Hk448/tny+4447dM899wR8b8mSJbrpppuUm5vrWzZhwgSCY6XUkSNHLJ+vuOIK3XHHHa7Xm5CQIK/XKyn4yLGCwbHExMSYGC3Rpk0btWnTJtrZKLUGDhxoCY5t3bpVv/76a9BjfuDAAS1btsyyrDhHjaFwkyZNChitPHDgQD311FMB7yj95ZdfdOutt1rqpHfffVfXXnst77+MovPPPz/aWXAsNzdX9957b0Bg7K677tLo0aOVnJxsWf7ggw/qX//6l+W+dc+ePXrooYf05ptvFkueAQAAihvTKgIAEAH9+/cPCLIsWbLEMqIM0ZWRkRHwNPagQYNsv9u9e3f17dvXsmzVqlVFljdEV3Z2tuVz1apVI7LegsGM1NRUbd68OeA7P/30k+/fkZi+EbGvf//+Ae+Zmj59etDvz5492xeElaSkpKSA+gvR4fV69b///c+yrHv37nr++ecDAmOS1KlTJ/3rX/+yLMvJydEXX3xRpPlE6TNhwgRt2rTJsuxPf/qT/vSnPwUExiSpXLlyeuyxxzR8+HDL8u+//17Lly8v0rwCAABECyPHAACIkBEjRuj1119XRkaGb9m0adN0xhlnBHz3yJEj+uSTTzR//nxt3rzZl6ZSpUpq1KiRevbsqRtvvFFVqlTxpfnpp590ww03BKxryZIlatWqlSRp8ODBevbZZy1/37Jliz755BMtWbJEO3bs0IkTJ5SYmKhq1aqpRYsWuvTSSzV06FAlJiba7tfOnTv12Wef6aefftK2bduUkZGhxMREVa1aVS1atFCvXr00dOhQlS1bttDjs3HjRn344YdavHixUlNTlZiYqLp166pHjx665ppr1KJFi4A0+fsVbHn9+vU1d+7cQrebzz8AIkmTJ08OOl3ZqFGj1L59e9/nwvZv8eLFmjRpklatWqW0tDRlZmaqXLlyqlmzptq2bauhQ4fq3HPPDUg3adIkPfLII77Pjz/+uK677jotXbpU77//vn755RdlZGSofv366t+/v0aNGuXLx5YtW/TWW29p4cKFOnTokGrWrKkePXrolltuUbNmzQK29corr+jVV1/1fX7hhRc0YMAAfffddxo/frzWrl2rY8eOqV69err44ot1yy23qFq1akH3ORT5nbpTp07Vhg0blJGRoapVq6pVq1a69NJLNWTIENtOOqeOHTumqVOnatq0adq6dav27dunlJQUNWzYUOecc46uueYa1alTJyBdr169bKevevXVV33Hyu6aClW3bt0sI4AWL16spk2b+j5v3LjRMo3VWWedZfl+YcIpc6+++qpeeeUVy7I//elPuuuuuyzL/vvf/+qll16yLHv55ZfVp08fSYFlyf8YFWe59t9W9+7dbUfsnup7/vXrzTffrIceekjr16/XO++8ox9//FHp6emqU6eOevfurdGjR/vq57S0NL311luaM2eO0tLSVL16dXXq1Ek333yzOnToEJCXU6lTp466du2qJUuW+JYVNrXizJkzLZ979uxpOw3a0qVLNWnSJK1Zs0a7d++W1+tVrVq11KVLFw0aNEhnn32247xK4dXT4bSB/rKysvTxxx9r6tSpvsBz8+bNdeWVV2ro0KH67bffLFOZFnYtz58/X1988YVWrFihgwcPqkKFCmrUqJHOP/98XXvttWFPK7dgwQJt377dsuyBBx4ICH4WdMEFF6hFixYyDEPdu3dX9+7d1a1bt6DfP3TokL7++mvNnDlTu3bt0v79+1W5cmU1adJEF1xwgYYPHx70OPqX+xEjRuivf/2r9uzZo/fee0/z5s3T3r17lZKSojPPPFPDhw9Xr169guZlw4YNmjBhgpYvX64dO3bo+PHjSkpKUo0aNdSqVSv16dNHAwcODLjfCFYXX3zxxb5///bbb0HzfN999+n555/XjBkzdOLECdWsWVODBg3Svffe6/teamqqPv74Yy1atEjbtm3TsWPHZBiGqlatqiZNmujCCy/UiBEjVK5cuYB8XH/99ZbrcezYsZaZBPzrlwcffFC33HKLNm3apPfee08//PCD9u3bpypVqqhLly66/vrr1bVr10K3ka/gvs6ZM0cNGjQI+I6dTz75xPK5UaNGGj169CnTPfzww5o+fboOHz7sW/b++++rc+fOtt9PS0vT+PHj9cMPP2jHjh3KyMhQhQoV1LRpU5133nm65pprTnn9/Pjjj5o8ebKWLVum/fv3Kzc3V9WrV1fbtm112WWXqV+/fjIMIyCdf90T7Pic6nv+x37x4sVavXq1XnrpJf3+++8qX768mjZtqmeeeUZNmza1rK9Fixb65ptv9M4772j8+PFKS0tTlSpV1L59e7322muWsh7OfbAUmfLl78iRI/r00099dfCRI0dUrlw5NWrUSGeffbauvfZay/TJ/kzT1JQpU/TNN99ozZo1Onz4sCpVqqRmzZqpV69eGj58uCpUqBA0/YEDBzRhwgQtXLhQmzdv9r2LsUqVKmratKnOP/98XX311apUqVKh+wEAgFsExwAAiJCUlBT17NnT0lG5YsWKgO+tW7dOt956q6VTPN/Bgwd18OBB/fLLL5owYYLGjRtnG1wL1eTJk/XYY4/5pnvM5/F4lJqaqtTUVC1cuFCff/653n333YAfod98840ee+wxZWZmWpbn5uZa0r///vt66623gr576H//+59eeeWVgCnjNm3apE2bNunTTz/VqFGjTvleHTeqVaumGjVq6MCBA75lr7/+uhYvXqwrr7xSF110kWrXru37W+vWrU85ksc0TT3xxBP67LPPAv52/Phxbd++Xdu3b9e0adM0fPhwPfXUU6fM5wsvvKBx48ZZ3juzadMmvfLKK5o3b54++ugjzZ07V2PGjNGJEyd839m1a5e++OILTZs2Tf/73/9OOQWk1+vVY489ps8//9yyfPPmzdq8ebM+//xz/e9//wvaIXYqO3bs0OjRo7Vx40bL8n379mnfvn1auHCh3nnnHf33v/+1DXqEasGCBXrooYcs51XKC4amp6dr1apVeuedd3Tfffdp5MiRYW8nHF27dtUHH3zgG9Xz448/asSIEb6/+79vrFu3bnrnnXcKXaebMjd69Gh99913Wr16tW/ZG2+8oQEDBqhJkyaSpN9//13//e9/Leu94oorfIGxcBVXuY6k8ePHa+zYsZb6c/v27XrnnXc0a9Ysffrpp9q4caPuuecepaen+76zd+9eTZs2TbNnz9azzz6ryy67zPG2BwwYYOmsDTa14pEjRyyjDyUFbG///v166KGHtHDhwoDtbNu2Tdu2bdOkSZN0/vnn65///GfERk4GE4k2cMeOHRo1apS2bNliWb5ixQqtWLFCM2bMCAj62jlx4oQeeOCBgHe2paenKz09XStXrtS7776rZ599VpdcconDPQ2cTrV+/fohBUy//PLLkB4cmDx5sp588kkdP37csvzAgQM6cOCAfv75Z7355pt64oknQi6H8+fP1/3332+Z2jErK0vfffedvvvuO1133XV6/PHHA9K99dZbeuGFFwLa+dzcXO3atUu7du3S3LlzNX78eL3xxhsRe49VVlaWbr75Zsu7I3ft2mUZmbdw4ULdfffdAVMMSn+0SUuWLNFnn32m9957T/Xq1XOdry+++EJPPvmksrKyLNuaPn26pk+f7gtwFIXU1FRt2LDBsmzIkCFKSjp1909KSor69+9vCa79+OOPMk0zIEA1ceJEPf300wH3iIcPH9Yvv/yiX375Re+//75efPFFnXPOOQHbOnbsmB5++OGAAL+UV4/u3btXc+bM0ccff6xXX321yOumfLNmzdITTzzha6+ys7O1adOmoIHJf/zjH5aHLfIDfAUDY5G+D3ZTvubPn68HHnjAEgCV8mZYWLt2rdauXauPPvpITz75pO27Ug8ePKi77rorYDrf/Pp76dKleuedd/Tyyy/b3kMuXrxY99xzT8D2pbxjt3//fi1ZskTvvvuuXn/99bAeMgEAIFRMqwgAQAT5d/L7jwLJyMjQ7bffbtsp6G/fvn265557LNNlObF69WqNGTMmIDAW7LvPPPOMZdn69ev18MMPB3R62Nm2bZtGjx5tOzrrv//9r/7zn/8U+i4lj8ej119/Xf/+979Pua1wGYZh+yN/5cqV+utf/6rzzjtP/fv311NPPaXZs2cHdDbaef/9922DFHY+++wzffnll4V+58MPP9Qbb7xhCSAUtGbNGt1666168MEHLQGEgo4fP65HH33U0mFiZ9y4cQGBsYLS09N1yy23BHQ+h+LAgQO69tprAwJj/rZu3aoRI0YoNTXV8TYk6euvv9aoUaMCAmP+srKyNHbs2LBHgIWrcuXKatmype/zTz/9ZLmeC3acJyQknPJJb8ldmUtKStLzzz9vGRmRnZ2tJ554QlLedfjII49Y6oz69evbdoQ7UZzlOlLmzp2rp556Kmj9uWPHDt1xxx26/fbbLYGxgnJycvTEE09o//79jrfft2/fgODItGnTbPNZMI/lypVT7969fZ8PHDigK6+80jYw5u/777/X8OHDdejQIcf5DVUk2sCMjAyNHDmy0Lpp4cKF+utf/1ro+r1er+68886AwJhdnu+++27Nnz//lHn298svv1g+h/qwSyiBsddff10PPfTQKduqI0eO6C9/+UtI70Bds2aN/vSnPwW8h7Ggjz76KCDo99133+mf//xnSO9MXLVqle67775Tfi9UX331lSUwJuW19/nBwN27d+uee+6xDYz527Ztmx566CHXeZo3b54ee+yxQuurf//737ZT7UaC3ZTeobQv+fwDGunp6QF5/fzzzzVmzJhT3iMePnxYd9xxh2/kXz6Px6PRo0fbBsb8LV26VHfddVfY98NOPf300wHt1SWXXKIyZcoEfHfr1q2211bBYHSk74PdlK8FCxZo9OjRtoGpgjIzM/XII48EtB2ZmZm68cYbAwJj/vbt26eRI0fq119/tSxPTU3VXXfddcrtS3nt1+23316kbRIAAIwcAwAggmrWrGn5fOzYMeXm5vqe1v3kk0+0d+9e398rVqyoq666Sk2aNNGJEyf0/fffW36Ibt++XRs2bFDr1q3VtGlTjR07VsuWLdPEiRN932nSpIluvfVWSbKM3Hr55ZeVm5vr+9ywYUMNGTJENWvW1MGDB/XNN9/o999/9/19zpw5lrx//PHHlvT169fXZZddptNOO005OTlavny5pk+f7utA2LRpk7766isNGzbMl2b9+vWW6dckqWPHjrr44otVpkwZ/fTTT5bptt566y1deumlvukMx44dq0OHDun555+3rGPs2LGSZPvOlsLccccdmjt3btBO1fwneMePH6/y5cvr0ksv1ahRo2ynusnKygp4l0y3bt3Uu3dvpaSkaMeOHZowYYKl43zOnDkaPHhw0Pxt3bpVUt60aBdddJFOnDih9957TwcPHvR9J380Sc2aNTV8+HBVqlRJX3/9taUzbNeuXfrpp590/vnnB91W/lPlHTt29HXEz5kzR4sWLfJ95/jx4/rrX/8aUqdqQU8++aTS0tJ8n8uXL68rr7xSTZs2VVpamiZOnOgLGBw6dEhPPPGEXn/9dUfb2Lp1q+XJbimvPFx55ZVq0aKF0tLS9OWXX1qm6nr33XfVvn17DRgwQFLe1ETHjx/X559/bnmnysUXX+wLMAQbDRmq7t27a/369ZLyOgnXrVundu3ayev1aunSpb7vtW7dWpUrVy50XZEoc02bNtUDDzygp59+2rfsp59+0pdffql9+/ZZRpUlJCTo2WefVcWKFcPa93zFWa4jJT/PBcvL+PHjtWPHDt938jvk8+vx0047TbNmzbKM+MrIyNCsWbN0zTXXONp+1apVdc455+i7777zLZs+fXpAUMG/Y/miiy6yTGX14IMPWtocSTr33HN14YUXSsobQbBgwQLLft9///16++23Q86rk3raTRuY77XXXtPOnTst22rdurUGDRqkcuXKacGCBZo3b17AyBl/H3/8sX744QffZ8Mw1KdPH3Xu3FmZmZmaNm2ar2PX6/Xq0Ucf1ezZs1W+fPlQD03AsT/ttNNCTluYn3/+WS+//LJlWfXq1XXllVfq9NNP186dOzVx4kTLNTZ27FidccYZ6tKlS9D15pfpRo0a6fLLL1fVqlU1a9asgGDY119/bZmG07+NaN68ufr166datWrp+PHjWrhwoeW8Ll68WIsWLVLPnj0l/VEXF5w6Ln/5qab3zQ8OX3TRRTrvvPO0ZcsWbdmyxTf6a9y4cZbprmvWrKmrrrpK9erV09GjRzVjxgzLKP+lS5fq6NGjrqZzy6/b27Rpo/79+6tcuXKaPHmy5YEpj8ejqVOn+kY4jho1SoMHD9Zzzz1nqcNHjRrlm4431KmO7aaozB8dHIrGjRsHLEtLS/M9ALZ3715LGyJJtWrV0pVXXqnTTjtNGzdu1MSJE30PO2RmZuof//iHPvjgA9/333vvvYBpJPPbM8MwAuqCpUuX6uuvv7Z9yCnScnJyVK1aNV133XWqUKGCpk+froEDBwb9riS1bNlSV155pTIyMvTNN9/47iHc3gfbCad8SXkjZR966CFLkLFSpUoaOnSoGjdurF27dlnuIbxer55++mlNnTrVNwrupZdestStycnJGjhwoNq2bav09HR99dVXvqlks7Ky9NBDD+nrr7/2ff+rr76yXI/Vq1fXoEGDfPdaa9eu1ddff+07rvv379d7771XpDNLAADiG8ExAAAiyO69VEeOHPFNH5SQkKBOnTppw4YNOnbsmF5++WXLVDM33HCDLr/8cssTtjt37lTr1q1Vq1Yt3zsuCgbHCi7P5/V6VbNmTbVq1UqbN29W+fLl9emnn6pGjRq+7wwdOlTnnXee70nWw4cPWzqE/DtXXn75ZbVr186S1zfeeENvvfWWmjVrpmbNmgVMk/T2229bnpS98sorLSPURo4c6ZuyKj/fH374oa+TdciQIdq5c2dAp6v//oaqYsWKeu+993TXXXdZggB2Tpw4oa+++kpTp07Vfffdp5tvvtny99TUVHXt2lW//fabdu7cqS5duuj999+3TKPTrl073X333b7P/h26dvLf+ZKvWbNmuvPOOy3fqVWrlr744gvfe7SGDx+uiy66yNKhtnXr1lMGEYYNG6ann37aN1XSddddp3/961968803fd9ZsmSJ1q9ff8opJvNt27bN0mGfkpKizz//XM2bN/ctu+mmmzRkyBBfGfvuu++0fft2R4GoN954wzJiolatWho/frwaNWrkW3bLLbfotttus3TA/fvf/9all16q5ORk9e3b17ePBYNjbdq0CbuM+evevbulQ3Dx4sVq166d1q1bZ3lyurB3CuWLVJkbMWKE5s6dawkMPPvsswGjtkaOHKnu3buHtqOnUJzlOlJ69eqlV1991Xd8e/ToEdAxm1+35gfQr732Wg0ePNjSeZgfaHMq/52A+bZt26Z169b5Rh8dO3Ys4Kn+gqMVlixZEvD3v/71r5apPa+//nqNHz/eMv3mwoULLYGLU3FST7tpA6W8kY4F2z8pLyjy6quv+h5CGTFihN555x0999xzQfPs9XoDpjB99tlnLed31KhRuuuuu3wPjuzfv19TpkzR0KFDCzscFv4jsJw+0BHMSy+9ZGlbmzdvrg8++MDSxo8cOVI33HCDbwSvx+PRv/71r4B3Ufnr2rWr3n77bd8I02uuuUYjR460TN+5adMmSxr/euajjz6yBHJuuukm/fWvf9XMmTPVvHlzNWvWzDI6Lr8u9g+O9enTJ6R3bF1wwQWWBywKPjRRvnx5tW3bVps2bVJubq4++OADyyj/a6+9Vr169fKNQDZNU7t27Qq5zQumX79+euGFF3zvl7v66qs1aNAgy8M5BY9jfr326quvWuq88847T2eddZajbRcMPuRzEuyz+27BPH3wwQeWUUv169fX559/bil/ffr00XXXXef7/NNPP2nz5s1q2rSpPB6P3n33Xcv6r732Wt8oZkm68cYb9eijj+qLL77wLfvkk0+KJTgm5QVV86fzu+mmm4KOfJak2rVr6+OPP/YdtzvvvNN3X+X2PjgYp+VLynt3WcGR9pUqVdLEiRMtwdDBgwdryJAhvvuBrVu3atGiRTrvvPN09OhRffrpp77vJiYm6q233rJMd3zrrbfqmmuu0Zo1ayTlvStw8eLFvmC6f13xt7/9LWDa5p49e+rxxx9X06ZN1bx5c9tgLQAAkUJwDACACLJ7YXjBKa9uueUW3XLLLTJNU3v27Al4r0VOTo7q1q1r6RgMZXo/fwkJCb6n9nNycnTw4EFLp4WUN8ValSpVLE+WHz9+3PfjPr+DOt+zzz6rm266ST169PCNTLj11lt122232ebBNE19//33lmV2L4O/7rrr9O9//9t3nObNm2f7botIqVu3rj777DN9++23Gj9+fMB0TP5ycnL03HPP+UY45GvYsKFee+01SXmd1NnZ2ZYghaSAY36qc5mUlKR77rnHsszufQ0jR460nJ+UlBS1a9fO0hFu1zlWUKVKlTRmzJiA43zvvfdq8uTJlmnPFi9eHHJH4fz58y2dSL1797YExqS8UTFDhgzRK6+8IimvrMydOzfkd4IdO3ZM06dPtyx75JFHLIExKe+4PP/88+rdu7dvFOSuXbu0bNmyYnt3VdeuXWUYhu+Y/Pjjjxo1alTA+8ZCCUJFqswZhqGxY8dq4MCBvgCd/9SALVu2jNiT2sVZriPp3nvvtRzfNm3aKCUlxXJMBw8ebBlZmpSU5Av+5As3z71791b58uUtQcvp06f7gmPz58+3dFBXrlzZEjgs2Kks5Y0YKxgYy5cfLC14nL/66quQg2NOuG0D165dawk4GYahxx57LOBdSjfffLO++uor36hNfxs2bLA8AFK3bt2ATveEhASNHDnSMqp6zpw5joJj/tNyRmJauB07dlhGnUp57zzyv/Zr1Kihf/zjHxo+fLhv2fLly7Vjxw6dfvrpQdf/wAMPWKZeTUhI0IABAyzBMf+gX926dS1B4Mcee0wjRoxQ165dfVPRPfnkkyG9dzMcBYMwkvVeLH+aRI/Ho7S0tIDRe/kPExUMGoQyBWNhDMPQI4884gtcSFKZMmV06aWX6o033vAtK2z6SjcKbjccduW0YLvuf293yy23BJS/bt26aeTIkapYsaJat26tVq1a+crdunXrLPcYycnJtu3NqFGjJMmX3m3AMlRnnnlmwHuuCrsnvfzyyy0BxfzvFtV9cLjlyz8vV111VUDgqWnTprrtttuUmZmpVq1aqVWrVr7vLFmyxFIfd+zYMeBeqkyZMrruuuv08MMP+5bNmTPHFxzz/23x2muvKTc3V+ecc47vnXKXXXaZBgwYUGS/AwAAKIjgGAAAEWQ3/7/dVGmGYahevXo6fPiwVq5cqTVr1mjlypVaunRpQKdMKO/wKExycrLq1Kmj3bt3+7a1fPlyrVq1yjJtoiTL5+HDh+vLL7/0dZIsXbpUS5cuVVJSktq0aaPOnTure/fu6tGjh+20azt37gzocB81apTtj92CnS5HjhzR9u3bAwIdkZSYmKjLL79cl19+uXbu3KkFCxZo8eLFWrJkSdB3Gzz33HMaMGCAZcqyfBUqVFD58uW1YcMGrVq1SqtXr9bPP/8c8M4t/+Ptr0GDBqpSpYplmd0T3B07dgxY5v+i+lOVm65du9pOD5aUlKSzzz7bMg3OqaYnK8j/PXvff/+9b1RAQf4Bg/ynjEOxceNGSwdNcnKy5T1LBZ122mnq2LGjfv75Z9+y4gyOVatWTS1atPAdw2XLlik7O9sSHDMMw9H7YCT3Za5OnTp64oknbN/9k5ycrH/+85+271cJR3GW60gpU6aM5X1x+SpWrGgpe2eeeWbAd/ynPgs3zykpKbrooos0depU37KCUyv6vyvL/504q1atsvy9X79+QbfVr18/S3DsVO+TcSvcNrDgVMBS3vR/wUYWXXjhhUGDY/711KFDh2zrKf/rxz/dqVSpUsXyzrlwHnbxt3r1akubmV/H2enYsaNOO+007dmzx7ds2bJlQYNjSUlJttO51a5d2/LZ//2i11xzjaVOmz17tmbPnq2yZcuqXbt26tKli7p166YePXpErF4pyD+QYScxMVGnnXaa9u/frxUrVmjNmjVasWKFli9fHnDv5raeqVevXkAQQDr1cYwU//pWyru38p/6Oxi7oF1+vZaTkxPQztg97CAFjgTM539dNmvWzPZeuUmTJgHvwy0OoZSnUL5fVPfB4ZYv//d/BTtvt99+u+1y//pv/fr1tvWm/3voCqa7/PLL9dZbb/nqwt9++0333XefEhIS1KJFC3Xq1Endu3dXz549Q55GFAAANwiOAQAQQQWfPJbyOlj9AxDHjx/X+PHj9e233+q3334rdKoWSaf8e2F27typDz/8UDNmzLB0joWyrTPPPFPPPPOMnnjiCUvHUW5urlavXq3Vq1fr/fffV5kyZXTJJZfo9ttvt4ygsAsyhTq92J49e4o0OFZQgwYNdM011+iaa66RaZpau3atpkyZok8//dTSkZmRkaGZM2cGvDNs2bJl+vjjjzV//nwdPXrUVV7sOgIKTj2Vz3/6ymDfK4x/J0pBdevWtXwO5cXp+QqORJTyRiT5dw7Z8X83T2F2795t+Vy7dm3bKU3znX766ZbgWMHO6uLQrVs3X3DsxIkT+vnnny3Bh5YtWwYEgQoTqTI3YMAATZkyJeB9g1dddVVEn9AvznIdjNPO7ipVqth2YPrnx3+0hKSAUUxuDBgwwBIcy59asXnz5pYpFyXrlIqSAur8wkYL+f+tKK8RN22gf11i10Gcz78eK8i/nsrKygr6LsqC0tLS5PV6Qx6ZU6NGDcuxDGVq3VPxr/8KO6/5fy9YFgo7t1WqVAkYjSrJMpLMTt++fXXfffcFTPeYlZWlZcuWadmyZRo3bpxSUlJ02WWX6fbbbw8YNRiu5OTkU9afBw8e1IcffqipU6eGdB/i5r5Lsq8XJPupt4uCXRBs48aNIQfH/Kfjk/KmvZXy7gf8j49d3V0Y//tDp+mdCCfQmb+vbr9fVPfB4ZYv//rTafDJv948duxYSPVmwfu7Bg0a6OWXX9Zf/vIXy72l1+vVb7/9pt9++02ffvqpkpKSdM4552j06NFBg3gAAEQCwTEAACLI/0evfwfz7t27df3111s6yAzDULNmzdShQwd16tRJs2fP1vz5813n5fvvv9c999xjCfCUKVNGZ5xxhtq3b68uXbroH//4h2VqG3+DBw/WOeeco4kTJ2rGjBm2T+FnZ2drypQpmjlzpl588UVdcsklkk49SqowRTF12q+//qpdu3Zp//792r9/v7p37x4wlZ1hGGrXrp3atWunyy+/XEOHDrVMi+XfCfDaa6/plVdesXQUVapUSR06dFCHDh3UqFEjy9QypxJqICAST977T/dVkH/Hl11naTDhPnHvJMjjdP/998ftlFNOdevWTePHj/d99n9fmpP3ekWyzKWnpweMLpKkb7/9VrfddluhgQcnirNcByt/dqN6I5GXohgFU9D555+vypUrW0ZyTJs2TZ06dQp4557/e4nKlCkT8B65YPyvkaKazsptG+hkWsLC9iHceso0TWVkZNiOcrFz5plnWqaIXLduXUjpHnvsMe3cuVOXXHKJevfubbkWi7L+C7buUOrM2267TX379tXnn3+uWbNm2QYBjh8/rgkTJujbb7/Vu+++G3TEmxN2I9cLWrdunW6++WZLoCIxMVGtWrVS+/bt1blzZ33wwQeORwUWJthxdNKWumF3XH/88ceQR0wXfE+nlBc0bdq0adDvu7nfi0T6fHb1g/8oplCcqkyF+v2iug+OVPlyWg9G6v7uvPPO0+zZszVp0iRNmzZNq1atCjh3ubm5mj9/vr7//nuNGTNG119/fVjbBgDgVAiOAQAQIR6PJ6BDwf9px0ceecTSKXjttdfqrrvusjwFWvC9HuHKyMjQX/7yF1/naVJSkh5//HFdccUVlifAQ5mupnbt2rrjjjt0xx13aP/+/VqyZImWL1+u5cuXa926db6Ot5ycHD3++OO68MILlZycbDutz/Lly22nJSwOL7zwguV9CwMGDCg0KNG6dWu1bt1aq1ev9i0r2Ln2888/6+WXX/Z9rlWrlv75z3/qrLPO8nUkhvqEcDTs2LEj6N/S0tIsn+3OZTD+37355pt973yJFP+gTVpamrKysoI+Ne2/r4WNmisK3bp1s3z2f9+Y/9+DiXSZe+qpp2yD44cPH9Zjjz2mN998M+R1RYt/ACRYR6iT0Y8lSf57ZCZOnOhbNn369IDz1rdv34CO0Tp16lj2e8eOHQEBtIJ/K6iorhG3baD/CKHU1NSg3/UfYVWQfz3Vrl27gHe0RUKPHj00YcIE3+e0tDQtX7680JEQWVlZmjVrltLT07V48WI9/fTTGj16tO69915JgfVfYXW53d8LO7duHxxo1KiR7r//ft1///3atWuXlixZol9++UXLli2zTMV3/PhxPfHEE/rqq69cbU8qfLSM1+vVn//8Z0vbfffdd+uGG26wTO366aefus5HQcX9AIa/GjVqqGXLlpYpkSdNmqTbb7/9lKOLDh48qJkzZ1qW9ejRw7dP+aNqCwZdU1NTVb9+/YB1HThwQOXLl1dKSopluf+IJf97joJ27dql0047LaRjalf/h/NeN6cB6GDHtKjug8MtX9WqVbOM4gpWf6anpysxMTFg+mP//enTp4/lnsSJypUra+TIkRo5cqQOHz6spUuX+n5brF692hdYNE1Tzz33nPr16xfyyEcAAJyI7l0bAAClyJw5cwI6LC+//HLfv/fu3WvpFK9YsaIef/zxgOlRCuvskwI7g+2m/5k7d66lQ+C8887T1VdfbQmMZWdnB32/lpTXebV27Vp9++23ev31130vre/fv78ee+wxTZo0SVOnTrV0Chw6dMj3TphGjRoFTClp916pgwcPavz48frpp58Cpmyx299g+3wq/k9Sz549u9CpJnNycmyn78s3efJky99uvvlmnX322ZZOCydTBRa3FStW2E6v5fV6A4K8TqbYa9WqleVzsHeJzZ49W1OnTtXGjRsdP13duHFjS7nLycnR7Nmzbb+b/669grp06eJoe27VrFkz6FP3hmGEHByLZJmbNm2apkyZEvTv33//vT777LOQ1hVN/p2Sweo0/3etxBL/6RK3b9+ub7/91rJs4MCBAen8r8Vp06YF3Yb/35y+Ay+UejoSbWDBqXulvGkm7YJDpmlq7ty5Qdfjf2w2b94c8K4zKa/++uKLL7Rq1aqw3hd2ySWXBEy59sILLxQ6Au6DDz6wTH9mmqblnUb+9fGePXu0YsUK23UtX748oE5wem5DcfToUa1cuVKTJ0/W+++/L0mqX7++Bg8erKeeekpTpkzRhx9+aEmzfv36UwatQ2nrCwsU/PLLL5YHBlq2bKk777wzoNO/sOBMtIU7xaP/SJvU1FQ9//zzp9zW3/72t4ARpwXXlZycrObNm1v+7n/PkO/BBx9U586d1atXL/3f//2f772G/mV427Zttm1XamqqLr74YnXs2FGDBg3SfffdZ7lO/af7tLuHDHW0ZkFOg0/BRqlG6j44UvyPe7Dz9sILL6hr164677zzNHLkSN87aP3rzXXr1tmOJlu8eLG+/vprrV+/PuC9Z1lZWfrtt980ffp0/fe//1VGRoaqVKmi3r1768EHH9Snn36q7777zlJv5uTk6JdffglrnwEAOBWCYwAARMDvv/+up556yrKsR48eOuOMM3yf/QMxx48fD+jU++677yzvRpICO0b832djN22W/7a2bNkS8AP1pZdeCphaL39be/fuVefOnTVkyBD95S9/0Ysvvmh5+j1f9erVgwbrkpKSdO6551r+9txzzwVMFfPmm2/qqaee0g033KCzzz5bl112maXj0O79PaFOFVbQoEGDLHnNysrSvffea9sR4fF49Pe//z3gHXI9e/b0/dv/GPtPOZmZmanXXnvNsszte0wiKScnR2PGjAkoA+PGjQvYt4L7fSoXXnih5fOSJUsCRgekpqbq4Ycf1p///GcNGDBAHTt2dDSCoGLFirr44osty8aOHatt27ZZlh0/flwPPfSQJfjWuHFjderUKeRtRUqwAFjz5s1Dft9KpMrc/v379be//c2y7LLLLgsYzfLss8+eclRKtPmPQNi9e3dAoGDhwoUB71WLJd27dw8IsBS8bk8//XSdeeaZAekGDRpk+bxw4ULL9J75xo8frx9++MGy7IorrnCUx1Dq6Ui0gR06dLCMHjNNU//4xz8CAuyvvfaa70ENO+3bt7cE5Y4fP65nn33W0vbk5OTo73//ux599FENGzZMnTt31l//+teg67RTpkwZ3XrrrZZlS5cu1SOPPGLbjs2YMUMvvfSSZVmLFi0s9WqjRo0Czvdjjz0W0F4dPHhQjz/+uGVZjx49Cn0XWziWLVumrl276qqrrtJDDz2kZ555xjJKO5/de5L86yf/KVhDmRKvsOkz/cvc3r17AwLo48eP165duwrNV3EK5xjYueKKK9SwYUPLso8++kh///vfbdeZkZGhBx98UDNmzLAs79mzZ0D7dcEFF1g+v/feewHHcNWqVfrxxx9lmqZ27dqlBQsW+KYfPOOMMyx1Wv7oIP+g8ZtvvinTNH0BlXXr1llGXfmPJPUP8h8+fFj/+c9/Avb1VJxOKxvs+5G6D44U//M2efLkgOlEd+zY4XtwJi0tTYsXL/aNSj7nnHMs5XPHjh164403LOkzMjL0+OOP64EHHtDll1+ujh07+r6TlZWls88+W4MGDdI999yjl156KSC9lDdCzX/0Xkm6fwYAlC5MqwgAQIh+/fVXTZo0yfc5JydHhw8f1po1azR37lxLZ2W5cuX0yCOPWNL7d256vV6NGDFC1157rSpWrKglS5Zozpw5AT8A/Tsx/N9tsHbtWr344ouqWLGikpOTNXLkyIBtbd26Vddcc40GDRqkrKwszZw50zJdYL789/LUrVtX5557rhYsWOD725NPPqkffvhBnTt3Vrly5bRr1y59/fXXlvxVqVLF8mT/jTfeqFmzZlnyOmDAAF1xxRWqXbu2li9fHjACol+/fpandu3e5fDoo4+qS5cu2rJlS8idlaeffrqGDBlimTprxYoV6tevn/r3768WLVooMTFRu3bt0qxZs7R582ZL+s6dO1tGHPkf46+++kqZmZk666yzlJaWpsmTJwc8CR1uJ1dR+e677zR48GANGjRIFStW1IIFCwJGW5x77rmFvmvEX+vWrdWjRw/LCJEHH3xQc+bMUZcuXXzvnCn4DooKFSrooosucpT3m2++WbNmzfJdd/v27dMVV1yhK6+8Ui1atNC+ffs0adKkgA67Bx54oNje+1JQ9+7dbUdihTpqTIpcmXv88cctI1OqVaumMWPG6NChQ7r88st9x/T48eN65JFH9MEHH0R9mrBgWrdurYSEBEtH4m233aZrrrlGtWrV0qpVq/Ttt98WSUdjcUlMTFTfvn0DRt7kGzBggO3yc845R+3bt7fU9U899ZTmzZvn6ySdP3++pZ6XpIsvvthRuZRCq6cj0QYmJydr+PDhlg7VefPmadiwYRo0aJCSk5M1Z84cLVq0qND8Jicna8SIEZYpwSZMmKD169erd+/eKleunKZNm2YZrWCapu0IvVO5/vrrtXDhQst71CZPnqzFixerf//+aty4sY4fP66FCxcGBCkTEhI0ZsyYgOvvtttu0x133OH7/Pvvv+uyyy7T0KFD1aBBA+3cuVMTJ060PPyRnJysv/zlL47zfyqdOnVS8+bNLdMm3nnnnerXr5/atWunxMREbdmyRd98840lXbNmzQKCGxUqVLDUTc8884x69+6tNWvW6Kmnngr5/YX5/MvckSNHdPXVV2vYsGFKTEzU999/b1tWnL6jMJL8p9x7+eWXtXPnTm3atEm33nqrTjvttJDWU6ZMGb3yyisaPny45Rr68MMPNX36dPXp00dNmzaVaZr6/fffNWPGjIDAYe3atfXPf/4zYN3XX3+9PvzwQ99xOnTokIYMGaJhw4apQYMG2rp1qyZMmGAJWrdv3943gj8xMVE333yznnvuOd/fp06dqp07d6p///5KSkrSDz/8oHnz5gVst6AzzjjD0uZ9+umnOnr0qLp27ap9+/Zp4sSJUR8VGIn74EgZPHiwXn31VV8gPTs7WyNGjNDQoUPVokUL7d69W59//rkleFenTh3fu4SrV6+uQYMGWe6jX3rpJf38888655xzJOVN31nwoYeEhAT17dtXUt5I74EDB1qmMR03bpzWrFmjc845RxUrVlRaWpqmTJliuXdLTEyMyPsJAQCwQ3AMAIAQzZkzJ6TRB4Zh6MknnwyYvqRBgwbq2LGjZVTDvn37Ap4S9+f/NHjLli0tn03T1Ouvvy4prwN+5MiRuvDCC1WhQgXL9DNr1qwJOsVdvv379/umy3n88cc1dOhQ3/SMXq9XM2fODHgXRUH33HOP5WnPbt26+TpR8u3du9eXX39t2rTR//3f/1mWVahQQfXr17f8UJ42bZqmTZumxMREjRkzJuRgx8MPP6y1a9daRtykp6fr448/LjRdnTp19OKLL1qWDRgwQF9++aVl2YwZMwKeui7o8OHDysnJcdzBVxQqVaqko0eP6vfff9e///1v2+9UrFgxYIRRKP72t7/p6quvtnRyFnZs/vrXv6py5cqOttG+fXs98sgjlhGbx48fDxpAkKTRo0erd+/ejrYTKcGCDYW9985fJMrcpEmTAgKgjz76qKpXr67q1atr9OjReuWVV3x/W7p0qd5//33ddNNNIeezOFWrVk3nnnuuZaRKenq6/ve//1m+d/7559uOZokVAwcODFq2/addzJeYmKiXXnpJV155paXTe8GCBQEBsXzNmze3dFiHKpR6OlJt4K233qoZM2ZYpstbt25dwPRpDRs21Pbt24Oud9SoUZo3b54leLhq1SqtWrXK9vtXX32146ChlHdP8O9//1t33323JRCTmpqqd999t9B0jzzyiM4+++yAv1188cW65ZZb9Pbbb/uWHTx4UOPGjQu6rscff9wyPWOkJCQk6O9//7uuv/56X2A9OztbX331VdARwYZh6MEHHwxY3rJlS8tUb4sWLfIdszvvvFMNGjRwlLfOnTurXr16limSt27dahvwKchuyuHi0qpVK8tonrVr1/o+9+3bN+TgmJT38MDrr7+ue+65xzKF5b59+/TRRx8VmrZBgwZ6/fXXbd/zVLduXT3++ON67LHHfMvS09ODvqeyTJkyAbMr3HjjjZo3b57lfBd2/XXq1EnDhw+3LBs4cGBAezZlyhTLlME1a9ZU7dq1w5peMRIicR8cKeXLl9dzzz2nW2+91ffAyIkTJ4K2LYZh6KmnnrLc199///1asmSJJQD2ww8/BAT28919991q1KiR7/O9996rBQsWWNqKgte5nZEjRxb7u2IBAPGjZD4CCgBAjKpdu7bGjRsXdEqqsWPH2k4tlC8xMVGDBw+2LPOfIqx+/foBU6Pky59CqHr16vrHP/5RaBCmatWqAVPTFdxWo0aN9MEHHwRMy2Mn/4n0ESNGBPxtzJgxuvHGG0/5FGzHjh311ltv2b7Y/Nprr7VN4/F4HD0VXLlyZb3//vvq379/yGnOOussffDBBwFTUZ133nm64YYbCk3bpk0btW3b1pJfuxF70dC7d2/deeedQf9eo0YNvfvuuzr99NMdr7tJkyZ65513Tll28jvMgo18OZURI0botddeK/SakvJGcj722GP685//HNZ2IqFOnTqWDqJ8Tjrb3Za5PXv26B//+Ifl++eff75l+r3bbrst4L1OL774ojZt2hRyPovb3/72N9WvXz/o3/v166enn366GHMUeWeeeabttdiqVauA81VQ/fr1NXny5JCmRr3ooov0ySefBLyPKVSh1NORaAMrVqyocePG2V5P+W644QZdffXVlmX+bVCZMmX09ttv2waf/F111VUBUxQ6UalSJb355psaPXp0wHuS7NSqVUsvv/xyodf7gw8+qKefflopKSmFrqtKlSr6z3/+ExBYiKROnTpp3LhxtoEUfykpKXr22WcDpuCVgpchKbx3eCYnJ+uf//xnoceofPnyAW1QsHe4FYerrrrKdppSKXCayFCcffbZmjhxonr16hXS9xMTEzVkyBB9/vnnhdYtw4YN09///nfbe7aCqlatqv/973+Wacbzt/P666+H9MBK9+7d9frrrwccl/79+2vIkCFB09WvX1/jxo2LemAlEvfBkXLeeefpv//97ykfSCpfvrztdVq9enW99957AefTX0JCgu68886AaWWrVaumDz/80HKfEoxhGLrhhht0//33n/K7AACEi5FjAACEKTExUSkpKapevbratGmjiy66SP379w+YJ7+gpk2b6quvvtK4ceP0/fffa9euXUpISFDdunXVpUsXXX/99WrVqpW+++4735P+P/30k1JTU1WnTh3fev7zn//ojTfe0LRp07R7926lpKSofv36lh+x/fr1U8OGDfX2229ryZIlOnTokMqWLauGDRvqvPPO0/XXX6+DBw9aRsN9/fXXGj16tO9zmzZtNGXKFH3zzTeaPXu21q9frwMHDsjj8ahixYpq1KiRunXrpmHDhqlx48a2+2wYhh599FFdccUVmjRpkn788Uelpqbq+PHjqlKlitq2bavLLrtMAwcODNpx8H//93+qUqWKPvnkE23atEkJCQmqXbu2unXr5rgToWrVqnrxxRd122236euvv9Yvv/yi7du368iRIzIMQ1WrVlWtWrXUtWtXXXjhhYV2nI4ZM0Zdu3bVp59+qrVr1+rYsWOqWLGimjdvrn79+mnYsGG+v+X76quvAt7tFC133323unXrprffflurVq1SZmamGjRooN69e+vmm28OmPLKibZt22rKlCn64osvNHv2bP3+++86dOiQEhMT1aBBA/Xs2VPXXXddSMHXwvTu3Vs9evTQjBkzNG3aNG3evFn79u1T+fLlfWX9mmuuiXrnmJQXCCv4XrSmTZuG1JlcULhlrlOnTnr00Uct0yWlpKToySeftKw/OTlZf//733XNNdf4nizPysrSgw8+qM8++yxop2001a9fX5MmTdK7776rmTNnavfu3apQoYLatm2r4cOHq3fv3rbvFow1AwYMCBhtEGzUWEF169bVO++8o6VLl+qbb77RL7/8oj179sjj8ahmzZrq1q2brrjiCp111lmu8hdKPR2pNrBRo0b68ssv9f7772vatGnauXOnkpOT1bZtW91www266KKL9NZbb1nyZxeUqlKlit577z3NmTNHU6dO1YoVK3Tw4EHl5OSodu3a6ty5s4YPHx7WiDF/SUlJ+vOf/6xrr71W3377rb7//ntt3bpVBw8eVGJioqpXr64zzjhDF154ofr373/KoJeUF0jp3bu3pkyZopkzZ2rHjh3av3+/KlasqKZNm+rCCy/U1Vdf7Xhkbjh69uypGTNm6Msvv9R3332nDRs2+M5hpUqV1LRpU5199tkaNmyY5VwW1K9fPyUlJendd9/Vr7/+Kq/Xq5o1a6pTp04BUySGqmvXrpo8ebLGjRunRYsWad++fUpKSlK9evXUo0cP3XjjjapcubJmzJjhmwZw5syZGjNmTEiBzEjr1KmT3nvvPb3++utauXKlsrKyVKNGDbVr107NmjULa50NGzbU//73P61fv14zZ87UTz/9pJ07dyo9PV2GYahatWpq2LChevbsqb59+xYaeC5o2LBhOv/88zV+/HgtXLhQO3fu9LVJ+eXvqquuCng3ZL4KFSrotdde0+LFizVp0iQtX75c+/fv9533du3aaeDAgbrkkkuCvtdr7NixOuecc/TZZ59pw4YNys7OVsOGDdWvXz+NGDEi7GB/JEXiPjiSLrroIs2ePVuffPKJvvvuO23ZskUZGRlKSUlRo0aNdM455+jqq68OOkqxQYMG+uKLL/TNN99o5syZWrdunW+E72mnnabu3btrxIgRATNo5Ktfv74mTpzoG/m+du1a7du3T9nZ2apQoYIaNGjge+9xKEE0AADcMEzebAkAAFDqvfLKK3r11Vd9nwcPHqxnn302ijkCAOdSU1NVs2bNU06n+5///Mcyveaf/vQn3XXXXUWdPQAAAAAxouQ9+gkAAAAAgI1+/frpxIkTqlGjhmrXrq127doFvM8oJycn4B1zTZs2Lc5sAgAAACjhCI4BAAAAAGJCs2bNtGrVKu3bt0/79u3zTcnVs2dPJScnKy0tTbNmzdKGDRt8acqXL69zzjknirkGAAAAUNIQHAMAAAAAxIQbb7xRf/nLXyzL5s6dq7lz5wZNc91116lKlSpFnTUAAAAAMaTo3/YJAAAAAEAEXHbZZfrTn/6khITQfsoOGjRI9957b9FmCgAAAEDMKZUjx7xer44dO6bk5GQZhhHt7AAAAERdbm6u5bPH41FWVlaUcgMA4Rs1apQuvPBCffPNN1qxYoW2b9+uY8eOyePxKCUlRXXq1FGHDh00aNAgderUSR6PRx6PJ9rZBgAAAFCMTNNUTk6OKlSoYPtwnWGaphmFfBWpo0ePWuaYBwAAAAAAAAAAQHxp2bKlKlWqFLC8VI4cS05OlpS302XKlIlybmLDmjVr1K5du2hnAwBiEnUoAISH+hMAwkcdCgDhow4FSr/s7Gxt2LDBFy/yVyqDY/lTKZYpU0Zly5aNcm5iB8cKAMJHHQoA4aH+BIDwUYcCQPioQ4H4EOzVW6G9xRgAAAAAAAAAAAAoBQiOAQAAAAAAAAAAIG4QHAMAAAAAAAAAAEDcIDgGAAAAAAAAAACAuEFwDAAAAAAAAAAAAHGD4BgAAAAAAAAAAADiBsExAAAAAAAAAAAAxA2CYwAAAAAAAAAAAIgbBMcAAAAAAAAAAAAQNwiOAQAAAAAAAAAAIG4QHAMAAAAAAAAAAEDcIDgGAAAAAAAAAACAuEFwDAAAAAAAAAAAAHGD4BgAAAAAAAAAAADiBsExAAAAAAAAAAAAxA2CYwAAAAAAAAAAAIgbBMcAAAAAAAAAAAAQNwiOAQAAAAAAAAAAIG5EPTiWkZGhyy67TDt37gz426+//qorr7xSffr00ZgxY5SbmxuFHAIAAAAAAAAAAKC0iGpwbOXKlbrmmmu0detW278/8MADevzxxzVjxgyZpqkJEyYUbwYBAAAAAAAAAABQqiRFc+MTJkzQE088oQcffDDgb7t27VJmZqY6duwoSRoyZIhefvllXXvttcWcS5REP837WTVqVlbz9i2Lfdsrl6zVkpUblZSQqHPPbqcWZzR2lH754tVatmqjEgxDTZo2Uq/enYomo0Ap9+PcBcrMzJK3bHX1urhztLMTFdmZWZo8cb7OPrutTm9WP9rZiUmLf1ijps1OU526NaKdlWK3c9MOTf9hvbKyslXGzNKIEf2UUqF8yOmzsrL04TuTlJlrKDExUed3b6u2Xc8IOf0XH0/T3gNHZEoyJFWtnKIRNw4MOf3PP6zUTyt+k+k15TUSVbtCGV1906CQ08O9Xdt3avLUJapWLkHNWzZQ5dqN1Lp5rZDTHz92Qj//+Lt+WLRBOSlVVdvI1jU3DFBSUvH9RHnvzS90NNtUbmIZlTezNaBfT53euF5IaXNzc/X5J7O071i2lJslla+soZd2Vr3Taxdxrv9w5OBhfTFpvo5nZcmQV73OPVOtz2wdcvrtm3dp+ozFyvV6ZChBp9WqriuuuthRHrKysvTmG18ot2xFdTgtRb0G9XaUPjszS59+NE1Hs7JUp3IZDb1+sKP0m9du1txFq5SVk6VzOzXTmWd3dZQ+kj5953Ode3EPNWh0eshpdu7YpxkzFikrK1NJCQkaMvgC1awbehn6bfUmzflhpY4nV1C5o/tVPqWcbhg5SMllkkNKv2XTLk2eu0KmpM5Na+jCi3uEvG0p7zr+9ONpOpGZI+XmKKVCigYPvlBVa1UPeR0fvfu10jOOyVSCkhIT1KNrG3Xq3s5RPtzK3HNUZetWlGEYjtPO+Wq+1u3cL3mzZShBZZMSdPXVfVSpWuWQ0s/8Zr5+37ZXppn3OTkpQcOGXarqNauElP5IeoY+nzhLmVnZMr2St3yKrh10rmrWrhZS+uzMLH3y4VRlZGXLNKXkpEQNGXyhatWtGVJ6Sfrs/a+0/2imTFNKTEzQReeeqdYdQv+t/uF7k3X4aJakvHuC00+rpkFDLw05/Xczf9KKLamSaSo5J1PVKpXXtSNDvyeY+sUsbdl7UKY373NyUqKGDe2t6rWqhpR+0XdLtWLtFnm9eScxKSlRvXt1VfNWjUPOw4ypP2jLtj3yeD2qlJKia0b0Dfk6lqRPP/xWB44ck+mVkhITdfE5Z6rFmS1CTv/Fx99q76G89EaC1KVdU/W4oFvI6adPWaQNuw/K4/GoTE6mWjatp0sGnBdyekn65L2p2uVNVMXcbI2+NfR7wnxTPp+j3QfSZcqrlk3r6cJLzwk5bW5urr6cMO9kXeRVw7o11HfQBY62/+2EOdpx4JBOr1FGl11V/Pek2zbu1Mw5P8nrzVXrpvV1QZ9zHa9jf+p+ffH5LPXp20ONmzcpglye2rFjGfrw3a9VJjlZ114/UOVSyoWc9uuJc7V73wF5cr0qXyZZ117f31H6KZPmaOvu/ZIpJScnq/dFXdW0VcOQ02dnZ2vihJmqUKmKdu/ao/Ytm+jc3qFfR5L02fuTtDurjBK9HrU5rYouufxCR+nHjftSh5MqKMWbowu6t1G7Dk0dpZekn+f/ou+3HVLDcokaepWz68CtXbv3avLEufLIVGJCkurVqKzB1/RzvJ6Pxs9R1zYN1LpzqyLIJUoTwzTzb8Gip1evXvrggw/UoEED37JffvlFzz//vD755BNJ0rZt23TrrbdqxowZp1xfVlaW1qxZU2T5RfS9nmooyWvq/04r/m1/u+G4dlapIElqse+QLm5X1VH6b34/oV2VU3yfb6ziVflyzn+EAfHu9dQ/rpu+GQfUuFnonTClxW/LtmpegyZqlJ6hfq0qRDs7Men1VEO1jmfryiahdz6UFgWvIUnqsWeHOnZsEOTbgX79ea3mn/5H52XT/em6tG1oHXk5uabe2W/I9OuEHOw9rDqnhdaZ+O2GE9pZJcWybHSdqN/WxpWC90T5+hkeNaod2uQUa1bt0cI61kDUpUfS1LRF6AE2N3b8tkdTqlq3f8aeNJ3fMbTt79h6UFPKWwPrTQ4eUZ82lSKWx1P5aclW/dLoj86rpgcO69IzQruGJGn+yv36ta51f6+r5FHFlNAnGPnxhzVa0by9JKlaZo6GN3IW3Ny0codm1c3reCqX69XI+s7ui+etPKDfTnbiN089oN4donM/sGntJs2q2dxRXShJ89Yd1W81/jhnnbZt1VndG4WcfvaadG2sZQ2CXHRor1q1rhNSev+2wGk9unrlXv1Q1/qjrNv27erSLbQA4Z6dh/VVclXLsoaHMtS/dfHd1ySme1Thl2xlNk1SdiPn9wP+x1CSzt6+VWd2C+08frrNo/Ry1uvmzL2pOvvM0IKky1alammdupZlTq7Fgtdgvg67dqtn59B+bGfsP6KPcitLBdr05vsOqXeIv5OPpJ/Qx5nlLenL53p1o4O6wO4cDE/KULUaoZWj93eZOpFkrfc67dqlszqH9rDE5E1Z2lvR2gHfMu2AerUPvT56b5epzAJ56J2epuatQmuP0lKPaJKs9U771HSd0yH0uujt3VJO4h/HsWF6hvo7+H3hfw6ctgc7NuzRlCp/HO8Ldm5Tmy6hByVysrL13sEy8iTk5aPGiWwNaxz69bxtywFNS/kjIOy0vynzWKY+OFpOXsNQ7WNZGtK0TOiJI+S7lfu1/mSbXjcjU1c0K+t4HYsWbdKqZs11xrYdOr976L8LImnZD2u09OR9xfm7tuiMzo1DSpeVZerddOt13Gv3NrXsFHo58q8LWqUe0EUO7it+WLpdqxv+Ufen5Hh1QwNn9zVu2uWtG7ZrepU/th9uWSyYh+sqnFDFiqEHGN1atPBXrWrxxwOXiV5TI2vkKrlM6PXJb7/t17yqedcCvw+Rr127dipbNrBejOrIscLYxeycPsUVbKcRaNmyZerSpUu0sxG6qcuVm2CoS5fiHy3y9cYfVONEtjKTEuVNTHR83L7atEg1j2er1sE0/dqggVq0bK06tSoWUW6BUmzqct8/E8tViGodFq06dPOqVEnS4XLlYqsOL0mmLte+lDJRaU+irsA1JElJZZyVo99XbZQktd66SbvqNZY3ISHk9EeOZspcsE6t9uxVVe9RHfWU07qGp6tunfrq3CW0J82/2viDah7P1ukbvtcvHfNGqnAdFK93di8LWGaUr6wuXUJ7QnPDb3MCllWpXK3YzuPRvT8ELDMd3NsdO7JCOuG1LNtfsXjbox+X/fHeZsM05XFwHUrSvNWzleQ11X7VTGVWa61fGzVS06bNdXr90EacSNLiH9b5/n08yfm98Z6N6ZKkqlm5ykh2nn7umjkq6zGVYJryJjrb/0ja9ut2SdLBihUd5WHObwuUkuNV0w0/ak3bnjKSkh2ln7lunu/fNY9na39KGVWo4CAPfm2B0+P328nr+KwVvygp84R+6OFsHxZnrpEOZ6vt1q1KTN+iHa3PddSeRMKRdWlK/WWDapWtproh1l8WJ4/hWb8uUUZiWa1teaYSHbSp43cs1enpGapzdI9OHNirtR3PkxKTQkq/bNkyJSbn9Xl0WfmTMmqert/q19Ohcskh39vkX4Ntt/wu48hurTnzApkhbl/KG72pbelqtXuvPPs3a0+bsxydw80bd0ob0tR61x5VM44ptUxN7ahWxVkZKFCOW+zdp9/r1lKDBg3VOsRZXt7ZvUxNDqSr8tG98h49pNXtz5bh4Bh8seVH1cnIVJ0dq+XNydDvZ1wg02E5fmvPMjXdf0jljuzTuqYtValS5ZDTL1+6QdqXoTO271DSoc1a1+48x9fRG3uXqdm+gyqf9rt2N+3suD0peA6qZuUq12H6o6nWNjk7sayj9Ccyjsvz/Xo1T92vE+XK61hZZ+mPHFouZUtnbN2srPJVtalOdUe/Dw7s2S/vL3ntgCfBiEpbNG/1bCV7TNU4keX8/J30w48n9yHZ2fGLpOU/rvf9O7ls+ZDzsX9/hrRkg1rv3K2krCNa06y1ypZPKTS9/+/4t3cvU9P96UrZvUS/t+3tuM9vzupDls+ZSWGUBRft8oG9Ryyfj5Z1dk9hl4dmTVqoQcPimxVh8aLfJUktNq6QKjXQ73VqqkOHDo5mOPl143zfv/l9iFMNoorqO8cKU6dOHe3fv9/3ed++fapdu/guRqAwSR6vElw8fJDk9Sop63jkMgQAQJR4co8oIcyJCBKzMnVm/0uUlJURVvokr1dmwsGw0gKlTbjzEBimJM8uJWQfi1oeJCnR6z31l4Jt1zTDrodKggTTVE7uzlN/8RSSPZ4I5CY8lfbtUbXUrWGnT8o6quPZR5Xkjd3zWCNrh3Jy08NKm5ybq1YXXaDj3vDSS1Lisd1STvjXsZF9WEdyw29TE3JOaHeSwv6dnJx1QmdddqmScrLDzoMkJWVnhbf9nBzV6NxDWbkHwtuuxyPjtLo6npQRdkdbcna2zKz0MFNLSVkZOuY9HnZdnJSTpaOVUlzVx5K7+tytxJwsd3Vh1hEl5borg9GUoLyyGM8Ss48rKSf8uiwpJ1vZCTtd9fnli8Q64tURI02J2ZnRzgbiQIkNjtWvX19ly5bVsmV5T6ROnjxZ559/fpRzBQAAAAAAAAAAgFhW4oJjo0aN0urVqyVJ//rXvzR27Fj169dPJ06c0A033BDl3AEAAAAAAAAAACCWlYh3js2dO9f37zfffNP379atW2vixInRyBIAAAAAAAAAAABKoRI3cgwAAAAAAAAAAAAoKgTHAAAAAAAAAAAAEDcIjgEAAAAAAAAAACBuEBwDAAAAAAAAAABA3CA4BgAAAAAAAAAAgLhBcAxwyAzy73DS5y0IZy0AAESW6bRVc9N8BWn7TNPrYqWINXalwFuM90Ver7ttFWdei4wRuMjpbhkuD4Pp8jiWgrNg4fRwmEbgSXR7TJ1lwG6hTcFykN50kLzkCv0c2H/TyUFweQ25LC9et9svoqbfSR1v9023V5Hp6BzapHdwXqJ9/5Sbm+t6HabLNtnj8Uvv8PDbHW+Px+Ng+9G/hy1Yd8Z22xjejtifgVLRoISuFNyb2tbH0b+8UIoRHAMAAEDMiv2fgECEcDHEOE4gEAlx1hVeBDiCAEoGaiMUB4JjQLSUgic6AABw92y0efJmNPw20eBnEyAp/A4EQ+5HOESCmxFohsv00Rb9o++eEYHgnnmyNMYqQ5IZdkE0ZRjuWjRTpsvrwHR1IRmmJJuRjKFv3VRkroYw98E0T1Ym4SU3JCUYcnUMTmbEXVrX5cj9WYjl+limGcvVkGI885Fhms5nxECJE357CjhDcAwAAAAAAAAAAABxg+AYAAAAAAAAAAAA4gbBMQAAAAAAAAAAAMQNgmMAAAAAAAAAAACIGwTHAAAAAAAAAAAAEDcIjgEAAAAAAAAAACBuEBwDAAAAAAAAAABA3CA4BgAAAAAAAAAAgLhBcAwAAAAAAAAAELdMI9o5AFDcCI4BjkW2tTRNM6LrAxA/qD0QWc5KlOnXHvp/DmdLXifbN/j1GvNsCkJx3hfZbcvR1kvkPZzD6yICu+B2FZE8ik7qoZIqEsejOEum7baM0HPgNZ3U/LHDbfXgLLm7cu+6vHg9get00EbblYFIdFAXa3tSBOtwsk67q8hpfehm+6Y3+u2h6VcOnebI67Uphw7KkGlJH/ttUTSZBc6e6eBM2p2u0tnCBFc6+hcDrx+vAtsZIFIIjgEAACCK6EAAgEhwEJMqkWI8+ziJVr0kcHE18fARQHsExBGCY0AY3N4uGqYpmlsAQGlg+v4TXmLD7d0onTiAJHf3p9yVRp8Z88+3eyWPm5J0cpxLDBfGBHllhLsDppRgyFWbZrr9jWmaLke8mXJTExmSEiLRQ+ViJxIMQ6arKLMhUwnhB6pd3VQVWEeYqzDMmL4ETzLdDd00YjzAG/sn0DVDCr8uluS2LkOkxPp9EWIFwTEAAAAAAAAAAADEDYJjAAAAAAAAAAAAiBsExwAAAAAAAAAAABA3CI4BAAAAAAAAAAAgbhAcAwAAAAAAAAAAQNwgOAYAAAAAAAAAAIC4QXAMAAAAAAAAAAAAcYPgGAAAAAAAAAAAAOIGwTHAKSPaGQAAIPJM02EDZ/p9dpLcG2SVXv+VojSzPdvFWQTMIAUx1OQ2eY21Emx32Tu/DN3ttVkgfawdv0gokn22K5xFJnBbTjZfaut9I/RG0b75Df24mC7Pt+myFHrcVaUl4rq3PwdR/uHv4LxG5DoKKLOh77/d5p3myOtyH9xeB/brDP27BfNfEsp0XLI5YUaxtofR5/LWtsQyPdHOAUozgmMAAACIGrddTzyzAkRC9K+k6OcgugwHwZSg64jxLlkz7ktB7DOkYg7OItIicRVyJQMAYgXBMSBM3PABACC5eT7WkLuONMM0ZUagQxmIZ74rKOod2u62H+s1QVGMeihWphR0WLADsXweIxOcNMJvExX9ESteN2fQNGVEYC9cnwc316LhrostMnk3XJwFM0JB6uiVRPe5N0tAexi+WK5DI8Xt2TPMvHUYsVsMSo8YvhYROwiOAQAAAAAAAAAAIG4QHAMAAAAAAAAAAEDcIDgGAAAAAAAAAACAuEFwDAAAAAAAAAAAAHGD4BgAAAAAAAAAAADiBsExAAAAAAAAAAAAxA2CYwAAAAAAAAAAAIgbBMcAAAAAAAAAAAAQNwiOAQAAAAAAAECMMmVEOwsAEHMIjgEOmRFOb7pdIYC4ZbqukYACHDZIphF++fNGKA+IdYHnuzjrtaDlMERmCSyvpuN+MZsEDvfLcNkZ5/YwOt/nEs7h/tgdvuIsm67PXwm8jiLB7X45Se3696nNChx1snsDa1NH+fcWTRnweNzV8m7bI6ep/b/v6LDYnUS3daOD9KbLY523Enfr8Ho81tU53bzNMXRSNr0u8x9pMd02FjjsTq4D23NYjHVxSeAtDW2qTdktFfuFEovgGAAAAKLGiOUf70AJ4iJeXTLEev5doipk1AMiI86rEvcicRlyEhDr+IECxA2CY0C4XD8mGZlsAAAQXaaMsBs1M+/Hp6s2kR+vgDumJLfXoXturuRYDwwaMl2NxvWJ5pPVpqkEF9s3ZEa9DLpmhn8KDEkJRt4/wr0W8tpit6Mk3KQ3T+5E+AxDrstB+JeSmXdLEuaQG+NkVZo3njb8nXB1DiJRjUSgPYjqnZn74ayRyUcUGaVgH9zIa09K1kg+OBeR+yIgBATHAAAAAAAAAAAAEDcIjgEAAAAAAAAAACBuEBwDAAAAAAAAAABA3CA4BgAAAAAAAAAAgLhBcAwAAAAAAAAAAABxg+AYAAAAAAAAAAAA4gbBMQAAAAAAAAAAAMQNgmMAAAAAAAAAgLhlRjsDAIodwTHABVOG80RhJAEAO2HVQUAQjsuTi1+Ppmmf2BtkeYQ3jxLC7nQHKxvFtX0nvF5v4DpjrloOPAgem/0qfA3udrrgeQjvlBTYvhFzJyBgnx0fA5t99hZjBWm3KSebt60HuL+Rkx+N1kPo/Ni5rQvd1tt26UtGG+/gHNh91emp8Pu+k8Pq5P7JTk5OTsAyJ2v0mu7bQ7fn3GnbFZgBm3Jos19BkxfYfuzdC5QskWwD4q89KRm1pxt2e+D1xP5+oeQiOAYAAICoibefrEBR4VqKbZE4f7FeBuj6in2GjKiXQ5OS5FK0zyAQfdQiQPwgOAaEwYjAU80GzS0AAEqQITc/QaPfDQfEONP9yBG4ZEqmXI56iDLXv49MM+brc3e/70zJSJCbwIRZ4L/hrsF0M/rSdNmhbObtvdtSYISbCfPkPYmLFSREpIvNbTlyfw5jva/CXf5jvR5CHnfXkWEaMX8dxDqvYYowJYoDwTEAAAAAAAAAAADEDYJjAAAAAAAAAAAAiBsExwAAAAAAAAAAABA3CI4BAAAAAAAAAAAgbhAcAwAAAAAAAAAAQNwgOAYAAAAAAAAAAIC4QXAMAAAAAAAAAAAAcYPgGAAAAAAACIMZ7QwAAAAAYSE4BgAAAACAG0a0MwAAAADACYJjQLGz/nL28rQlgDDRD4fIctgemdYSaEagRJqmgzxwAcQ806bMFeddken12mw/tguW0+Nnu7+ewONSGMPlSbNc9i4Pf1TvqiO18Uisx0ld6npb7pJ77fIa25ehJGeHxW29YxTyKaTt22Y29D0wPY43aeH1Fk15dbJeu2+6zpXT26qABQ7yb/NdR2XQYb0fkN5d8rx1uD3g/nkwnF0Ldtt3cl/q9ZS0fp1SUJFKclQX2Z1Dt5t3WI6izdFvqRLK7oh7I1HJAEEQHAMAAED0uP3NGVu/WYEiY7joAjJKwoUU+/05rrg5f6UFRyD2xVg/MoqI24cmgGijCAPxg+AYECY39/2GSWMLACgd7Eb/OEnttiMt1kf6ANFmKP++NLp3p+6Ce7FdE/xxDlyuJ4qn0DBNGS6f7DbN2O5UNwyvwj6TZl7njKnwf2fmDRhw1ya7Hgno6ko0lZAQiashvPRG/n8S3GzfkJng4hiYkekoCDcHhswIRTijdyGbrsuxqWi3h3DHNCNxTxDLdxWlgykjpu8JEDsIjgEAAAAAAAAAACBuEBwDAAAAAAAAAABA3CA4BgAAAAAAAAAAgLhBcAwAAAAAAAAAAABxg+AYAAAAAAAAAAAA4gbBMQAAAAAAAAAAAMQNgmMAACAu5ebmRjsLAAAAAOCeEe0MxD4z2hkAUOwIjgEAAAAAAAAAACBuEBwDHCr4JIkZxpM5/k+imF43uQEQz3iyDZFkOixQpn8JdNAmBqT94w8O8HhszLM7304Loqvt22zLSTkuzryGzP114XW6X4bb4/DHzXBJPKJFzjAK/3wKUb+MQlwWfAWB3y4N5cDtPjhJb/1uOHVA4A9S00E59K8zDMlRObarS0tGGXCSC5v9dXgtB2zdyYVs+1UH59Dr7oh75XGVXnJfb3k81nLsdHV2bZ+Tc2DaXEfRVDKuIfdct2cuVxBrx7FE3po6ZNrVXaVhx1BiERwDAABAFBHkAiKCfoPY5jrIKMV+IaA9QCTE+nUAAACKC8ExIEoi8vsXAICoM8Nv00wpwZCMMDuyDFOun8wGSgs3V4IpI+zrMFJcX8k8VRx9Ls6BYeaPKY7l82iGNbNIPsNQ3oUQ9iHwukmcN6o7wU0XkSnDVReTqcgESMNdhynD7T2FYciQu9/6povEhkyZSgi7GJiKzFmI5p1Z/mUUrqCzG8SQeL8zjsT+Oxk9i6KSXyMBRYvgGAAAAAAAAAAAAOIGwTEAAAAAAAAAAADEDYJjAAAAAAAAAAAAiBsExwAAAAAAAAAAABA3CI4BAAAAAAAAAAAgbhAcAwAAAAAAAAAAQNwgOAYAAAAAAAAAAIC4QXAMAADEJdNrRjsLAAAAAOCaKSPaWYh9HEIg7hAcAwAAAADADTrUgDw8ewQAAGIEwTHAKX74AgBKIcd9WW7aQ6/9arymN+CrweTn16Bdjll2Za44B3TaljYn2zcDv1wantp2OqrWdH0RuktvyW00D3+YZbdoinzxXUj2Wwr9RNhcRqWDgx0zbQ6Xs6rI7UF0d+F45QlY5vq0usiScXLrTo6L2/xGohi7WYeT+yfbbXtdpg8sAo7bQzPa+2BTXpys0lug7SwN9wIlhZPrwmvz7XCvq1htmryldGYUj7vLGygUwTEgTG5ud/Ju2EtnowUAgBOGYbhsEemAAKTwrwRDeQEu7kyjyzTc9/xEszY0THfdwXnlL7brc8OUwv2NZ8jMe9jDMFxdy27k1QVuVxL+CgxJCREoAm5ihYaL9CdPn0wX3Wxu+wnyiqAR9jqMAv+NV4YU0101Bl1NJ4V/EIyTlYDBcYwq0/4xNiDiCI4BAAAAAAAAAAAgbhAcAwAAAAAAAAAAQNyIanDsm2++Uf/+/XXJJZdo/PjxAX9fu3atrrzySg0aNEi33Xabjhw5EoVcAgAAAAAAAAAAoLSIWnAsNTVVL774oj7++GN99dVX+uyzz7Rx40bLd/7xj3/o7rvv1tdff60mTZro7bffjlJuAQAAAAAAAAAAUBpELTi2aNEi9ejRQ1WrVlVKSor69Omj6dOnW77j9Xp17NgxSdKJEydUrly5aGQVAAAAAAAAAAAApURStDaclpamWrVq+T7Xrl1bq1atsnzn4Ycf1k033aRnnnlG5cuX14QJExxtY82aNRHJa7xYtmxZtLPggCGpZOTZbR5+/32DUvfw+j/AOcP3r6OHj0S9PojG9k9knojq9mOdJ9ej/Fuh+Dx+huVTTna2o+OQmZkVsCzU9MdPmMp/RmvDhg2+5Xv37tWyZTkh50GScj1ex9tH0Tl85HDI5yErMzNg2ZHDoad3Kz09XSpf1bLMNM2Qt3/g4EGpWp2A5cVZDr1eb8AyJ9s3ZQYs27J1q44e3RV6HjzWPDjd/4yMY1K1mmGn9xeteuDI0QypuvM8mH7n0Ov1ut6HjIxjDtZhbQucbjsrK7AtyPXkhryeffv2SVVqBywvzvOYtDdXKZIOHjyoXWFtt8AxNPOuqZyc0I9Bvs2bt8hr5pUHJ+UgN9e+3Qw1vf81KEmmN/S6MG17mpSSVxf+UaeEnj5192Epsaokaf369b7lzo6fEbBkx44d8uqog3VIu/fs9u1DONdiRkaGzJPn0DTdleMjR46GnH7v3gNSBb9zqNC3n52ZLalswPJQ05844S69JKWnH5bqVvF9zsnJcZQ+42CGpEoutn9Iql7XsmzV6pUqUyY5pPSH9h6SjOphbTtiTGubHqvtaW6ux/fvrMzMkPNx5JhXUqJl2fETJ06ZvvC/h16X5X078L7K+XEMv10+cuSwlFLLsiy88/hHHjZs+E2p+3aEsY7I+e3Xtdq5q3zI3z9x4oRUKa8+4PchTiVqwTHTDKwwDOOPiy8zM1NjxozR+++/rw4dOujdd9/VQw89pHHjxoW8jXbt2qls2cAGGoGWLVumLl26RDsboZu6XJKikucvNy+yfHaah0mbF1s+t2jRUg3qVQnybQBBnawHJKlSlcpRrcOiVYf+virV9++YqsNLiJzsHGn2aklxevwKXEOSlFymjKPjsH7FxoBloaY/cOCY9NNvkqSWLVtq6495DzTVrVtXXbp0CGkd+e1pUuIfD5jE5XmMond2B/7YrFK5irp0aRVS+rXrDgYsq1ylSrGdx7SthwOWGYYR8vYPpv0kmz6QYi2HPy7b6Wr7c9bMCVjWpHFjtWxZ1+bb9hb/8GvY25ekbesPuEo/a+1cV+kjZduv28PKw8wNCy2fExISHKWftn5+wLKKFSuEvg6/tsDp8VuzblbAsqTEpJDXc+TQcik7cHlxnscj69KU+usGVa9eXXVDrL8sCh7Dk30aycmhH4P3d/0sSWratIm2LMlr00ItB8uWLVNSkn3nfajb978GJclICL0uXO1ZLe3PC9AZvg7V0NOvLbNZ2pEuSWrdurXWrUuT5LAM+JVjSTr99NPVqUvL0NJ/m9ee1TutnjasztsHJ9fihK0/SZIqVqyozPS8c2gYDvdhirVNrVy5UsjpM4+vko7mWpYZCn37JzKOS9+vD1ge8nWcniEt2hCw3Mn+7992xPI5OTnZUfrU7XulNbvD3n7ansUByzq0P1MVKobWIb9l/RZp86Gwth0psyPQHv7wY3htWST9vPg337/LlisXcj527T0sLd9kWZZSvnyh6QN+x0/xv7cNvS6TpDmrZwcsc3wcXbTLu7bsd799vzy0bNlKDRoGPsRSVH5YvClgWas2bVWnbrWQ1/Hrxj/ujfh9iKysrEIHUEVtuEqdOnW0f/8fF21aWppq1/7jYtuwYYPKli2rDh3yOkiGDx+uJUuWFHs+AQBA6eS16dQGAAAAAMQffh4C8SdqwbGePXtq8eLFOnjwoE6cOKGZM2fq/PPP9/29UaNG2rt3rzZv3ixJmjNnjtq3bx+t7AIAAAAAYC9wVjcAAAAAJVjUplWsU6eO/vznP+uGG25QTk6Ohg4dqg4dOmjUqFG6++671b59e40dO1b33nuvTNNUjRo19Mwzz0Qru0ABhiL5PIndFKMAEApqD0STaVp7gp2UR7v5+KW8d5wgftjdAxXnbZHdtkwHAY6ScQtnFPrRaXJJ8jjcMcPlcSj4xq2wVmUE/VC8wj4OZiGfwhP4Jroi5DLDefVA6YssmsW4T5ZtGc63a1teHJxX03Pq7xTG4wnMQSSuA9PJhWB72NydQ8f74HfunNwS2d0/Odm+x2OT3kFZ8kagQXS7Bq/be0i7ewIHuTJtjmE0ObmfKWnCPZJ213zJOiunlp1tM8+wE0Vy3ov1rkL2O1HceUA8iVpwTJIGDhyogQMHWpa9+eabvn9fcMEFuuCCC4o7WwAAACgmYfTj+a8hEtkAEGVGyYg2Rk9EogERWAcQ4wwuhKiL+/ocABAzojatIhDLDFPuf3xyvwgAKCXCD0/ljRgIvxOFxhRA3r15LIfJ8/Ie69ExU/K6GUJkypQR2+fR9LoaUWoYhqsnRkzJ/ZBWwwh7HYZMR6ON/JknNx/NwIqR4GzElIUp5XWxuSvHrtKaeZVh2OswzZgecSQp/0IIP7npLQUB1ljPv0umGYF6JNYvhNhnRnbSLiAogmMAAAAAAAAAAACIGwTHAAAAAAAAAAAAEDcIjgEAAAAAAAAAACBuEBwDAAAAAAAAAABA3CA4BgAAAAAAAAAAgLhBcAwAAAAAAAAAAABxg+AYAACIS6bHG+0sAAAAAAAAIAoIjgEAAAAA4IYR7QwAAAAAcILgGAAAAAAAAFwzZEY7CwAAACEhOAY4ZL3Vd/6IqBmQhh8PAIDYYwY0Xw7axAJpC6YyA1caAm5nY5Xt2Q6rDISbAbupVUMvx16bvJpRHD1kmM7vKvPvSwven5peZ1POuj1jRsHjaIRzAAvk3WVeXAlz475jH3bZMQL+FV5dGh7TZsftlgXjLVDe+FUUHrfBKMO2LnPSppewM3cyP6ZtHR8kSYjLgqYvgrrfkIP8250DJ6fQ6wn9yza8Hvftodti5PU7387bQ5t9cNAeeoPd3CIMBdp1t9WL6xUU78n0etxdi0VRH3tsru/i5vVGPw8ovehNAKKGyh0AEPvyOhPCa9OMk//vpCPVPz2ACDAZ7RFVphmZ4x/FU2jYPALoLH3sMwwzvPiudPIazCsFYa/CQTCoMOFvX2EGuE9u18w7ftEqxob5x+MC4TFlGIbrQJmbvu1w76esDNuAaewI40mRUoS2/CTD1YWUt4rI5ASuUJ5R9AiOAQAAAAAAAAAAIG4QHAMAAAAAAAAAAEDcIDgGAAAAAAAAAACAuEFwDAAAAAAAAAAAAHGD4BgAAAAAAAAAAADiBsExAAAAAAAAAAAAxA2CYwAAIC55TW+0swAAAAAAKAHMaGcAQLEjOAYAAAAACIsR7QyUFBwIAAAAIKYQHAMAAAAAAAAAAEDcIDgGuBDWkGu/p0pNxm0DCBPVByKpOMtTsAktnbSJdl/NyckJJzsIU1GUGbMYb4y8tpsKffvFmdegeSiKdTrcL9NlLtwex+ifhZMilBGnqykx+2/hbhidacT+MDwn14XdN52cV8t3wzh2bq9Br2360PPhtamMI1Gu3U6eXZzXVm5ubsAyrxn6MXTbHLkvA+6nKnfdFnjd5cFu+45y5PWEly6CSmZ7EA4zyL9Pkcr2HLptj1wld8zrcXstRigjBZWAgmUWyY4BeQiOAQAAIHpc/+iM/U5UIBIMV70X0b+Oop+DKDMi0fET251HbjsxEX0l4wzG9nXgWgk4CRGpzoAooggD8YPgGBAGQ6a7DogS8KQxAAAR4aJNM828Phw3/ThGQgnoBQJiWH4nZtTvTl1kwG09UiLEem+y15Th6jeOWQIKoTt5ZTD8nTAMhTXy64/tm+5+Z5qm+5+pLkf9JchwfSmEmwNTkuHq+Of/r4tjYJpyVYYsOQkzfcxXpi4Zivn+mng/hXnXkMv2qBSMYI51pumy3xUIEcExAAAAAAAAAAAAxA2CYwAAAAAAAAAAAIgbBMcAAAAAAAAAAAAQNwiOAQAAAAAAAAAAIG4QHAMAAAAAAAAAAEDcIDgGAADikumNdg4AAAAAIAKMaGegFDA4iEC8ITgGAAAAAAAAAACAuEFwDAAAAAAAN3jYHAAAAIgpBMcAAAAAAAAAAAAQNwiOAWEw8//BE6IAoogqCNFk+i9wUCBNS+I/EnrNgLUGd/KdAAVTmB5eJBdTHJzu4tq86aggRywrERRey2C5jrxOryN3rdEfxzwvF9nZ2a7WFzWuy0P+ChwezwJfN/JXUYxVof115CC9k3ofttweQUf1ng1L223TNofFxXt/zJNbN72h56IoSqHpYB+c5NWOo/snu/QeT8AyR2sM2H4Y+XF5Erwe6wpMh0XIti5zcF5K2i2o2+u6pHC2H4HnK9w2JlrHz2sGXotOuK1L7HiL+0XdNofeWwT7BeQjOAaEwW0zWTpuUwAAkGSYf3TIOk0q0917r02TF2cDJ4V7HUqSIcOmc7N4ub6SY7jfxJAiFNCK4kEwTclFB5ppmpKZICOGg2UJ8jrukP+DqQTj5LUYJvfXkFcy3OTAdB6R8E9vGHJdjsO9JzHzjn/Yu2BKCQmJMl0cw0iEFk0ZYbcHpsyIBAWifWfmavsxXAfhJNPkPJYGhrfUBHlRshEcAwAAAAAAAAAAQNwgOAYAAAAAAAAAAIC4QXAMAAAAAAAAAAAAcYPgGAAAAAAAAAAAAOIGwTEAAAAAAAAAAADEDYJjAAAAAAAAAAAAiBsExwAAQFzyyhPtLAAAAAAAACAKCI4BAAAAAOCGEe0MACWDYUY7BwAAAKEhOAYAAAAAAAAAAIC4QXAMcMh0+Vio/4N0Xp6sAxAmqg9Ekmk6LFF+33eUOti2nObBj5dGtViZLkfKmDalpjhPoeMy75/eJv/RLoFOt2/3fafXkeGyIATcG3tcTHlrRHH4VpgnP7Jlxji5zuIriW6vI7f1fknl5Dej2yNg2VYY14Dd9p3kybSrMxxkw3UZCr7i0L/rsu5w20dgdwgdHReb7zrJk8frDX1btukDlzk9q17XbbK7fbA9hk7yVOAYRq1WKyXVacGy6+Qc2H011g6JJ9d9ixBpRVZHB2VXd7m8voFCEBwDAABA1LjtyjaYywxwLdY6j0qliFRlsX0mYzv3KCm4KwAAAKEiOAZESyl9ShIAEF9ctWYnE4f7RKLhdvtAKRJuh7CR/7x9tO9N4/5iju0DYJimq3dNGTJdj76JNsP0ygxzJKVx8r9ewwj7OJoy3V3HhiQjIeyiaMjdACxDphKiGiQ2ZRjuR8NKRvjnwXQ38tPtCA+jwH9diWp7YspNfWq6TB9thqnot+cxLv/3RWy3SLEv7xxQllH0CI4BAAAAAAAAAAAgbhAcAwAAAAAAAAAAQNwgOAYAAAAAAAAAAIC4QXAMAAAAAAAAAAAAcYPgGAAAAAAAAAAAAOIGwTEAABCXTE+0cwAAAAAAEWAY0c5BTMvNzY12FgBEAcExAAAAAEB46IvLw3EAAAAAYgrBMQAAAAAAAAAAAMQNgmMAAAAAAAAAAACIGwTHAKeMoB/CY5ru1wEgLlF7ILKctWmmizbQLND2GZblTrafv4I/1mB6vWHnCcXP7nQbKr5z6PYWzCwR93BGoR9DT17gOnK4W66PwskNGifX5PU4XGPBOsBtXtyI0Madrsbu+2axHonAQudk63blrSRcWe452Aub69bJMTDcV2YukxdM7/zewAxS7Yf9/p+T2fGWiDo6NKbH5iA4yL/rXbU9B07OpXUF0Zhl1ut1dxDskjtp50vGPUEBMTzVb0nKenGfVa/H3Uuxi6IYBquji5PH6b0h4ADBMSAcpinDZd1slJKffQAAhM+UYRiufgS7Sw2gNDBUCu6tE9znP7q1oakgPeyO1hDTDMnNXiS47J1xe/zMk//v4tEXmYabB2ckw0X6gvkIl2G4CS6bkpHg8kJ0dxYNSYZphL8e05RpRLsucSm/IIcppvcdeVzWxb5HAEtawDPOeGVyDlAsCI4BAAAAAAAAAAAgbhAcAwAAAAAAAAAAQNwgOAYAAAAAAAAAAIC4QXAMAAAAAAAAAAAAcYPgGAAAAAAAAAAAAOIGwTEAABCXvKYZ7SwAAAAAAKLM9PLbEIhHBMcAAAAAAHDDiHYGAAAAADhBcAwAAAAAAACuEScGAACxguAYAAAAAAAAAAAA4gbBMcCFsGYk9nuUjlfeAABKAtNxq+b/fffPipsuG0WPh0Y1ltidbq9ZfGMObMu8g83blbbiLoGB23N4/Gwy7Pw6dLfX/qm9pqcYtx5B4WbE75QZTi+BAgnys2AW63Xkju37P0vB0CNnx8Vuh0M/CJbT7bgAuT+HdnWGk3Waptd+ucv3/4Ral+Xm5tqnd1IQbb7qJPdem2Ngf1Ts2e6rg+x7PIFbc1KNeG3OleOz57Igmn73gI7On+zvSZy0h5amK4zrMNJKTNsYBjPIv0+laF4ZVrzn0uk9kL+i6F+0q5+Kku21G8sFGiUewTEAAADEruj3PwAlgkHPQUyLxNmL/eow9vcg3pWMMxjndWHJOAkAAMQEgmNAGNzebxqm4v6eHQBQWjh9NtfKOLmO8NNzOwtIcnVv6e4qLgFifCoGw4z9/mzDNGXE+HlwyzA9J3/ohcE0JRmSYbgoC6a7Edim6aouME3T9YgZQ9EN9BuSzDDPoaGTu2+6OYfhjOQPTO36LLg8BeFeBpHhdjRzbNdjhmK/PXHLME139Ygpefl9EXWmUbwj1hC/uNoBAAAAAAAAAAAQNwiOAQAAAAAAAAAAIG4QHAMAAAAAAAAAAEDcCDs4lpOTowMHDkQyLwAAAAAAAAAAAECRchQcy8zM1GuvvaYBAwaoY8eOOu+88yRJ//d//6dVq1YVSQYBAACKgtcT2y/cBgAg+mhLAaAkoDZ2x8sBBOJSUqhfPHTokK6//npt2rRJpplXYxiGob1792rhwoVavny5PvzwQ7Vt27bIMgsAAAAAAAAAAAC4EfLIsZdeekkbN25UjRo1dPPNN/uWZ2VlqW7dujp+/LheeeWVIskkAAAAAAAllhHtDAAlBcMvAABAbAg5ODZnzhwZhqFnn31WN954o295o0aN9MILL0iSVq5cGfkcAgAAAAAAAAAAABEScnAsPT1dklSvXr2Av1WsWFFS3jvJAAAAAAAAAAAAgJIq5OBY48aNJUlvv/22srOzfctzcnL0xhtvSJKaNGkS2dwBJZDbSSL80zPpBIBwUX8gqsxCPxaetOCXC0xF5vV6nWejQHrT63GcHuFzXwdFtxYzbd68bjqZG8+0yX8Up9YLa9NGYCrT4XVouNzp/KNonDyentwYbd3CzLZpWo9fRPbermyWUKZtXkvBHJUOToHdVx21qQ6+WxTb97o8h8FqHJsq2pkQ09u1BRHZlE39GjwPoa7Unt39k6MyFMb9lzW9TS+Hw8vYvi4IndftlWCzfSd5KngdxE4NHMh0UG5LHLtzGO17TYff93hcXouuUhffOp3y2laSQGSEHBy74YYbZJqmJk2apL59+8o4WWF26dJFU6dOlWEYGj58eJFlFChp3HcDlIQmBgCASAi3TcsPRYSX3jClUtGJCkSRYfr/A8XPVPAQgZPVRO8cGvL6ApxhpTdNmTJiuhgakqOAiL8Ew7eWMEWgDBnh58A4+X9hb9o08w6f6zIQ5j2FTF8/V9gMFwfQtw43B8CUGXo3n03y/FBCDF+IyjuX4YvtfZcUUw9IFAW3gVZJLmszREJ8l2IUp5BbzWHDhun222+XYRjyeDwyTVOmaSo7O1uJiYm66aabCI4BAAAAAAAAAACgREty8uV77rlHQ4YM0Zw5c7Rt2zYlJiaqQYMGuuSSS1S/fv2iyiMAAAAAAAAAAAAQEY6CY5J0+umna+TIkUWQFQAAAAAAAAAAAKBohRwce+GFF0L63n333Rd2ZgAAAAAAAAAAAICiFHJwbNy4cYW+nNQ0815eSnAMAAAAAAAAAAAAJZWjaRVN0wz6tzp16qhatWquMwQAAFAcvKY32lkAAAAAAESZ6eG3IRCPQg6OrV+/PmBZdna29u7dq+eee04rV67Uq6++GtHMAQAAAABQ4gWfZAUAAABACZTgJnGZMmXUsGFDPfPMMzpw4ICef/75SOULAAAAAAAAAAAAiDhXwbF8mZmZMk1TixYtisTqAAAAAAAAAAAAgCIR8rSKL7zwQsAyr9erY8eOac6cOZKk5ORkRxv/5ptv9L///U85OTkaOXKkRowYYfn75s2b9cQTT+jw4cOqVauWXnjhBVWpUsXRNoCiZEZk+pTg7/IDgMIwgxMiymFzFPD1Yi6Qdm1wYe/HRQlkBJ7E4jyHbrfktV1BMV8IfscwEkfPa79jQbnept8593o8LrYfxZYx3AMRySyfLA9mMf6+sNuSo63bfJma3BmjwAEzbOrVU7Ktdx2sx2GdEbD5IOndvv8n1HfLBs9+8ZVEr+zqPbfbD/0cOq33Q0vvrCy6Ptpe6zF02ldjd//h5J7E/hxGT2zXo3+cPCf74fEGXvOmy067yPT5Odiey3rPLIJ3agero4sMv/FQzEIOjo0bNy7ojVZ+IR0wYEDIG05NTdWLL76oSZMmqUyZMrr66qt11llnqXnz5r513n777RozZozOP/98/etf/9K4ceP0wAMPhLwNAAAAlGwEeYHoo8uhlIjxzqPYzj1KjBi/DgAAQPEJOTgm2Udqk5KSVLt2bV1yySW67777Ql7XokWL1KNHD1WtWlWS1KdPH02fPl133XWXJGnt2rVKSUnR+eefL0kaPXq0jhw54iS7QBHjphsAAK+8LptEQ+GvwLQdfQTEI3dXghH1W1s3+TdkHT0TawwpIh36Ua8NbZ7ajyeGaSrcC8mQZMiQaRgyXFyM7p6uN2W6KkVmBEZZRKAUh5sJM2/knRnufYVp5qVXQth7YZimy6rAtPxPeJkwXJ+F6NZFLutSUwRYY57h6hwaEanLEBFciigGIQfH1q9fH9ENp6WlqVatWr7PtWvX1qpVq3yft2/frpo1a+qhhx7SunXr1LJlSz3++OOOtrFmzZqI5TceLFu2LNpZcCCvpSoJeXabh02bN+vQ/q2RyQwQV/64Yz16+EjU64NobD8zMyuq2491GRmZkspLitfjZ/3Vl5ub6+g45OTkBCwLNf3RY15JiZKkdevW+ZYfOLDf8bnIzcn1/XvVqtUqX6m8o/SIrMNHDod8DrOzsgOWZWQcK7br8VhGhlS1lnWhaYa8/SOHD0u1UgKWF2d94vUGTuPkZPt2fUe7d++WlqU7WIc1IOJ0/09kZlo+r167RhUqBh7XUEWrPj9yNEOq7jwP/sEMrzf0MhjMiROZDtZhbQucbjsnO7At8Hq9Ia/nUPohqUbdgOXFeR6T9uYqRdLBgwe1K6ztFpwCLO98ejwex/uwYePv0skptUwHdVFurv10bqGm978Gpbz9CDX9ntRUqWo9SdZpUUNOvztdKldNUt5D0vlWrFyh5DKhdlkF9mTv3btXy5YFlk9/efcRea8I2bljh8yTQV6vg3OQLz39SIEpyEJPn3kiR1IZy7KMY8dDTn/gwH6pah2/paFvP2P/EUmBrzAJNf3hQ8clVQg7vSQdPXJUqlzd9zknJ8dR+gM790vJ1jbdSfojR49I5Stalv366zqlbA/tvjJte5qU8sc5KAm/LdzmIVr7UPDeJic79HJwIP2Pazlfdtap28P8v+dkB6aX6bBNt4nmOEl/OD3wWnSSPiPjmFQ1/O3/4Y86dcuWLco4vj+MdUTOxo2bdOhwasjfP3HihFSpkqSScS2iZHM0ciyS7J5oKjhtY25urpYsWaKPPvpI7du313/+8x89++yzevbZZ0PeRrt27VS2bNmI5Le0W7Zsmbp06RLtbIRu6nJJikqeJ2750fLZaR4+32pN36xpUzVtVMN1voC4c7IekKRKVSpHtQ6LVh26ftUfN4gxVYeXEPv3pUtLN0uK0+NX4BqS8mYDcHIcVv+8MWBZqOn37D0iLc9Lf8YZZ2jDD3l5qVGjZsjryG9Pk5L/uJ3t0KG9Klfn/bTF5c1vA39sVqlcRV26tAop/Yo1gT+0K1asUGzX47bfbH7oG0bI29+1Y6Ht8uKsTxYt3+Nq+7PWzQtYVq9ePXXp0izkdSxeaH2I0un+/74mzfK5fdt2qlazWsjpp//6navtR8q2X7eHlYdpv/9g+ZyQEHoZlKQpGxYELCtfvlzo6/BrC5wev1/WzAhYlpCQEPJ60nYvtl1enOfxyLo0pf66QdWrV1fdEOsviwLHMH/MTWJiYsj78M7uvLq0ZfMWWrd0ft56QqyLli1bpqSkRNu/hbp9/2tQytuPUNPnpi+VTj6vlZD4R15CTe/J+VU6eEKS1LZtW61asVuS1PHMjiqXEmJ/jl85lqS6deuqS5cOp0yalZUjzVktSWpw+ulKXZ0gSUpw0B58sn2JJKlq1co6lp7frxV6+qNHM6QFGyzLKlZICTn9of1LFfjKq9C3v2frHmld+O3Jzu1p0pqdYaeXpF2/H7J8Tk5OdpR+Y5kN0o6MsLe/Y+v3AcvatDlDNWtVt/l2oNWe1dL+P4Kx0WiLZvq16eHkYeFPO1ylj4Qli3/3/Tu5TOjlYNPW/dK67ZZlZcoW3h4W/B2feTxL+m6t9QuGs+MwZ/XsgGVO0qfu2iut3B12+s2/ubsv9ClQpzZp0kSt2jRyvo4wLfxpa8Cy5s2bqWnzBiGv49eN833/jsvf+bDIysoqdABV0ODYCy+8ENYGQ51asU6dOvr55599n9PS0lS7dm3f51q1aqlRo0Zq3769JOmyyy7T3XffHVaeAAAAAAAAAAAAAKmQ4Ni4ceMsI7lCFWpwrGfPnnrllVd08OBBlS9fXjNnztTTTz/t+3unTp108OBBrV+/Xq1bt9bcuXPVtm1bx/kBAAAAAAAAAAAA8hU6raK7l7kWrk6dOvrzn/+sG264QTk5ORo6dKg6dOigUaNG6e6771b79u312muv6bHHHtOJEydUt25dPf/880WWHwAAEGd42TYAAAAAxD2v3ztUAcSHoMGx9evXB/tTxAwcOFADBw60LHvzzTd9/z7zzDM1ceLEIs8HAAAAAABhcz7pCgAAAIAoSojkyjIzMyO5OgAAAAAAAMQIg4H5AAAgRhQ6raK/xYsXa8GCBTpy5Ig8Ho9vucfj0cGDB7V8+XItX7484pkEAAAAAAAAAAAAIiHk4Ni0adN03333Bf27aZoyDOaSQBxwXcytK+CVN0AExOl1RP2BSDId3seZLhpEb5CL1u37br0FHt5C0bM7W07OoOm1+XYxVmymy8bDLqvFXS0HbM/hZWl7Dl2cg3BS+tclXtPpdWzY/rPYRezkO62LbZYVZ0F0uS2312EkRfK4OWlTbTfroBi4zbZte+5k+JfplZTol95JBuy3Fer7f7xe++/ZtjF23/MESe/yHDpqD22qPUfp3d4/2RxrZ2t0m96uLnC2hiCn0RWvg+PquOkqsU6W+5LSv+ukGNgWgui2MU637nH53jPTjPx5c1u/RCQPDg9LScgzYkfI0yq+//77Mk1TFStWVMOGDSVJ7du3V/PmzX2BsYcffrjIMgoAAIDSx+1vb8OI6CzhQOwq5qBWxJWITCCa3Dx0gZKhJJxBg8ok+jgFAIAYEXJvwqZNm2QYhl599VW9/PLLMk1Td955p7755huNGjVKpmlq+/btRZlXoMRgHnWghCkJv8SjIV73GyWKoQgURRftKpdBycL5iD2G77/ub3Ddnf/4vsE2bEZexBSP6eoUmvKeHHwSvXLg9mGNhKDjokNgmjIS5Go2HpcDDk5ewG4OgukyfX5q92Ph3Gw/IcHFPhgJUoIRdhbyjqCrC0mSEfZZiExL4MuIS2Huhelu86YR22H6iOU9hkfeGDJdddrl73osl4NSwfCGXR8ysx2cCDk4lpWVJUmqX7++WrVqpUqVKumXX36RJA0ZMkSSNH/+/CLIIgAAAAAAAAAAABAZIQfHateuLUmaNWuWpLwpFWfNmqXs7GxfkOzAgQNFkEUAAAAAAAAAAAAgMkIOjl100UUyTVPPP/+8Nm3apHPOOUebNm3SWWedpTFjxsgwDNWvX78o8woAAAAAAAAAAAC4EnJw7O6771b37t2VnJysJk2aaMiQIapZs6ZOnDgh0zRlmqZuvfXWoswrAAAAAAAAAAAA4EpSsD8888wz6tu3rzp37ixJqlSpkj744AP9/vvvSkhIULVq1TRx4kR9+umnOn78uC666CKdffbZxZZxAAAANzxuX1wPAAAAACWAaRjRzkJMM/ltCMSloMGxDz74QB9++KFq1aqlSy+9VH369FG3bt3UokUL33fq1q2re++9tzjyCQAAAABAyUSfJAAAABBTggbHJMk0TaWlpWn8+PEaP368atSood69e6tv374666yzZPBUAgAAAAAAAAAAAGJI0ODYt99+q7lz52ru3LlatWqVvF6v9u/fr88++0yfffaZqlWr5guU9ejRQwkJIb++DAAAAAAAAAAAAIiKoMGx5s2bq3nz5rr11lt18OBBX6Bs8eLFOnHihA4ePKjPP/9cn3/+uapUqaKLL75Yffr00fnnn1+c+QcAAAAAAAAAAABCVui0ivmqV6+uoUOHaujQocrKytIPP/ygefPmad68edq/f7/S09P1xRdf6Msvv9S6deuKOs9AVJkRT+92jQDilckLThBJprP2yE3pC/rCa0d5CMyBx8ubtIuVyyrItEkf7TPo6CpweM3ECrOY98t/ex6Pw1JQoBxF9YyEuXH/ZLFWquzzG3rlUNzlrdi43C8nqQt+N5xXX9ifg9DX47FJ7iT/3iDHKui9QojbCrVseUPdUBGyOwaOcuW2vNndPzkoSh67QuA0D25X4fIe0GtXjp2sssAOlIhaLezX4EQ/9+HmIFhdElUOT4PttehsBe7S2/DaXRxFKvCglYR6GqVXSMGxgsqWLatevXqpSpUqSklJ0YQJE5SZmSnTNEvvjS0AAACKhOsQL+/ABSS5DFhHLBcIG1UZD/2UAryXvgQoAaegBGQBAICQhBwcy8nJ0aJFizR79mzNmzdPBw4c8P0tPyjWsmXLyOcQKJFMGS56EQyp1D5tDACIL66aM9N0HduiMxWIANNQLIfIDJWCzthI/DaI6in0KtHF2E/D739jkdu3sOddhUbY59GQ190Dy2bept38zo3IeDnX5TjcFZy8J3Ed4As/vaHojqA280pAUUyXU4zcZz6mu2piOvORYspNOXBbFyMyvIbBKUCxKDQ4duzYMc2fP1+zZs3SggULdOzYMUnW4elt2rRRnz591KdPHzVp0qRocwsAAAAAAAAAAAC4EDQ4NmrUKP3000/KycmRZA2ItW3bVn369FHfvn3VsGHDos8lAAAAAAAAAAAAEAFBg2MLFiywfO7QoYNvhFiDBg2KPGMAAAAAAAAAAABApBU6rWLHjh3Vt29f9enTR6eddlpx5QkAAKAYRPOtDgAAAACAksArT7SzACAKggbH5s+frzp16hRnXgAAAAAAAAAAAIAilRDsDwTGAAAAAAAIgRHtDABAPjPaGQAAICYEDY4BAAAAAAAAAAAApQ3BMQAAAAAAAAAAAMQNgmOAK+7nT/Ey4wHgXtxeR3G74ygCTkuT6ZfCjECbaDrIhO1Xva6zAAfszoHrWqkYqzXTrrw4KMamTYGNdq0cie17nd6cFjhm4WzfP43pidELOWIn32ldGvj96JfD0HNQJPVImJy0Qadcl4PzaEZ5Sk6358DtcbOrSyXJK0+IKwi23hCTB6tynOyX4e4kej2BGzMc1MV2++ok+3bbd9Ygutu+ZLcPztbgX46cbt+wuYkMVjbteCNZgURAuLnx1V0uy3SkOKqLbK4Z1/WTu+SO03ty3d0DFUUpdHIdRGR7IS4rdB0l7HpEyZYU6hcnT54sSerXr5/Kli1r+duePXs0depUValSRUOHDo1oBgEAAFCaufzxXTJ+uwMxrSR0IcT7pRzv+y+VjHIId0pCOaZTFAAAhCrk4NjDDz+shIQE9ezZU7Vr17b87fjx4/rnP/+pWrVqERwDQsH9OhBZJeGXOBCnTBfDtYwC/x9+eiZCKEmojqPHcHF/aUToCXFXa3HToW36/hOTDEneiJyC6B2DBNN0dQ4N05QR5RrE/WXglZtzkH8dhp0NNw2qJMkrGeG3qaZpukovmTIM9+1I2OnN/DJghFWWDeWXYUNGuOXANOWqDBX4b/jpFX7+fSJRF7kpCe6272r/S0BwNjLNSfT3I3pMcUcbfaZMGWGWw0jd1yI+BA2Obdy4UU899ZRlmWmauvvuu1WmTBnLsr1790qSjh07VkTZBAAAAAAAAAAAANwLGhxr3ry5ypcvr/nz58swDF/UdcWKFUFX1r59+4hnEAAAAAAAAAAAAIiUQqdVfOyxx3T48GGZpqmVK1fKMAydccYZSk5O9n3HMAwlJyerRYsWuuWWW4o8wwAAAAAAAAAAAEC4Cg2OnX766fr0008lSb169ZIkjRs3TjVr1iz6nAEAABQhrzee59IHACASaEsBALHP9EQ7BwCiodDgWEFz584tynwAAAAAAAAAAAAARS7k4JgkrVq1StOmTdPBgweVm5tr+51///vfEckYAAAAAAAAYocR7QwAAACEKOTg2OTJk/XII48E/btpmjIMg+AYAAAAAAAAAAAASqyQg2NvvPGGTDNvPvHy5curSpUqSkhIKLKMAQAAAAAAAAAAAJEWcnBs165dMgxDd955p+66666izBMAAAAAAAAAAABQJEIe+tWkSRNJ0sCBA4ssM0Bs+GMWdTOM1CaTsAMASoGANtBB+5Y/G4E/b5DlofJ4vK7So5jZnm53ZcDZ5m22VXybLyLObjRNw+77Dg9ChG9uPaaz67iknLJg9dop07ndruXwG/mZcblWB9u3Pf+OGgSb5KXgB5PrXQhvBUYU3vhlW5c6KgNBFntCTB4kfaj3FF7Zb8hJ1Wa/pdBX4LWp95xcxbb76uA6MuV2++7vv9xWW6GWl2C8difcQaYKnoOY7vMpYfWvff1iz2Nzvty3hsV7PEyX11IxNv9Fx6YMmt7SsGMoqUIOjt19992SpAkTJhRZZgAAAABnStaPeMSwmO9RiPX8w73YLgOxnXughCgBt0UlIAtRE8/7XqpwIoG4EfK0ir///rs6dOigd955R3PmzFGzZs1Urly5gO/9+9//jmgGgdLJlMGvPwBAaWDIRY+mKcNw9lSoX/IS94QrYlesl6Rw828obwQZ96ZRZEoKMnrFiWiX4bDrcuWNtDBlxHSQOsEwXYwYMWUYhus2LRJHz01d4ir/Zv62XY+nDCuVIfnOQVh7YUpGgvtzaNiM4go9C6Zkuhw/GO2KxCVDLnchdqsgSSf3PYbr0UgwTVNGRMaMxfdxjDavwYwgKB4hB8f+85//yDAMmaaprVu3atu2bZa/m2bezRzBMQAAAAAAAAAAAJRUIQfH6tWrV5T5AAAAAAAAAAAAAIpcyMGxuXPnFmU+AAAAAAAAAAAAgCKXEO0MAAAARIPpZR55AAAAALGPXzbueOP8XW1AvAo6cuwvf/mLJOmJJ55Q5cqVfZ9PhXeOAQAAAAAAxCM6mAEAQGwIGhybMmWKDMPQgw8+qMqVK/s+B2OapgzDIDgGAAAAAAAAAACAEitocKxevXp5X0hKsnwGAAAAAAAAAAAAYlXQ4NjcuXML/QwAAAAAAAAAAADEmoRIrmzXrl2RXB1QIllmUA8+02jo62NKdsC9OL2OqD8QWQ4bNdP6fWfF0f7bpoNCbdpk1/R6HeUC7tidLSflwLT5drFWa3YbK2QaeX9em/R25bIomZG4GfXjtduxEEXi/Lm5jk0H5y/SvBFqlCNyDIugXARltykHm3dR3CIukvdVbtflrC6NLtPmknVSF3rsVqDQr6mg9w6hpveE9DXHHJ0Xm7w6KgMu07up94Nu33E15F8OnOXJv7xE4rpw0hqZJakyi4QotqeWNszBYS2S3wEuD4PTUuEpgb9lPJ7o58nJb8Rwvo/4FnTkmJ2JEydqwYIFOnLkiDyeP+4gPB6PDh48qG3btmndunURzyQAAABKJ7c/vQt7Jy6A2BHvV7IR90cApQGlOPZF5BzGccc01wAAxJaQg2Mff/yxnn76aUl5EVjDMHyR2Px/0zmBeGHI3U1PXtr4vWEEIo7mB4gqI8w2Le9e0nDRiUJbWtLEcnUcy3l3xTRlyojIU7bujmH42zdkyojp6sBuHGVsMUyvDFdlKK8cRvM6dNudYXhPtmnhMKUEIy8P4WbD7ehFU+5GHZonz6G7HERiapbwExoufukbNv8KJw/uLqOTdYmLYyAj0UUGIim84+i+NjXl6v4y1ivzfDEc4DQk+yGtoTLNqI6cQx6zwH+dIj4BJ0KeVvHzzz+XaZqqV6+eOnfuLEm6+OKLdeGFFyohIUGGYej5558vsowCAAAAAAAAAAAAboUcHNu+fbsMw9CLL76osWPHyjRNXXHFFXr99dd1//33yzRNLViwoCjzCgAAAAAAAAAAALgScnAs/x1j1atXV6NGjVStWjUtX75cknTRRRdJkpYuXVoEWQQAAAAAAAAAAAAiI+TgWN26dSVJH330kTwej84880zNmDFDu3fv1rRp0yRJ6enpRZJJAACASPN6Y3cufQAAAADw4T1Lrng9/DYE4lHIwbH+/fvLNE29//772rFjh8477zzt3r1bF198sV5++WUZhqGmTZsWZV4BAAAAAAAAAAAAV0IOjt1+++0aPHiwKlSooMaNG2vw4MFq2rSpTNOUaZpKSkrSvffeW4RZBQAAAAAAQInF4AsAABAjkkL9YmJiosaOHauHH35YkpSSkqLPP/9cU6dO1fHjx3XeeecxcgwAAAAAAAAAAAAlWsjBsQEDBqhZs2a6/fbbVaVKFUlShQoVNGzY/7d352FyVPX+xz+nZyYbCSGBLEhAlGC4GgIYdn1AUWQN7lcU5fKgiHHBB64oAio/HxDRYLxX3EC9LCYQBEJAYoiIQTCRLKwxhCQsIQSYSchGVma6zu+PnulMT1eTOnWqu7qn3i8fw3RPnapTZ6/6dvV8umqZAwAAAAAAAAAAAJIU+WsVX375Zf3tb39TU1NTNfMDAAAAAAAAAAAAVE3k4NiRRx4pSVq+fHnVMgNkkuVL2QHEw+iBJFnHFuW6fUnaSkk9G3UQ0CtqyhjfHZS9E3ju0YXvEszUNLfRWN8qkWQdC8b3mD3HknyHa7kmcNL1JIk6THmF4HL0sObWO0by6GeR9vmGtRfr0BBt4DcW2gpzd5CPWjLh20UdyoJqXY879OV8WBE6dSSHbcOSRy7rCukTmQ79Br9Aea/dhc59Dm3bde6stti5qbsp1WEsCqmD1OvFca1s87ve5i3Tp32+iQi5PuAaD1UU+WsVP/jBD+qpp57Sd77zHd1222064IADtNtuu6mlpaVku4suuijxTAIAAKB38o2vGO8ADdCpwa+7G74nNHj5IwkN34ozjxpMn/e6KplsZBblBwCNJXJw7MorryzefHjsscf02GOPhW5HcAyZYIv/eKQHAKDxWY+nZ7puIJiYn3I0kqyJ/EUIwFtq+BtaHutLU+hNiWUFboykXOQr87eQZhVaK+PxyW5jG/8LNYwJYj9C0zX+WI/Ihv+zvFa2czSIf3y/XBijBBpC/PQ5I68ykIxkjExKtwqM9cy9tV5tsLifNAcjK682lPaTt74afi2TCOv/JH0Sj+LDU2P3RTQOpyV473g8EwAAAAAAAAAAAFkVOTi2dOnSauYDAAAAAAAAAAAAqLrIwbHrrrtOxhidc8452m233Up+99JLL+nGG2/UsGHDNHHixMQzCQAAkLxE/no4AAAAAKCBBTG/FhdAY4v8Rxquu+46XXfdddqyZUvZ77Zt26apU6fqpptuSjRzAAAAAIBwCfxpGAAAAADIpIpPjq1YsUITJ05UEJRGzj/zmc8ol9sZU7PWauPGjZKkfD5fpWwCAAAAAACgnhGzBwAAjaJicGz06NF673vfqxkzZkiSTOfHEl999dWKOzviiCMSzh4AAAAAAAAAAACQnLf8m2Pf/va31dHRIWutZs6cKWOMTjjhBPXr16+4jTFGffr00ejRo/XpT3+66hkGAAAAAAAAAAAA4nrL4Niee+6pa6+9VpK0Zs0aSdJVV12lIUOGVD9nAAAAAAAAAAAAQMLeMjjW3S233BL6/tatW2WMUf/+/RPLFFDPbLcvUbdx0vf4FnYbZycAStGPgBQk/1dFrGdnDmyw641QVS41GFbfJqjdgB52fJejBzb9v6zju44MS+66y+6lEGttbHuujRuzH8eti+o0oxr2o7ATdzmpOroYSjYr6Y8PUfXsgzH24Je6wrgfdU6vNG2Ets2w9Pnw7Xpet78V/6ZTfq6+86lLEwzC5kOX9AmM234lINl8z3cc23VIe3E5q0J7KxzTpe3Um2IpmDTPwXb7KXo7sEFIjXmeR61nqKC8ITupRn6DGq7NK4k6nsfdHtmWc9n4iSee0E9+8hNJUhAE+u///m8dfvjhOuKII3TNNddUJYMAAADovXwvvRv39gOQtMa+EZD1vpz180fvQDuuB361kEgdNvZ05IU+UC+oCQDRRA6OzZ8/X1/4whc0depUWWs1ffp03XfffQqCQB0dHbrxxhs1ZcqUauYVqCPWc8Fn6+pTkkDDY+0LpGbn52TjMcbjiTEryTh91gtV1sjDcSPnXYqff6POT7knUABeu8j42jiZs0+vDI0keT09EqT+tIXvgxJNkoLY+7DKGd9M+D29Y63tqsi4O/DKv0l7COg6/8KgGGcHMjnPwJS1kolfj13rKe8PHnmOx8n05Lh78W1IDX6vxkomibmgkctAks942FV+qY9JGRcoUNz+bFJ98hGNJvLdhN/85jdqb2/XsGHDtGXLFt1zzz0yxugTn/iEPvCBD8haqzvvvLOaeQUAAAAAAAAAAAC8RP6bY88++6yMMfrxj3+sPn366PHHH5ckXXzxxXrttdc0Z84cPf/881XLKAAAAAAAAAAAAOAr8pNjGzdulCSNGDFCjz/+uN58800dcMABGjJkiPr161fYWY6vtQEAAI0hX+EPsAMAAABAI+HKxlPDf5UkgDgiPzk2dOhQrVmzRgsXLtSjjz4qY4ze9773SZJuu+02SdI+++xTnVwCAAAAAAAAAAAACYj8qNf73vc+WWv13e9+V3fffbck6fTTT9e0adN00003yRijCRMmVCufAAAAAAAAAAAAgLfIwbFvfetbes973iNrray1Ovvss3XwwQdr//33lyQdddRROvvss6uVTwAAAAAAAAAAAMBb5K9V3HPPPXXHHXdoxYoV6tu3r/bbbz9J0kEHHaRJkybp5JNPVnNz5N0BAAAAAAAAAAAANecUzTLG6MADDyx5b/DgwTr99NMTzRQAAAAAAAAAAABQDU7BsVmzZmnmzJlat26dOjo6Qre57bbbEskYAAAAAAAAAAAAkLTIwbHf//73mjRpUvG1tbZsG2NMMrkCGoZ/my/vSQAQjWUEQYp8Wl9QIXHY+rLy8cvnYBsEcbOEVITUYQ2PXo1j1XxU9l2Khly/Ofcj72vA0lLLOx7f1sklqMv4VcrvBErHQtOZl1oWSsixjMNYHrvc6ptLFYSVQNgc55Lejd8efGfeim0gatuosFkQcSwJbHXWDi6lGoQujGrYj4K8V3K/3BcY33Pokd51b6GtwCFPgbVK4v5QUuJOA6YuzmFnHnz7UT1cLwdBoFwuF2nbfN63H/glrwe94BTQYCIHx6ZMmVKccEePHq3BgwdH7twAAABAuHq4CAcyjm5YB5KohMa+pdTYuYfEUFIXvD80kUguAABoCJGDY6+//rqMMfrhD3+oT3/609XME1D3jBJYM3L1BwDoBfweGun6bHz8nfDNBUAnr7VlY3/o0dgGv59rOx+28nxyItUyCPLKeeTfdtahw0Nndcco8HrqwpicZEzsMrCFTMQ/fte/XsePnwHbuSYw3hfK8dN3lUGss7DqPH/fZ298zt90lqJPGSQxH6TXkY1Jdyxs5DGs1/CcS6nCOpETlYGaiDzr/cd//Ickafz48VXLDAAAAAAAAAAAAFBNkYNj3/72t9XS0qLf/va3evPNN6uZJwAAgOrjk2gAAAAAkHl5/nQxkEmRv1Zx+vTpGj16tO655x797W9/06hRo9SvX7+y7W677bZEMwgAAAAAAAAAAAAkJXJw7E9/+pOMMbLWavPmzVq6dGnZNq5/8+Hee+/Vr3/9a7W3t+ucc87RWWedFbrdnDlz9MMf/lAPPvig0/4BAAAAAAAAAACA7iIHx4444ohED9za2qrJkyfrrrvuUp8+fXTmmWfqqKOO0ujRo0u2W7t2ra655ppEjw0AAAAAAAAAAIBsihwcu+WWWxI98Ny5c3X00Udrjz32kCSddNJJmjVrlr7+9a+XbHf55Zfr61//uq699tpEjw8AAAAAAAAAAIDsiRwc6+65557TihUrZIzRu971Lu2///7O+2hra9OwYcOKr4cPH66nnnqqZJubb75Z7373u3XIIYfEySYAAAAAAAAAAABQwik4tmrVKl1yySV67LHHSt4/4ogjdPXVV2ufffaJvC9rbdl73f9m2bJlyzR79mzdeOONeu2111yyWbR48eJY6bJq0aJFaWfBQaGtpJ9n652HF1e+qC0bX0ooP0CW7Jwz3ti4KfXxII3jt7/ZnurxG93617dIGigpq+VX+rdiOzryTuXQ0dFR9l7U9BveCCQ1SZKefvrpne9v2OhcF9u3by/+vHzZcrVuaHNKDx/lf29446boddge0oa2bttWs/64bevW0PejHn/z5jek3QbFTp+EICi/pnI6fuc1Wfdrs9a2Ni1atL1SipA8BPGPL2nHjh0lr1esWKH1m9Y77cPn+EnZunVLvDxYq+59yVr/64sdO3Y47KO0H7seO2wuCILo57DpjU1S/4Fl79eyHptf69AASevWrdPqWMfdWYaBLfSHIB84n8OzS5cqn89LkqzDdWbQmaanqOl79kHX469du1baa19JUntHu/pKsg7Hb2tbK+1e+OD0093u4fx7yRINXNVvl+k7OqykXNn7r7/+eqQ8bN68XVJ/SdLKl15SYDvrIEZfXL9unYL8zjExcj/YsE3SgJL3tjnMhxs2bJT2KpyDuo3nUdO//vJaqaVQB3mbV4tj+jWtb0javex9l/LbvGWrNHTn6/b2dqf0r61ulXYbGfv4W7dslgYNLnlv2bLlam1bHSn9q6+9Jg1+W6xjV4tvHtI6h65xVCrMMVHz0dq6sy93aX9z1+m7fr9lU3k/7P77KKzK12WPLXpMJle+Zg6z5pU2qWlE7ONv27at7L149bgzv6tWrVLeboqxj+SsfPFFbX8z+tpw27Zt0qDCGr0e+iLqW+TgWFtbmz73uc9p7dq1ZYGt+fPn66yzztIdd9yhvfbaK9L+RowYoYULF5bsf/jw4cXXs2bN0po1a/TJT35S7e3txeNPnTo1apY1duxY9e3bN/L2WbZo0SKNHz8+7WxEN7MQoE0jz9NWPrrzhTHOebjtpfklr/d/+/46aPSwClsDqGjmzg9qDNp991THsLTG0KefbC3+3FBjeJ1Y+fyr0tJXJWW0/GaWftipqbnZqRwem/9cyWvrMCe++NI6afGLkqSDDz5YT89ZIEkavMfgyPu4bWVhPu3Xb+eNswMOGK393rVfpPRIQI82JEm77z5Y48ePiZR8wZN/KXuvf//+NeuPyxe3hr4f9fgvPDfHK30SHnn8vpLX1vH4s5Y+JKn0Q4rD9hqu8ePfE3kfc/+5LPbxJWnJU6X1cMA7RuuAg94ZOf3MZ//R7fjua/OkLHvq+eLPLnn484q5pW+YnFP6e1b8s+y9vn36RN9Hj37sWn4Lnrq/7D2Ti34Oq178R9l71tS2H21a0qbWZ5ZpyJChenfE8atEtzLMmUKQJpdrin4O9xVu3I056CA99lihPZiIbXnRokXKNTWF/i7q8Xv2wUIGovelTa/uvBnb0twVVnFIv+ExqTM+d/DYsXpyUeGDowcd9G6NGDlkl+m3b2+XHny67P0999wzUh7Wtq2XFr4gSXr7fvtp9eJCeRqHMrjl5cI6ZsjQoXpjw4vq3EHk9K+8vEZ6alXJe/36RZ8PX13dbRzoHM9dxuNnm5dJqzdLkppMV3uKnv9nn1kpvfB62fsu/fiFJWtKXre0uK1Ln9j+pLSxNFDskv655eU3/0cfOFr77bd3pPQdGxYU23Gtx7Auf+mc07vEycM/FrxS+CHGva6kzJu7ovhzs8P1yZNPrpRWl7bDlpaWt0zf/Tq+9bX10mMv9NjCrRz+9vQDZe8dNv69asqVB/DDLO2zVFpV+uEtl+MvXVwezI1Vj93mtVGjRumw8e9y30dMDy9YVfbeqH3307hDR0fex5Llc4o/Z/I6HyV27Njxlg9QReudkq677jqtWbNG/fr100UXXaRp06bp1ltv1YUXXqgBAwaotbVV1113XeSMHXvssZo3b57WrVunbdu2afbs2TruuOOKv7/gggt0//33a8aMGbr++us1fPhwp8AYAAAAej9ron0SE9gVU/5h34bS6D3BhHzaGtlibaO3YgCSxHAOAGgUkYNjc+bMkTFGl112mb785S/rkEMO0WGHHabzzz9fl156qay1evDBByMfeMSIEbrwwgt19tln62Mf+5hOP/10jRs3Tuedd17J1+sAdclzsVe4+cKKEUgM91KA1IR9fYhDYhnjc1PcithYfaE60hSvHxkVnriyNtjltlH2lZ7GXVubzn8auf8YSfJoQ7lidDq9evSfT/zmJNPVEGIf3ZeVlYmdAyPJmMi3mELSF8ovrRZQOL7xaghGXenjr2t8zr+wnopfh7KFp57qQ7yMWKuSr5Z0Tu/ZAtMuPiObTCfyKMP0WRmP/Bvr2Y+QiMDjU2uGC0Q4iPy1ihs2bJAkvfe97y373WGHHVayTVQTJkzQhAkTSt674YYbyrYbNWqUU+ANAAAAAAAAAAAACBP5Yz0jRxb+sOVDDz1U9rt//KPwPeF77x3tu3gBAAAAAAAAAACANER+cuzDH/6w/vCHP+jaa6/VypUrdeSRR0qS5s+frzvuuEPGGH34wx+uWkYBAACSFCTwFWIAAAAAgEbHtSGQRZGDYxMnTtTs2bP18ssva9q0aZo2bVrxd9Zave1tb9P5559flUwCAAAAAACgvnn8mRgkhCoAACCayF+rOGjQIN1+++06/fTT1dzcLGutrLVqamrSiSeeqFtvvVW77757NfMKAAAAAAAAAAAAeIn85JgkDR06VJMmTdIVV1yhF198Uc3NzRo1apQGDhxYrfwBAAAAAAAAAAAAiXEKjnUZOHCgxo4dm3ReAAAAAAAAAAAAgKp6y69VtNbq97//vT72sY+F/n7q1Kn6z//8Tz3wwAPVyBsAAAAAAAAAAACQqIrBMWutvvrVr2rSpEl69tlntXjx4rJtZs2apaeeekrf+MY3dOWVV1Y1owAAAAAAAAAAAICvisGxP/3pT/r73/8ua62GDh2qTZs2lfy+6/1+/frJWqspU6Zo9uzZVc8wkDYr0+3nOOlLBTbOXgAg3hgEJMXsepOKuk99ptIvImdg53LWKvDIFWovpBXVcGCzPQ5mJFnj0LJD2mutx+Xy47n1zGL6kmRu/ch4jQbdirHzv4HNu6UvzUxqTMzK920zYelr2w79jhZ0S9671jXRzybtOgyfej07k9NQWmnM8ZvTbcQ1Rb7CYWpZB0EQNp845MD3nkKwsxC6qs5ljz3L2jimLxzPr831vK/iXiIhKVyaYMV23GBc1kF1loewPu/Uj0ry0Jk+ieJwyEJHyFhQtYNF3WONm7bvWAC4qhgcu+uuuyRJ48aN06xZs3TssceW/N4Yo5///Oe699579Y53vKMYIAMQRe+69AMAZFegQCbmTZlCqji3UFRMWQ/X8EA9iN0VbCHAlX5Xir8+Nn7J64BNJv8pfujOWN/bWVapRjcTkJNVEPMUjLWFmzO5nEzsxuAZTDKxbyN37SGBG8kmdqC5WzZiJzPGxG6JRpKM52hqPW7md+3AxA/WF46exHyQ5oDs2YqN9dpHfYxiDT0hJiSRSTWBfSCuwoceqQNUX8Xg2PLly2WM0UUXXaRBgwZV3MG+++6r73znO5KkJUuWJJ9DAAAAAAAAAAAAICEVg2P5fOHrLPbZZ59d7uSd73ynJGnHjh0JZQsAAAAAAAAAAABIXsXg2KhRoyRJCxYs2OVOFi5cKEnae++9E8oWAABAdUX9WxQAAAAAUNf4rnEvYX/7D0DvVzE49qEPfUjWWv3kJz/RokWLKu5g4cKFuuaaa2SM0QknnFCVTAIAAAAAAAAAAABJaK70iy996UuaPn262tra9PnPf17vec97NHbsWA0ZMkRBEGj9+vVavHixnnnmGVlrtddee+nLX/5yLfMOAAAAAACAusHTFwAAoDFUDI4NGjRIf/jDHzRx4kStWrVKixcv1r///e+Sbbq+jmjkyJH69a9/rSFDhlQ3twAAAAAAAAAAAICHisExSRo9erRmzJih2267TTNnztTSpUvV0dEhSTLG6MADD9Qpp5yiL3zhCxo0aFBNMgwAAAAAAAAAAADE9ZbBMUkaMGCAzj33XJ177rnq6OjQpk2blM/nNXjwYPXp06cWeQQAAAAAAAAAAAASscvgWMnGzc0aOnRotfICAAAAAAAAAAAAVFUu7QwA4A8WA97oRkDN+XS7wOYrvO93/MBlB6gKtxoo37qRajAIe9PUOhfJH9A6VoLtVmtx6q9nmnzedS81L/RQ1rXgegnfszahPSmdOk2yCq3vKTik9812eProew2be13yVGnqjjqnV+x7kSs0dDRXbdthSB4cCjG8qKLn33f8Cq0rx+LzXRFYzzVgWGqXcnGeuqrBdv/Rs/2aeplbHbYNyvtRXVSL5zm0t7dX5VjR91lpjKwd12u8rK7JEA/BMQAAAKTG/9q7Pi7e0fiy3JK4hYB6QDtsfFkeR3uLROow05050ycPAA2H4BgQi5XxXPMYFk1AcrgSB1JjPDqg6fxcq9eMaFjO1pPGHo4bfG3m15ESyYLPXvxy4P0Z+brQ0OdgA5kKTwVHSm4la9Mdz30/rOFzfWelzrszafYiScZnVpd3IRbWBNV5Fm6Xx7ZSzsjjHGznCRivewU+JWglGd91Uc7UyXToUQ8+rOcu6qLs4FUN1lKPdcCa+KOAqZMnH9EYuJsAAAAAAAAAAACAzCA4BgAAMqkOvj4dAAAAALzxsJMf379bB6AxERwDAAAAAAAAAABAZhAcAwAAAADEw591AAAAANCACI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgGujF9y2/N1zzcAICLGDyTJtTmVzWcOaYN8+Nax2nS3edlW2C/qU4VWULPjBz6NuML2abZAI8nGXad270dlBbMLSZ101wBgg/i7SCgraRy7qwqSOAdb0wVCeaNzmg9CGm3WRnL/8/W8QPXdZ7c+a+LkpUJ7jToWVWruUbtBUOk4JnrNWM9+kA9Zv7ik736uccYSmw9J79IEehS2kXVv157jVs88uM6HYTOPy1jatalRY49htsd/082Fm4p9OZbkxlWnOTnIl71l8w7roipUXPxVGdAYCI4BMRSmyfizjvFLDgBA3Yhx+6MktVS4iRIvuY13Iw4I0fgtKV4/KvS/WLcykRDT9Y9nFaTZhh3iCOHpO1tgI/fDnMfNfWOlnIxXAeSMX+n51qH3ENJZft75iHv44pokZjlaySgnedSDkVXgUQCFfuTTDuojNOLFFv/x2UHDMraxx9FEWCu/euxcGTV2U2h4VrX+sA+yiuAYAAAAAAAAAAAAMoPgGAAAyKTA46u7AAAAAKBuZP6RMT/Jfi0jgEZBcAwAAAAAAAAAAACZQXAMAAAAAAAA3nh4pR7wBAwAAFEQHAMAAAAAAAAAAEBmEBwDAAAAAAAAAABAZhAcAwAAAAAAAAAAQGYQHAMAAAAAAAAAAEBmEBwDAAAAAAAAAABAZhAcAxzZCj8DSFFmO2NmTxx1wMjETmuDCm3XRm/TYVsG9InUudRA2LYOTaAq3A6ffnuzHv2woDx94LGPOCVie6TqqDQ+7PrwPV/UVM/ziJyuLMv+51DLlhl6LJNePfhIdPwpr9jKQsqrpnUYcuK+Y7mLwIaPOkHksSB8uyBihVZak/iPrw5CsuBUrmHn6pD9qGVVTX6tMLwduzAh7cBlXI87B1SLyxBUqg7G7+5josN84juWVU/0XORD2mH0sbA651vxuq1KwsZe1/7tOx4gWwiOAQAAoHHVwTU8ACAJDOiAL+/YdBLB7Qzfl2YUA4DGQnAMSE2GV4xA0rgKAVLj9UlZ2/X8mcc+cixn60kjD8eNnHdJMh7dyBgjk8CnbH3K0Cttr1hW+zyP2ynVcgjU5JHadM0mKZ6Db0zAKIjx5GUXK2Mka0z89ux9AlaBz1PhsvIdSZN56DBeARor5YwUeNSB8V2SWHk+wuhXC4XzrpfZMG4+rNc4kvY4lAiempFfJdokZmR4siaQiTueN+gT7EgHdxMAAAAAAAAAAACQGQTHAABAJvGZSgAAAABAnL+6CqDxERwDAAAAAMTDN9cAAAAAaEAExwAAAAAAAAAAAJAZBMcAAAAAAAAAAACQGQTHAAAAAAAAAAAAkBkExwAAAAAAAAAAAJAZBMcAAAAAAAAAAACQGQTHAAAAAAAAAAAAkBkExwAvJkaS0jTW2oTyAiB7YoxBQEKses5f0dujDYLw92PNid2OG+RjpEc9KW9X1T1aGePQjiO+V02+s8DO/Hbbkw3vn9VjSjJTaXxw2VUabOB3cNNVGY67sQ5ttjr8ekLYuG/TPqUE1PIcvMfNsDbkkH/fy9mg4g6ijQVBheRR1xRBpR04CK/v6IUYhI27DtkqPVf3xhfIb/2Uz/fMv3ur9K2Fyu0o/vGtQ9voXoUNfYenq/mkObd061AuLSnRe2ulSxMvLkOMDRkL3NZFVWh9Nb5nGdb0ar40RaYQHAPisNbv2tvamk8wAABUg5XP/WgrGeM1JfaCe6hAQnw6Uj30JM+1cSMvrW1SgeH0CsEkcGO6cIXVuBWZk5U18fNvZBR7VrPWvwtZSSanuKdgZCXjc4up6xo7iROJyeQUtw6K52+M17rIJ/v+txisTyssSntG8Tu+lU8b9BgCEpF22dcHv6sT08DzUG9CLaBWCI4BAAAAAAAAAAAgMwiOAQCATHL5qhQAAAAAqFdc2fjJ5ylBIIsIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4Bjmz3F8YzPYBkZLRjZfS0USXu7al0ErQOc2I+icZryg+YDxLYL7y4VW1Io6nhwGY9jxXa3ELaZTX1PAXrujgNqwKPfhSnSMvSBPnY6dOcF23MozvXWZR9pr5AcDin9DNblEZWOjo6KvzGvV3EzX7ctltMH5LcuoyFFcacvO9iIXLy6iweXNZFNqQQvevFoQ3ZkGHXe2xynQ/LysDx/IPS7V1LL2x7lzqwQbd2VNulQMLqIPPdsuBSj0EQ0o+8x/UEysMhD2FjgctYWI15LKjx5BjaFx0Xp2HlCFRCcAwAAACNqw6u4dE7mCxfR2f53OsGgxnNsPHRiutAjWNaQD1iPgEQFcExIAWsN4GE0amA1BhZn4+ry0gyMT/dZ2S5i1NnqI30xC17YyUZk8iNpDTrP5kzSIfp/Me3/FIt/65x3OvT2ibdc/ANKiiQ8dhJLmekXFOsMjBKYjq06daAlQpnklZftspJMiZ+KRh19cP452A8PqlhbGcdehRhIk+zJvLURrx82G7/xjtq4NWX0p+L0j5+PbAycnsKvXwPrGnTZn3GQq4P4YDgGAAAyCS+bgEAAD/MpABQJwgI+GFCAzKJ4BgAAAAAAAAAAAAyg+AYAAAAAAAAvMX9qmQAAIBaIzgGAAAAAAAAAACAzCA4BgAAAAAAAAAAgMwgOAYAAAAAAAAAAIDMIDgGAAAAAAAAAACAzCA4BgAAAAAAAAAAgMwgOAYAAAAAAAAAAIDMIDgGeLBJ7COJnQDIJIYPpMmr/QVB+NsOO925qdnlflGfbEgrquW41vNYJjRHlRmXBlst3Zq/bI/XDmy3hG6lUJoJvxIppM47l2vMk06Yyeyivrz8XUqi+6idxSK0ldq7U7P27AM2rA6j79PanbVo4mTFVpi7I7aHysmj7SCfD9+upvOR5/LFezqyoauq6MdPYD50aXPhO/AsxLBTcDqt7hunPy/FrRHb47+psBV+dkjo16K77TGBqrQOk1vYGsgG+ejHirxldC75T0Z5oXOFh2oiOAYAAIDYCjef4l+CG0kyMdNn8EYqUDUpRybiDgNSPdyG9OV9Wzh9nR9OiHsexgYyMg09rvvcXDFdJ+7REezOvcRWqIX4c7pfS7adwTXf3uCxJvE5vpWMMYUy8KiIwOc2sLGyxniUYCG973yQ7njm+TENm0YwIFkNP58kwMSK1HeyKsxHjTwh9QJ+15hAdATHAABAJiXxKVcAAAAAQGMLfJ8+BNCQCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgGujEl0d9Ymujsgm+hHQM35dDtbIbVR4LFXyTKpps6tBsrXVI1Uh42T08rCzsGrDmIsk3sezlq3caBe6iGIc/IhXM+nXs4/tpATsAmVpaskh5+ouwoqbOiSFd9sV5qTa3X8Sj0+iDgWVMp/5LGs4mbR26F3GYScq8s+w9ZP1qEbBWFllUA37OjoiLxteX25lWq+51zilDp8+9ByqaB7DTT0uGzKfqi5krbrcP8tCBtQTWPVRti45bQuq0a11UER2kqTZaXtG+h6AukjOAYAAIDGld61O1BXTD3cvUB8VF9qQTGgN/HtRYn0wgzfmG6wWEzvRT0AiCjV4Ni9996rU089VSeeeKKmTJlS9vsHHnhAH/3oR3XGGWfoq1/9qjZu3JhCLoFyRlbGZ8Fni/8ASAL3UhADnyhLhpVPF7Re/ddIyjEA1JVGro2s3tAykoxMHQTX/J5HbeS2l5g05zXvx5fSbn/+XxCSUz7+U1jF6TBeJuqi/VspZzxuMdmu8civLcRPb2SMZI3xKE8jeaX37wfWcz8Jf1GOh3T6QuGppfTHo7iMVSNnPxlxh+Fu85CVoRxTFsjGXhuY+hnI0ABSC461trZq8uTJmjp1qmbMmKFp06ZpxYoVxd9v3rxZV1xxha6//nrdc889GjNmjH7xi1+klV0AAAAAAAAAAAD0AqkFx+bOnaujjz5ae+yxhwYMGKCTTjpJs2bNKv6+vb1dV1xxhUaMGCFJGjNmjF599dW0sgsAAAAAAAAAAIBeILXgWFtbm4YNG1Z8PXz4cLW2thZfDxkyRB/+8IclSdu3b9f1119ffA0AAAAAAAAAAADE0ZzWgcP+zkfYd4K+8cYb+upXv6qDDjpIH//4x52OsXjx4tj5y6JFixalnQUHhbZSD3n2zcOqVS+pfevqhHIDZMnOOeONjZtSHw/SOH5HR3uqx290ra9skpoGS8pq+ZWuu/L5wKkcgiBf9l7U9K+1bZfUX5L05BNPFt/f9MYW57rYsnmztHvh55UvvqQtdqtTevgoX7tv3LQxch3m8+VtaMeON2vWH7fv2BH6ftTjb922TdpjSOz0SQi7popz/I4gkJoKP69bt85pHz1z4Hr89vb2ktcvv7RKQVPgtA+f4ydlx47tieXBN317e7vDPkr7seuxw/qxtTbyfrZs3SLtPrjs/VrWY/NrHRqgQttfHeu4O8sw6Gy6Ucugo71DUosk6d9Llqi9vaNrB5HLIAjC+0vU9D37oGv6Des3SHsX6nD7ju1q6TPQKf369euloXtLkp588qni+yuWP6d1G17bZfodO6zCPve9YUO0+Wj961skFfL8/PMvKOhs01bR66DLmrVrlO/oKL6Omn5t62ZJg0ree9NhPtz0xhZpQGExZIuN0GFdtrpVGjhSktTR0aEm9ZUUPf0rL2+UWvYoe/+xRY+pqbkp0j62b99W8tptHJPaWluloaNK3nNJv23bVmlI6Zz+/PMvaNMbayOlX7t2rbTXvrGOXS1pz0VxdV/b5PP56O24dbPUt7QftbfvOn3X719fU94Pu/8+irC/O/nEk0+qpTna38Ba/fJqaeC+Je899dTT6j+of6T0O7aXr23j1ePO/L762qtatCh8zVwrr77yihYtin6Nt23bNmlQoS7roS+ivqUWHBsxYoQWLlxYfN3W1qbhw4eXbNPW1qYvfvGLOvroo3XppZc6H2Ps2LHq27evd16zYNGiRRo/fnza2Yhu5mOSlEqep65aUPLaNQ9TeqTfd9/9NO4/RnrnC8icznFAkgYN3j3VMSytMfTxJ3Y+cd1QY3id+Hef56VVGyRltPy69SFJamrKOZXDgvnPl70XNf2/l7wsvdgmSTrk0EP0+IP/lCTtPmi3yPvomk93Gziw+N7b999PB733PyKlRwJ6tCFJGrz7YI0fPyZS8n89Oavsvb59+9SsPy55Kvwr26Mef9mzf/NKn4R/PPEXr+Pf9+w/JEnNuZ03locOHarx498beR9z564oee16/k92m8skadR+++qw8YdFTn/vske8jp+Ufy/aWQ4ueZjx3Lyy91zST39ubtl7LS0t0ffRox+7lt+/nrq/7D1jTOT9PLf876Hv17IeNy1pU+szyzR06FCNjDh+lehWhl1dKWoZ7NjRLv3taUnSe979bs178rGuHURKv2jRIuVy4V8IFLUMn3wiPAAVNf2alZuKP/fr2885fetrO9vwIYeM02MLXpAkjT7wAL3jgH12mX7TG9ulh5eUvb/HHoMj5WHl869KSwvzwTvf+Q6tWFIYU4yit+MbVxfubQ3ba5jeWP9S8f2o6Zc/+5L0XGkQpo/DfPjSC3OKP5tiI4x+/Ce2PyltLAQFm5ubpTcL70dN32SWS6+9Ufb+e8e/t7C/CJ59uq3ktdM4JmnHug6pR5zXJf3SpRvK3nvnO9+hA9/19kjp32jdXvI6jbnovmUPe+dhzmMzvdInYe6/dl5jNDU1Rc7HmzuWShtKAygtLW+dvvt1/PMrXpaWtZVt41IOf3v6gbL3Dj3kEPXr1xIpfX6HlTaUvjdu3MHafWj5h0jCLH5qVdl7seqx27y298i9NX78WPd9xPTQwlfK3tv7bW/T+PHRr/GeWfFQ8edMXuejxI4dO97yAarUvlbx2GOP1bx587Ru3Tpt27ZNs2fP1nHHHVf8fT6f11e+8hWdcsopuuyyy0KfKgMAAAAAAAAAAABcpPrk2IUXXqizzz5b7e3t+tSnPqVx48bpvPPO0wUXXKDXXntNS5YsUT6f1/33Fz6NNnbsWF111VVpZRkAAAAAAAAAAAANLrXgmCRNmDBBEyZMKHnvhhtukCQdfPDBWrp0aRrZAgAAAAAAAAAAQC+V2tcqAgAAAAAAAAAAALVGcAwNp729fdcbVZFNOL21vnsE4N0xATh3o55/DtYlfRAE4e9bv78xa/NeyVFjYW2mlsO57xKsHqYeK9P9hXueujpytw7tVy5x+nDpAZ3XxnXyp6ltYi3C8YRCNq9pP0pwD/XQp5ITrR5tPnw+dJH09alz+pAduOzTBuFbRx4LKmwXNQ+B9a8DX6HzocNY2H39FGdIDEqOZSrmqZJKea1Ut6HbOhwvPA893nAuiPIcuNRB17nWyZTkUZ6m5D+NJKy+6uF2m9O6JmQ4CvLRL3Cqcb42LFNVFbawqYOKRK9FcAxIhZVhbAcA9ALWc0Izkt8dkQa8eEd9avSmFD//tvPfBl6cNnDWJclYyRo19PVBU+fNs9jt0HQFexu3EIxU/skRl/RGUi4nE6cMbI9geQyBAlmP/BfqzvP8lcAHF2KmN9bKGCNj4rZD23kSHmWYQKTTqw6tJGMSmA/T68dWVsarEQUNvx5o5HE0GVZWPp+Yq69AZ1bZXFBYHAFVRnAMAAAAAAAAABpU1kNivurgIVIAKSA4BgAAAAAAAAAAgMwgOAYAAAAAAAAAAIDMIDgGAAAAAAAAAACAzCA4BgAAAAAAAAAAgMwgOAYAAAAAAAAAAIDMIDgGAAAAAAAAAACAzCA4BgAAAAAAAAAAgMwgOAYAAAAAAAAAAIDMIDiGhmPzQdpZKLJpZwCAJMlmtDNm9LRRNcZp67J+55A8CPxbb9gegqwOBnXErQbSrq+w4zs05LSzXyXWsR9Zs7PM4hVJaZnnPcaH7nlpFNa89etdC0uQbuO0Dv0oNKcpVWOcKaS9vd3rmIGtn2vbEi5DYVjBudRhhXKPWjQVqy1ihbqOeVHz4NQPQse92nWE8OO7pA9/32+3risK2+O149FCEsRtGnWxPIjbfDrTpXsO8TJfF+UewqUd5YN8yHvR54lqXArV/PLKlB/Q9RoviXEd2UFwDAAAAA2s8W6GA3WnDoJKJuM3Mox7VAwA6lLIvW2gxphTAURDcAyIwXexZ8QnGQAgbfk6ehK5sQWeH9U0HhOrleHit65QG2ny6Yjp11xj574+pFoOXdc2sSuy8NxHmufgGyNuaYrz5F+XrrM3scrQdPs3LiNJxsSekq38ytB2PmtlUnr+o1ADppCLmHWQy8lrXWKslZXP+rSwLoqfA1uow7q4VxHzLHyzbqzX6TMfpS9uHXR/kpfri/QFRor9nQR18KEvNA6CYwAAAAAAAAAAAMgMgmMAAAAAAADwxlfqpY8qAAAgGoJjAAAAAAAAAAAAyAyCYwAAAAAAAAAAAMgMgmMAAAAAAAAAAADIDIJjAAAAAAAAAAAAyAyCYwAAAAAAAAAAAMgMgmMAAAAAAAAAAADIDIJjaDhBYNPOwk7GPYntkaaOzgZoWPQjoB44TIo2vNfaCu9HFSjvlR7+vMfjGg7oNqTNuhy+t849nt3Q/Xhlx2/Qkk0p22GHTb8Io2cg8EqdrDjlZvNhZxD9HGx48tDxqVrC8+oyp/ulDyrkIKhUOOU7qJA+WvKKdeBSBWHbOi2LyjNrHXpCeHrf47ukr9APKvSPiHt129rnUBWO5lIHdXWfSlKsm1V1ontJuozLNihvBL61ksxY7NeXK41xYapR67VfF4aszx0z0bBrSaSC4BgAAACAzGvc20i9A+UPWkHjM9Rh+nyrIJEqzPCNaW7KA0BDITgGpMRkecEIAOhVYs9ptuuzgXHTx0uG6mno26IZvaFlrFSoubTP3+/4pqHrr5B37/6TYhlYz3MwNv0WaBIYwEzcndjOsjO5+O0ggfz71YGVNR63mKxNphJinoWRLR4+fi5ykukaV91Zo/iJC3uQZPx2UTe3CePWgt9gYm0jr2UK4enGzX8ybMw2UExiXZ4ZRLUUaiHmeJ7IXIKsqJdZDwAAAAAAAAAAAKg6gmMAAAAAAAAAAADIDIJjAAAAAAAAAAAAyAyCYwAAAAAAAAAAAMgMgmMAAAAAAAAAAADIDIJjAAAAAAAAAAAAyAyCYwAAAAAAAAAAAMgMgmMAAAAAAAAAAADIDIJjAAAAAAAAAAAAyAyCY2g4NgjSPX7JK+O/P7vrbQDsCh0J8GWd+1HpHOiSOggqbO0wKdqQKdjmHTKBqnBrReWV6N4O47Nh7c1paVcHc0+P/FrjtjYNOwNr3dbapttO4pRIWRrHpb5NYD2ehOTW9EmcT+3KJPy0ox/fVJoPUhCnDivOZ1HTqz4nLqc5PaTgXNKHjsUO+6g0ZEStz8BxzAsXNp9F59uOfAegSnUQVaUSdCnb8vnfLU897xO5n1FIO3ZpGt02Dluj1lrcGi3OqY7riWpx60dVyEACxeDUvUM2zudd+lHyaj1Nhxa5Yx58xzRkC8ExAACQSd43IlAX6uPSHciuxEbSjA/JjGUSpQD48+1F9EI/aZbfzqBcxifUmGzJtSE9AcgKgmNALLbkU7IxkrNeAQD0CtZrQkzgg6l18slWFDRybTRy3iV5rU1Nw59942v0SwPj++SKgkIZpPhp7ySmk7hPvxYPnYt3i8YoiTZk5Tsa+JSh6fx/mm0gZxT/JKztPIH4tWGs5/PTxu+pJSObyFNPvuNB515ipvO72WI7/9e4Gjv3ybCyHk/lms4S9LzEgSdrgthjieH6EA4IjgEAAAAAYuH2A4DuDLfmAQBAgyA4BgAAAAAAAAAAgMwgOAYAAAAAAAAAAIDMIDgGAAAAAAAAAACAzCA4BgAAAAAAAAAAgMwgOAYAAAAAAAAAAIDMIDgGAAAAAAAAAACAzCA4BgAAAAAAAAAAgMwgOIaGk8/btLNQlExO6ud8gIZls9mPMnraqBrjtLVP87MVGm/gsc/OHfvuAZ5caiBs20aqwXpobtax30bbp+P2CWchH+TdEnQ7frpVEu/o1anDlEvC4ZTqaRyI06dtUGHmilgGtlJz9yxDF2mPZZXWBDaIlrFK6aOeWBKn718H5Xtw2WdoKzTRG1EQVlYO6SvVlXVY2NmygcN5Nip96ZD/wvHD9hg9D3WwJEBIT2i0esmH9KWK80yIiuOhD5eOnMThQt4LHPNQlXJAr0VwDAAAAACAVCUfJAOQQZ5DCSORH8qvXrjXBAEVIJsIjgFxeM+ZTLoAgN4hMF7PkHX+J94+jCTj+MlgVFcj10Yj573Apy8az/QJ8Lkp1eBLa9Pjv43IdH7aPe45GNmqPEXnlIcEDh//HGzh+F6Z8D0Bq8Dn+NaqUJPxj59MC4iZg878G2Pit2OTk289eD352XkO8Z9ktZ3nXw+DakrjgX83SpeVvJbmvYCxSbWejBdkyqwUuyK5PoQLgmMAAAAAAADwxi1JAADQKAiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI6h4dggn+7xjWd6le7AWr/9AchwP/Icj4C02KBCp/Xsy0FmB4P64VIDYdvWsgpDj++Uvje0t/KJxL0Odu7Dd51cOH5vKNfaSbu0fI+fdv67i9P08vnwRFF3VWneqm25eF/g+iWvVAaeY0HU9JXWJD2v22vOc0J1Kr2QMnBKX6GsA7ncu+m5D7f6D5y2jsalCdbb1NXQY7Pp1vccumF4Hfj14yTKwTo0zrBxK6h03RQq+XGr5m3bhKxNncqAtSTcEBwDAACZxKIZADqF3IioeRbSzgBSx7TcC9CRATQotyAUgN6C4BgQW/yJ03imBwCgXvh9srozdewp0XIjrs40dHVkdGlmiv80bgGYBs67pG5F73ceqZaDy0fj61QSMWLjsZOuvpjaOGokr1tExsp45N4W8+D9KFrspLmcZGPWoVGhDRnjVwpeTyVbyXjUgrGSTE5pP03oe2i/lenOf+NIez5q6HVYQqysrPF88rQOPjSUec252B9m9ZmLkT0ExwAAAAAAAAAAAJAZBMcAAAAAAAAAAACQGQTHAAAAAAAAAAAAkBkExwAAAAAAAAAAAJAZBMcAAAAAAAAAAACQGQTHAAAAAAAAAAAAkBkExwAAAAAAAAAAAJAZBMcAAAAAAAAAAACQGQTH0HCstWlnIVFWvet8gDRktxeZtDMAFFmH5hhUnMuj92Yb0v4r7xe14l8DKdehcWjIddjcXLMU2m9tkERWoueh5xuB21nYkp/TmxfjDj8968BlLJXk1mbrUVi5pXROcerQBvkKv4l2DkG+DgYSU54H61AHYdfnLmdVadsg4ljgez2dxP0F3z3k8+XjbvyRMAbPcb/S+stW6h4RuI7nNqS9dHR0RE8fcgoubaP7pnXQq+Mr9v36mFucxhLH9UM0/uXgNEaFbBrUeF1Wfvz0W7TrON3b7hujugiOAQCATEr5OgNJafQbw0BdqId+lPEbGRk/fQAAEhPnww7dAir1sCoCUBsEx4AYjPwnS8MnGQAAvYKNP6dZdQa34qfPcflaV6iN9MQve9vZDxu79ho79535b+DLg5z3tY2VlZFJsRAS+axFc8xbLNaq0AqaYj3CZqzb015vlYfYe7GSycXPg+l8Vsi7GnxOoJiLOMk7x1KferDWsx3awvl7dCPTmQ8/KfZjWa/j217x6bkGnkwS41MGttiVkC7qALVAcAwAAAAAAAD++BAoAABoEATHAAAAAAAAAAAAkBkExwAAAAAAAAAAAJAZBMcAAAAAAAAAAACQGQTHAAAAAAAAAAAAkBkExwAAAAAAAAAAAJAZBMcAAAAAAAAAAACQGQTHAAAAAAAAAAAAkBkExwAAAAAAAAAAAJAZBMfQcIJ8Pu0sFFkZ9zQ9klibUGaADMtqN7KZPfNkWBuknYW64tqafFqfrTD5+bZoG9An0uZdA8Z9bRWXd3tLJBd+4qxFd8W1GyWdh7xPP65d8ykTO9dlCf1Pohrtwk3044eVW1p9K851WaX5LKqg4lrErwxd+F6PhiZ3GcsrrQmiZqxCEUYdSoJKG3p2I+s9n0WvGN82kPdtAxXqKnBoXOWb+o8EbuvCkG2d8p/+dUX3sb/nfafo++iU9jTSxaEKw2rA+3o5iXJwaEdhc4INorct3zmpXrmeVu8sBVQLwTEAAAAAQLp66Q0dIFvq5Y56dvnWADXoxzCV1QWqAUBUBMeAWLw/WsVkDQDoFWylj2xHZDr34rkD1IlGrg6T1dWZlWRNQ3/a2EgNfSfMyErG+PefNMug89PusW8M2x7/TUESD6761KIxCWUiJqugUPweY4HxKQFb2ENaV8pGUs5IxpjY7dgUKtErH9YrumJljc9tPiubxFiUtrQfp0xbg2ffl/F9ftpaNfaKthdo9D6IhkJwDAAAAAAAAAAAAJlBcAwAAAAAAAAAAACZQXAMAAAAAAAAAAAAmUFwDAAAAAAAAAAAAJlBcAwAAAAAAAAAAACZQXAMAAAAAAAAAAAAmZFqcOzee+/VqaeeqhNPPFFTpkwp+/0zzzyjT37ykzrppJN02WWXqaOjI4VcAgAAAAAAAAAAoLdILTjW2tqqyZMna+rUqZoxY4amTZumFStWlGxz8cUX63vf+57uv/9+WWt1++23p5RbAAAAAAAAAAAA9AbGWmvTOPD06dO1YMEC/ehHP5Ik/fKXv5S1Vl//+tclSatXr9Z//dd/6YEHHpAkLVy4UP/7v/+rm2++eZf73rFjhxYvXqyxY8eqb9++1TuJXuKF5Wt1y2/+qUb6ls2uRmtSOrYJ+blW6QEUdJ+8styP0hwPe4Msl1/PBWCcMvCZ03pu75M+y/WYprCLiDjrIp/0vnyPn0QZ+Ore/n3XlnHT+87JXcdt9LWx31hkZWW82mCcMkxuLrCdP8c7B5/5IG076912+9ctve/5+86JSV7jxtlHEum7xO0HSeY/jqTrwOf4cfbnWwdh+3CV9r2aeprLfPJQL/mX0qtDH0mvLdNYW6d9ryWJ+6a9/fpw0O5bdeEPPpN2NhrCruJEzSnkSZLU1tamYcOGFV8PHz5cTz31VMXfDxs2TK2trTXNY1b85eaf6B2vD5Q1TWlnBQAAAAAAAAAAhLBb1koiOJaE1IJjYQ+sGWMi/z6KxYsXu2csg3Yb9Q5tNc/K2CDtrEQWKKecrMI/r1t9zUGHrIzyuXgBxeag8Pfz8rnmlM4AaHxWRlZGMlKugcav5HV9PhFxpD2fpMsob3JqsoGMDWQd11mSZKxVs83LyqjDcU40kpqCjmILjjMnNnemD2TUnmtRTlkeC9JQaEPGFkZka4zzeJyzgXLW6s1ci5psIFvzj3cWWmAuKOQ7yLl/k4KxVsZatTe1qMnmE85fpBwoZ/PKWevcDyUpFwRqUhC7Hxb3YwMFJt43URhr1WTz6sjFuzxtCgrlHndtnpTA5JSz7nNKU5BXTlZ5k4tVhoWyb1JL0K4O0+Q4nnfNBXnFXVPkbKCmzr4f5xyMlZpt4fooznyStkCF8mvu7P9xyqBrPpOkvHLOY1H3OmjPtXS+G70uu+bzuMfvPqcHMa6Te64J3MeCQunlgrxyCrzWFFK8MuiePk4ZdN+Hez/2r4Pux4/TD7vafHPQoZwNYo/nkpQ3TbHn0657LXGOv3NdGy991/F9zt1XrnM+8ctD+teXpvOesE8/iNOPpNLrizj9qHtfiL0uUvwa2PkEd/w6DJSTNUbG2lSur7rmNJ++WBhHAqXdlqulvd9gLVq0KO1s9AqpjdgjRozQwoULi6/b2to0fPjwkt+vXbu2+HrNmjUlv4+Cr1WMZvz48Vq0aJHGjx+fdlYAoCExhgJAPIyfABAfYygAxMcYCvR+XV+rWElqf2Tq2GOP1bx587Ru3Tpt27ZNs2fP1nHHHVf8/T777KO+ffsWo6B33313ye8BAAAAAAAAAAAAV6kFx0aMGKELL7xQZ599tj72sY/p9NNP17hx43Teeefp6aefliRNmjRJV199tU455RRt27ZNZ599dlrZBQAAAAAAAAAAQC+Q3hfhSpowYYImTJhQ8t4NN9xQ/Pmggw7SHXfcUetsAQAAAAAAAAAAoJdK7ckxAAAAAAAAAAAAoNYIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgMwiOAQAAAAAAAAAAIDMIjgEAAAAAAAAAACAzCI4BAAAAAAAAAAAgM5rTzkA1WGslSW+++WbKOWksO3bsSDsLANCwGEMBIB7GTwCIjzEUAOJjDAV6t674UFe8qCdjK/2mgb3xxhtatmxZ2tkAAAAAAAAAAABASt71rndp0KBBZe/3yuBYEATasmWLWlpaZIxJOzsAAAAAAAAAAACoEWut2tvbtdtuuymXK/8LY70yOAYAAAAAAAAAAACEKQ+XAQAAAAAAAAAAAL0UwTEAAAAAAAAAAABkBsExAAAAAAAAAAAAZAbBMQAAAAAAAAAAAGQGwTEAAAAAAAAAAABkBsExAAAAAAAAAAAAZAbBMQAAAAAAAAAAAGQGwTHo3nvv1amnnqoTTzxRU6ZMSTs7AFAXrrvuOp122mk67bTT9JOf/ESSNHfuXE2YMEEf+chHNHny5OK2zzzzjD75yU/qpJNO0mWXXaaOjg5J0iuvvKKzzjpLJ598siZOnKgtW7akci4AkIZrrrlGl1xyiST3cXLTpk368pe/rFNOOUVnnXWW1qxZk9p5AEAtPfjgg/rEJz6hk08+WVdeeaUk1qAAENWMGTOK1/HXXHONJNahACojOJZxra2tmjx5sqZOnaoZM2Zo2rRpWrFiRdrZAoBUzZ07V4888oimT5+uu+++W//+97/15z//WZdeeql+9atfaebMmVq8eLEeeughSdLFF1+s733ve7r//vtlrdXtt98uSfp//+//6XOf+5xmzZqlsWPH6le/+lWapwUANTNv3jxNnz69+Np1nPz5z3+uww8/XH/5y1/06U9/WldddVUq5wEAtbRq1Sr94Ac/0K9+9Svde++9WrJkiR566CHWoAAQwbZt23TVVVfplltu0YwZM7Rw4ULNnTuXdSiAigiOZdzcuXN19NFHa4899tCAAQN00kknadasWWlnCwBSNWzYMF1yySXq06ePWlpadMABB+jFF1/U29/+du27775qbm7WhAkTNGvWLK1evVrbt2/XoYceKkn6xCc+oVmzZqm9vV0LFizQSSedVPI+APR2GzZs0OTJk/WVr3xFkmKNk3PmzNGECRMkSaeffrr+8Y9/qL29vfYnAwA19Ne//lWnnnqqRo4cqZaWFk2ePFn9+/dnDQoAEeTzeQVBoG3btqmjo0MdHR1qbm5mHQqgIoJjGdfW1qZhw4YVXw8fPlytra0p5ggA0nfggQcWF88vvviiZs6cKWNM6HjZcxwdNmyYWltbtX79eg0cOFDNzc0l7wNAb/f9739fF154oXbffXdJ5evNKONk9zTNzc0aOHCg1q1bV+MzAYDaWrlypfL5vL74xS/qjDPO0NSpUytes7MGBYBSAwcO1De/+U2dcsopOu6447TPPvuopaWFdSiAigiOZZy1tuw9Y0wKOQGA+rN8+XKde+65+s53vqP99tuv7PfGmIrjKOMrgCz605/+pL333lvHHHNM8b2kxslcjksXAL1bPp/XvHnz9NOf/lS33367nn76ab388stl27EGBYByS5cu1Z133qm///3veuSRR5TL5fTPf/6zbDvWoQC6NKedAaRrxIgRWrhwYfF1W1ubhg8fnmKOAKA+LFq0SBdccIEuvfRSnXbaaZo/f77Wrl1b/H3XeDlixIiS99esWaPhw4dr6NCh2rx5s/L5vJqamorvA0BvNnPmTK1Zs0Yf/ehHtXHjRm3dulXGGOdxcvjw4Vq7dq1Gjhypjo4Obd68WXvssUdKZwUAtbHXXnvpmGOO0dChQyVJH/rQhzRr1iw1NTUVt2ENCgDhHnnkER1zzDHac889JRW+KvH3v/8961AAFRH2zrhjjz1W8+bN07p167Rt2zbNnj1bxx13XNrZAoBUvfrqq/ra176mSZMm6bTTTpMkHXLIIXrhhReKX3fz5z//ufhVDX379tWiRYskSXfffbeOO+44tbS06PDDD9fMmTNL3geA3uz//u//9Oc//1kzZszQBRdcoBNOOEFXX3218zh5/PHH6+6775ZUCLgdfvjhamlpSeWcAKBWPvjBD+qRRx7Rpk2blM/n9fDDD+vkk09mDQoAERx00EGaO3eutm7dKmutHnzwQR155JGsQwFUZGzYc6TIlHvvvVe//e1v1d7erk996lM677zz0s4SAKTqyiuv1J133lnyVYpnnnmm9t9/f1199dXasWOHjj/+eH33u9+VMUZLly7V5Zdfri1btujd7363rr76avXp00erV6/WJZdcotdff1177723fvazn2nw4MEpnhkA1M5dd92l+fPn68c//rHzOLlhwwZdcsklWrVqlQYNGqRJkyZp1KhRaZ8SAFTdHXfcoRtvvFHt7e163/vep8svv1yPPvooa1AAiOD666/XXXfdpZaWFh188MH6wQ9+oBdeeIF1KIBQBMcAAAAAAAAAAACQGXytIgAAAAAAAAAAADKD4BgAAAAAAAAAAAAyg+AYAAAAAAAAAAAAMoPgGAAAAAAAAAAAADKD4BgAAAAAAAAAAAAyozntDAAAAABAo7rkkks0ffr0XW738Y9/vLjdFVdcoc9+9rPVzlps1lo999xzGj16dNpZAQAAAICq4MkxAAAAAIAkacmSJfrsZz+r3/3ud2lnBQAAAACqxlhrbdqZAAAAAIBGtHHjRm3btq34euLEiVqyZIkOPfRQ/c///E/x/b59+2rHjh2SpN13310DBgyoeV6jeP/73681a9bo4x//uH784x+nnR0AAAAAqAq+VhEAAAAAYho8eLAGDx5cfN3S0iJJ6tOnj0aOHJlWtmLjs5MAAAAAsoCvVQQAAACAKnv55Zc1ZswYjRkzRrfeeqsk6dFHHy2+t3LlSl1++eU64ogjdOSRR+pHP/qR2tvb9dBDD+mjH/2oxo0bpwkTJmjOnDkl+83n87r++uv1kY98RGPHjtUJJ5ygSZMmlTzNJklPP/20zjvvPB111FEaO3asPvCBD+jiiy/WqlWrituMGTNGa9eulSRNnz5dY8aM0aOPPup0nK7zufvuu/WHP/xBJ5xwgsaNG6cvfOELWrJkSdLFCgAAAACx8OQYAAAAAKRs4sSJeu6554qvb7rpJq1YsUL/+te/lM/nJUnLli3TN77xDc2cOVP77ruvJOm73/2uZsyYUUy3evVq3XDDDXryySd10003KZfL6ZVXXtG5556rTZs2Fbd79dVXdc8992jBggWaMWNGydNvYaIcp7vf/e53Wr58efH1/PnzddZZZ+mOO+7QAQccEKOEAAAAACA5PDkGAAAAACnbtGmTbrzxRk2ZMqUYqPrnP/+pD37wg7rvvvv0ta99TZL05ptv6pFHHpFUePKsK2B1/vnn6y9/+Yt+/vOfq3///po/f37xdw8++KA2bdqkIUOG6I9//KMeeOABXXvttWppadGWLVs0b948SdJDDz2koUOHSpJOPvlkPfTQQzrssMMiH6e75cuX6/zzz9d9992nyy67TLlcTlu3btXPfvazKpYiAAAAAERDcAwAAAAAUnbOOefomGOO0eGHH66jjjqq+P4VV1yh0aNH6/Of/3zxvfXr10uS/vrXv0qShg0bps997nMaMGCADjvsMJ1wwgmSpPvuu0+S1LdvX0nSli1b9Oijj2rz5s069dRT9fDDD2vBggU6+eSTJUkjR44sPgHWv39/jRw5Un369Il8nO4OPfRQXXTRRRo9erTOPvtsfeQjH5EkPfLII+ro6Eio1AAAAAAgHr5WEQAAAABStv/++xd/7t+/vyRp4MCBGjZsmCRpwIABxd93fc3iypUrJUlr1qzR8ccfX7bPZ599VpI0YcIE3XnnnXr88cf1i1/8Qr/4xS80ePBgHXPMMTrzzDN1zDHHvGXeoh6nu7Fjx5a9njVrlrZv367169cXzwsAAAAA0sCTYwAAAACQsj59+hR/7np6q1+/fsX3jDFlaZqamt5ynxs2bCjuZ8qUKfrlL3+pM844Q8OGDdPGjRs1a9YsnXPOObrtttvecj9Rj9Pdm2++WXH7nn+fDAAAAABqjSfHAAAAAKABjRo1SpK0zz776MEHHyy+v3z5cu2xxx7Fp7NWr16tZcuWKQgC/fSnP5Ukvfjii7rwwgu1ZMkS3X777TrzzDMl7QzCWWudj9PdE088IWttcX9LliyRVHgCbsiQIckUAAAAAADExEf2AAAAAKABdf2tsNWrV+uqq67Sc889p4cfflhnnnmm3v/+9+v73/++JOk3v/mNvvKVr+iiiy7SHXfcoZdeekltbW3avHmzJKmlpaW4z64n2FauXKnly5dr3bp1kY/T3bJly3TVVVdp+fLlmjp1qmbPni1JOv7443lyDAAAAEDqeHIMAAAAABrQ4YcfrpNOOkn333+/br75Zt18883F3w0dOlRf+tKXJEkTJ07UnDlz1NbWpssuu6xkH8aY4naSNHr0aK1evVqPP/64Tj/9dE2ePFmnnnpqpON0N2LECN1yyy265ZZbiu8NGjRIF154YWLnDwAAAABx8ZE9AAAAAGhQkyZN0kUXXaR3vvOd6tOnj/baay+deuqpuvXWW7XffvtJkt72trdp2rRp+sxnPqN9991XLS0tGjhwoI466ihdf/31OvHEE4v7++Y3v6mDDz5Yffv21ciRIzVgwIDIx+nujDPO0Pe//33tvffe6tu3r44++mj98Y9/1Nvf/vbaFAwAAAAAvAVju3+ZPAAAAAAAMY0ZM0aSdN555+lb3/pWyrkBAAAAgHA8OQYAAAAAAAAAAIDMIDgGAAAAAAAAAACAzCA4BgAAAAAAAAAAgMzgb44BAAAAAAAAAAAgM3hyDAAAAAAAAAAAAJlBcAwAAAAAAAAAAACZQXAMAAAAAAAAAAAAmUFwDAAAAAAAAAAAAJlBcAwAAAAAAAAAAACZQXAMAAAAAAAAAAAAmfH/AdrREtMBpL5WAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2160x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_max_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns='timestamps')\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "X_max_u_sparse_bool_train, X_max_u_sparse_bool_test, y_max_u_sparse_bool_train, y_max_u_sparse_bool_test = utils.split_and_suffle(exogenous_data, y_max_u_sparse_bool)\n",
    "data = {'X_train': X_max_u_sparse_bool_train,\n",
    "        'X_test': X_max_u_sparse_bool_test,\n",
    "        'y_train': utils.convert_df_to_bool(y_max_u_sparse_bool_train),\n",
    "        'y_test': utils.convert_df_to_bool(y_max_u_sparse_bool_test)}\n",
    "# Plot prediction_gb_max_u\n",
    "sns.set(style='whitegrid')\n",
    "fig, axs = plt.subplots(1, 1, figsize=(30, 10))\n",
    "axs.plot(y_max_u_sparse_bool_test.reset_index(drop=True))\n",
    "axs.plot(threshold_signal)\n",
    "axs.set_title('Dataset Sample of Maximum Voltage Constraint Occurrences', fontsize=30, fontweight='bold')\n",
    "axs.set_xlabel('Timestep', fontsize=18, fontweight='bold')\n",
    "axs.set_ylabel('Constraint Value', fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, not all busses have constraints, so it is only possible train classification models for those busses that have positive values in the target dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bus_8',\n",
       " 'bus_9',\n",
       " 'bus_10',\n",
       " 'bus_11',\n",
       " 'bus_12',\n",
       " 'bus_13',\n",
       " 'bus_14',\n",
       " 'bus_15',\n",
       " 'bus_16',\n",
       " 'bus_18']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.cols_with_positive_values(y_max_u_sparse_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from thesis_package import aimodels as my_ai, utils, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Tunning of Models\n",
    "The objective of this section is to tune the hyperparameters of the models to obtain the best performance on the sparse dataset. In order to perfom the hyperparameters tunning, we are going to use the optuna library presented in the `optuna_introduction.ipynb` notebook. The models are the ones implemented in the `aimodel.py` file in the `thesis_package`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters\n",
    "num_trials = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06450243730741544\n",
      "0.054008631153497785\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "train_x, valid_x, train_y, valid_y, scaler_max = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "data_max_reg = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "train_x, valid_x, train_y, valid_y, scaler_min = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "data_min_reg = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "# Threshold\n",
    "threshold_max = utils.compute_threshold(y_max_u) / scaler_max['y']\n",
    "threshold_min = utils.compute_threshold(y_min_u) / scaler_min['y']\n",
    "print(threshold_min)\n",
    "print(threshold_max)\n",
    "# Classification\n",
    "y_max_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse_bool[utils.cols_with_positive_values(y_max_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "data_max_class = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "\n",
    "y_min_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse_bool[utils.cols_with_positive_values(y_min_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "data_min_class = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06450243730741544\n",
      "0.054008631153497785\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "train_x, valid_x, train_y, valid_y, scaler_max = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "data_max_reg = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "train_x, valid_x, train_y, valid_y, scaler_min = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "data_min_reg = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "# Threshold\n",
    "threshold_max = utils.compute_threshold(y_max_u) / scaler_max['y']\n",
    "threshold_min = utils.compute_threshold(y_min_u) / scaler_min['y']\n",
    "print(threshold_min)\n",
    "print(threshold_max)\n",
    "# Classification\n",
    "y_max_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse_bool[utils.cols_with_positive_values(y_max_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "data_max_class = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "\n",
    "y_min_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse_bool[utils.cols_with_positive_values(y_min_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "data_min_class = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-11 21:22:47,663]\u001b[0m A new study created in memory with name: no-name-2a52eabb-c0ab-4ec4-8434-4bdf0d05f1bb\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 21:24:24,186]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 0.13670113255180008, 'alpha': 0.5453177075081312, 'subsample': 0.2774171048028863, 'colsample_bytree': 0.49798535922125303}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 21:28:35,330]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 6.434183235199802e-07, 'alpha': 2.0240015920198777e-07, 'subsample': 0.8478319879248852, 'colsample_bytree': 0.4545206369114196, 'max_depth': 3, 'min_child_weight': 4, 'eta': 2.9351338886594904e-05, 'gamma': 3.368884300956653e-08, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 21:31:25,599]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 0.0054469246383618535, 'alpha': 0.7277519454819869, 'subsample': 0.26460625093086637, 'colsample_bytree': 0.43691097034373244}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 21:43:38,966]\u001b[0m Trial 3 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.0065214178334947175, 'alpha': 4.820633863592838e-06, 'subsample': 0.5951052799905222, 'colsample_bytree': 0.3167547514940576, 'max_depth': 7, 'min_child_weight': 5, 'eta': 4.209393344016283e-08, 'gamma': 1.0605437246252266e-06, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.027668379225360383, 'skip_drop': 0.0005334661471667172}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 21:46:02,665]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.0008963210406641808, 'alpha': 0.28254870937979487, 'subsample': 0.9784391047730223, 'colsample_bytree': 0.7468429453589132, 'max_depth': 7, 'min_child_weight': 3, 'eta': 0.012657211823256253, 'gamma': 5.4761335797847265e-05, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 21:48:06,502]\u001b[0m Trial 5 finished with value: 0.3213953046680232 and parameters: {'booster': 'gbtree', 'lambda': 4.132685448758951e-07, 'alpha': 0.004617250154276343, 'subsample': 0.5483064687856387, 'colsample_bytree': 0.7011581880701645, 'max_depth': 3, 'min_child_weight': 8, 'eta': 0.4399323952771567, 'gamma': 0.0035216012315344263, 'grow_policy': 'depthwise'}. Best is trial 5 with value: 0.3213953046680232.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 21:57:57,238]\u001b[0m Trial 6 finished with value: 0.48885217429521804 and parameters: {'booster': 'dart', 'lambda': 1.660626170904603e-05, 'alpha': 0.17308249083479166, 'subsample': 0.4122865849645826, 'colsample_bytree': 0.7497490544496421, 'max_depth': 9, 'min_child_weight': 5, 'eta': 0.19322318334438532, 'gamma': 2.0254178495480695e-08, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.7726710493153832e-05, 'skip_drop': 1.2261892812507639e-06}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 21:59:08,981]\u001b[0m Trial 7 finished with value: 0.39268370339639164 and parameters: {'booster': 'gblinear', 'lambda': 0.09033244422757719, 'alpha': 1.8406975497768286e-06, 'subsample': 0.357914109682332, 'colsample_bytree': 0.6920635708904019}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:08:32,357]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 2.0078180430778415e-05, 'alpha': 7.437833312860745e-05, 'subsample': 0.9488241415613614, 'colsample_bytree': 0.5721857637722414, 'max_depth': 5, 'min_child_weight': 7, 'eta': 0.0001977356261753588, 'gamma': 6.20384182269366e-05, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.8803272661967356e-08, 'skip_drop': 1.3994410959040856e-08}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:17:48,151]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 2.71043430659519e-06, 'alpha': 0.019185207208433412, 'subsample': 0.7735757667225798, 'colsample_bytree': 0.5575823025533674, 'max_depth': 5, 'min_child_weight': 3, 'eta': 2.2000770322064462e-07, 'gamma': 0.02044867139500148, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.9779767766744487e-06, 'skip_drop': 0.0014575496991785207}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:28:02,958]\u001b[0m Trial 10 finished with value: 0.28086320634263706 and parameters: {'booster': 'dart', 'lambda': 2.6430887058379268e-08, 'alpha': 0.0013335065750026567, 'subsample': 0.4615400980987262, 'colsample_bytree': 0.9855096289946896, 'max_depth': 9, 'min_child_weight': 10, 'eta': 0.8884032937043137, 'gamma': 1.1598417125266097e-08, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.00032974863339250505, 'skip_drop': 1.5037618160265874e-07}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:29:09,068]\u001b[0m Trial 11 finished with value: 0.3876970055007342 and parameters: {'booster': 'gblinear', 'lambda': 0.00011408364947583489, 'alpha': 1.3305399099663407e-08, 'subsample': 0.4289586558407614, 'colsample_bytree': 0.8605665466193739}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:30:17,671]\u001b[0m Trial 12 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 0.6189801833390416, 'alpha': 6.504926065903419e-06, 'subsample': 0.38341671835747015, 'colsample_bytree': 0.7142905273137685}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:31:28,310]\u001b[0m Trial 13 finished with value: 0.38624450678900907 and parameters: {'booster': 'gblinear', 'lambda': 0.000147922130306591, 'alpha': 6.641953316931558e-05, 'subsample': 0.338343795462706, 'colsample_bytree': 0.8517985320922229}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:40:46,905]\u001b[0m Trial 14 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.03590591553890686, 'alpha': 1.0919647671259882e-06, 'subsample': 0.5211244747374819, 'colsample_bytree': 0.8393303367481928, 'max_depth': 9, 'min_child_weight': 6, 'eta': 0.0023237771647925355, 'gamma': 0.9028925959940711, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 7.826331309482897e-05, 'skip_drop': 0.42296558102943604}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:41:51,806]\u001b[0m Trial 15 finished with value: 0.38497284854933533 and parameters: {'booster': 'gblinear', 'lambda': 1.5074883692676657e-05, 'alpha': 5.750759932132836e-08, 'subsample': 0.6959086451914128, 'colsample_bytree': 0.24450991507746123}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:49:26,528]\u001b[0m Trial 16 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.0011145764707574054, 'alpha': 0.00043205212879302783, 'subsample': 0.20337615655689073, 'colsample_bytree': 0.6560109816064309, 'max_depth': 9, 'min_child_weight': 2, 'eta': 8.194006458838086e-06, 'gamma': 1.6318783430609174e-06, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.22519778606595783, 'skip_drop': 1.4160603185642445e-05}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:50:33,124]\u001b[0m Trial 17 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 3.639941400107543e-08, 'alpha': 0.06377231570205501, 'subsample': 0.6720940137738154, 'colsample_bytree': 0.7571087528347697}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 22:58:43,805]\u001b[0m Trial 18 finished with value: 0.018154392724488367 and parameters: {'booster': 'dart', 'lambda': 0.8978552117327623, 'alpha': 1.1964513949608643e-05, 'subsample': 0.33850706593772156, 'colsample_bytree': 0.9925441584989154, 'max_depth': 7, 'min_child_weight': 9, 'eta': 0.021960093483139414, 'gamma': 1.9729413052040833e-06, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 9.74218949302994e-07, 'skip_drop': 1.1327074503989528e-06}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:01:01,541]\u001b[0m Trial 19 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 7.765463341152158e-06, 'alpha': 6.563251586670011e-07, 'subsample': 0.48166778741962063, 'colsample_bytree': 0.608956277162892, 'max_depth': 9, 'min_child_weight': 6, 'eta': 1.8951695862139244e-06, 'gamma': 1.9209528826252623e-07, 'grow_policy': 'lossguide'}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:07:55,429]\u001b[0m Trial 20 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.0005157891621110491, 'alpha': 0.00043445103806846405, 'subsample': 0.40414966072451297, 'colsample_bytree': 0.893192427291132, 'max_depth': 5, 'min_child_weight': 5, 'eta': 0.00017540231367348908, 'gamma': 0.0011025513663749703, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.0003722893300446173, 'skip_drop': 1.166175005706811e-05}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:08:54,742]\u001b[0m Trial 21 finished with value: 0.3850774410277333 and parameters: {'booster': 'gblinear', 'lambda': 0.0001046972847749544, 'alpha': 1.3680277903017075e-08, 'subsample': 0.4542212857112732, 'colsample_bytree': 0.7967896648286306}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:09:53,239]\u001b[0m Trial 22 finished with value: 0.3890911760804451 and parameters: {'booster': 'gblinear', 'lambda': 0.014318724755918569, 'alpha': 1.6917082840027765e-08, 'subsample': 0.3630424566509429, 'colsample_bytree': 0.9124579488536161}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:10:57,004]\u001b[0m Trial 23 finished with value: 0.3859048363835912 and parameters: {'booster': 'gblinear', 'lambda': 0.028550374469260405, 'alpha': 9.77907013033335e-08, 'subsample': 0.32074947045433105, 'colsample_bytree': 0.8922374232285003}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:11:59,231]\u001b[0m Trial 24 finished with value: 0.390916983129521 and parameters: {'booster': 'gblinear', 'lambda': 0.11330673689530595, 'alpha': 2.0244921263859542e-06, 'subsample': 0.26664427855136424, 'colsample_bytree': 0.6527815600811685}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:13:07,047]\u001b[0m Trial 25 finished with value: 0.3887634874463471 and parameters: {'booster': 'gblinear', 'lambda': 0.006111735292927492, 'alpha': 1.2773438632869165e-06, 'subsample': 0.20284682407500026, 'colsample_bytree': 0.6632136016958016}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:14:07,172]\u001b[0m Trial 26 finished with value: 0.3840746637129327 and parameters: {'booster': 'gblinear', 'lambda': 0.18497331854862498, 'alpha': 2.903570559119404e-05, 'subsample': 0.266811953174287, 'colsample_bytree': 0.635115140286658}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:21:21,438]\u001b[0m Trial 27 finished with value: 0.4116768486245213 and parameters: {'booster': 'dart', 'lambda': 0.15564884628797115, 'alpha': 2.8541232803689266e-06, 'subsample': 0.29244271416408935, 'colsample_bytree': 0.5307919982558987, 'max_depth': 7, 'min_child_weight': 7, 'eta': 0.0914890368997676, 'gamma': 1.135597447077061e-05, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 3.624635913648933e-06, 'skip_drop': 0.06405608850479977}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:28:57,162]\u001b[0m Trial 28 finished with value: 0.48493406836901726 and parameters: {'booster': 'dart', 'lambda': 0.002634794835724349, 'alpha': 3.131441147810552e-07, 'subsample': 0.5570706563798259, 'colsample_bytree': 0.5164162812323714, 'max_depth': 7, 'min_child_weight': 8, 'eta': 0.06104063970964773, 'gamma': 2.8503353305603823e-05, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.9665913826086575e-06, 'skip_drop': 0.08708894259704222}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:36:29,916]\u001b[0m Trial 29 finished with value: 0.4623935011990181 and parameters: {'booster': 'dart', 'lambda': 0.0011656062429379463, 'alpha': 2.370445321333357e-07, 'subsample': 0.5989698576633715, 'colsample_bytree': 0.5207991648739589, 'max_depth': 7, 'min_child_weight': 8, 'eta': 0.10788385035119248, 'gamma': 1.7499281759722595e-05, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 3.945234204369428e-06, 'skip_drop': 0.22618540767353956}. Best is trial 6 with value: 0.48885217429521804.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "  Value: 0.48885217429521804\n",
      "  Params: \n",
      "    booster: dart\n",
      "    lambda: 1.660626170904603e-05\n",
      "    alpha: 0.17308249083479166\n",
      "    subsample: 0.4122865849645826\n",
      "    colsample_bytree: 0.7497490544496421\n",
      "    max_depth: 9\n",
      "    min_child_weight: 5\n",
      "    eta: 0.19322318334438532\n",
      "    gamma: 2.0254178495480695e-08\n",
      "    grow_policy: lossguide\n",
      "    sample_type: weighted\n",
      "    normalize_type: tree\n",
      "    rate_drop: 1.7726710493153832e-05\n",
      "    skip_drop: 1.2261892812507639e-06\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_max)\n",
    "    return metric.hybrid_mcc\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_xgboost_regression_sparse_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-11 23:36:30,194]\u001b[0m A new study created in memory with name: no-name-a543b3eb-7746-419b-a3f7-f43b0d3fd766\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:37:35,565]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 0.6759834895874343, 'alpha': 2.8167088278698263e-08, 'subsample': 0.6769441807817325, 'colsample_bytree': 0.7025913460945041}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:39:06,001]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 6.013164496331703e-05, 'alpha': 0.007902571123566808, 'subsample': 0.8053802307888185, 'colsample_bytree': 0.2562132164684144, 'max_depth': 3, 'min_child_weight': 5, 'eta': 1.4633632762942516e-06, 'gamma': 2.250856113282099e-08, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:40:38,397]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 5.733747992335217e-05, 'alpha': 0.0001415348931723752, 'subsample': 0.3749952766641846, 'colsample_bytree': 0.46302799031700065, 'max_depth': 3, 'min_child_weight': 4, 'eta': 1.5580038593562038e-05, 'gamma': 0.013637847012641724, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:48:30,684]\u001b[0m Trial 3 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 3.230758024756456e-07, 'alpha': 9.926471186708532e-05, 'subsample': 0.4156434185717761, 'colsample_bytree': 0.7338891179742542, 'max_depth': 9, 'min_child_weight': 2, 'eta': 0.003204310467314988, 'gamma': 0.030828072166801883, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.04565865643966802, 'skip_drop': 0.02963150338932663}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:50:09,089]\u001b[0m Trial 4 finished with value: 0.5715224657616713 and parameters: {'booster': 'gbtree', 'lambda': 2.6580291627497087e-05, 'alpha': 0.00035117433161679327, 'subsample': 0.6864536953686452, 'colsample_bytree': 0.5974147772320402, 'max_depth': 3, 'min_child_weight': 10, 'eta': 0.17352920427247187, 'gamma': 0.003769732785934237, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:51:12,685]\u001b[0m Trial 5 finished with value: 0.331676700795639 and parameters: {'booster': 'gblinear', 'lambda': 0.002484820508095643, 'alpha': 0.00017141908157568194, 'subsample': 0.7642155957068006, 'colsample_bytree': 0.3519064892744792}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:58:07,589]\u001b[0m Trial 6 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.5680104910207862, 'alpha': 0.07402440116325618, 'subsample': 0.47176289142642125, 'colsample_bytree': 0.9563623722795573, 'max_depth': 5, 'min_child_weight': 9, 'eta': 2.5192476227518867e-06, 'gamma': 0.13134195683372485, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.012876481565418768, 'skip_drop': 1.8947373161189485e-06}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-11 23:59:12,388]\u001b[0m Trial 7 finished with value: 0.33883778048983415 and parameters: {'booster': 'gblinear', 'lambda': 2.703743386090791e-07, 'alpha': 3.962331378516581e-06, 'subsample': 0.9365943940819219, 'colsample_bytree': 0.4665704581959486}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:06:55,786]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.07563022242733851, 'alpha': 3.076317786862332e-07, 'subsample': 0.9122082670924583, 'colsample_bytree': 0.9776286737057982, 'max_depth': 7, 'min_child_weight': 6, 'eta': 1.5252836479802872e-06, 'gamma': 7.759878059736149e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 2.4360994052920395e-08, 'skip_drop': 1.9839583587745726e-06}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:07:57,847]\u001b[0m Trial 9 finished with value: 0.3335956078890926 and parameters: {'booster': 'gblinear', 'lambda': 0.0027571230773359625, 'alpha': 3.724136865653398e-06, 'subsample': 0.8727014574466614, 'colsample_bytree': 0.8221773752490187}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:09:33,233]\u001b[0m Trial 10 finished with value: 0.5277616734075888 and parameters: {'booster': 'gbtree', 'lambda': 1.0882404536068757e-08, 'alpha': 0.3181961735923376, 'subsample': 0.23372049095634306, 'colsample_bytree': 0.5577091150245115, 'max_depth': 5, 'min_child_weight': 10, 'eta': 0.7578615296988962, 'gamma': 4.844893936096603e-05, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:11:07,237]\u001b[0m Trial 11 finished with value: 0.5097377785058678 and parameters: {'booster': 'gbtree', 'lambda': 1.1805679241622094e-08, 'alpha': 0.43883241109657245, 'subsample': 0.23167261039452486, 'colsample_bytree': 0.57744600947928, 'max_depth': 5, 'min_child_weight': 10, 'eta': 0.9567213827122031, 'gamma': 1.8095050393636206e-05, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:12:45,075]\u001b[0m Trial 12 finished with value: 0.5337467988463663 and parameters: {'booster': 'gbtree', 'lambda': 1.9115752482275063e-06, 'alpha': 0.005996935283741996, 'subsample': 0.588097030350234, 'colsample_bytree': 0.5810059879465723, 'max_depth': 3, 'min_child_weight': 8, 'eta': 0.7000023327805248, 'gamma': 0.0005504955930933008, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:14:22,219]\u001b[0m Trial 13 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 2.6371278769833545e-06, 'alpha': 0.003857797727954155, 'subsample': 0.5875491217279519, 'colsample_bytree': 0.6555292503449379, 'max_depth': 3, 'min_child_weight': 8, 'eta': 0.003798159938128104, 'gamma': 0.002199035401157991, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:16:01,795]\u001b[0m Trial 14 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 9.730861007123337e-06, 'alpha': 0.0027579477809153443, 'subsample': 0.5547742556551523, 'colsample_bytree': 0.8169695718026583, 'max_depth': 3, 'min_child_weight': 8, 'eta': 0.008574836998608606, 'gamma': 0.0016710445468009335, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:17:56,624]\u001b[0m Trial 15 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.002071691805698939, 'alpha': 0.0005800493804212052, 'subsample': 0.6634429948158042, 'colsample_bytree': 0.4864636536782006, 'max_depth': 7, 'min_child_weight': 8, 'eta': 3.953916221490128e-08, 'gamma': 8.337962065595749e-07, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:19:27,403]\u001b[0m Trial 16 finished with value: 0.5562640768650915 and parameters: {'booster': 'gbtree', 'lambda': 7.170812761958461e-07, 'alpha': 1.0787394680721389e-05, 'subsample': 0.7149165123431943, 'colsample_bytree': 0.360199262075956, 'max_depth': 3, 'min_child_weight': 7, 'eta': 0.05283456608615715, 'gamma': 0.44464151962945947, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:21:04,177]\u001b[0m Trial 17 finished with value: 0.4981309576377806 and parameters: {'booster': 'gbtree', 'lambda': 1.721352623018351e-07, 'alpha': 8.597792366602274e-06, 'subsample': 0.728909517718186, 'colsample_bytree': 0.22004802270992374, 'max_depth': 5, 'min_child_weight': 6, 'eta': 0.03944562251273107, 'gamma': 0.3804545059464271, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:23:03,771]\u001b[0m Trial 18 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.0002581311166541574, 'alpha': 1.2820820088295242e-05, 'subsample': 0.986117551414154, 'colsample_bytree': 0.35065284494680704, 'max_depth': 9, 'min_child_weight': 3, 'eta': 0.00031486871265259497, 'gamma': 0.914212291049449, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:29:27,555]\u001b[0m Trial 19 finished with value: 0.5382626362094696 and parameters: {'booster': 'dart', 'lambda': 1.1476183291511317e-05, 'alpha': 3.373280107656107e-07, 'subsample': 0.8039077170803965, 'colsample_bytree': 0.32696670311342624, 'max_depth': 3, 'min_child_weight': 7, 'eta': 0.05571668828899287, 'gamma': 0.03501172857006806, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 4.112285483436458e-07, 'skip_drop': 0.7288816589149173}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:31:22,977]\u001b[0m Trial 20 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.00023207097462886125, 'alpha': 4.819854526097769e-07, 'subsample': 0.512856597371998, 'colsample_bytree': 0.43273187336479, 'max_depth': 7, 'min_child_weight': 10, 'eta': 0.0002131418426637374, 'gamma': 0.0059150010452513595, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:37:57,458]\u001b[0m Trial 21 finished with value: 0.5571754464609675 and parameters: {'booster': 'dart', 'lambda': 9.148212175577508e-06, 'alpha': 1.1879367968262519e-08, 'subsample': 0.8418125455689854, 'colsample_bytree': 0.33188343747953764, 'max_depth': 3, 'min_child_weight': 7, 'eta': 0.0517333500264941, 'gamma': 0.0722223653382884, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.4894403948252687e-07, 'skip_drop': 0.6451618584839981}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:44:27,683]\u001b[0m Trial 22 finished with value: 0.5576070868853072 and parameters: {'booster': 'dart', 'lambda': 2.3199901670089763e-06, 'alpha': 4.8352671728767104e-08, 'subsample': 0.6586152091451773, 'colsample_bytree': 0.2769360641953728, 'max_depth': 3, 'min_child_weight': 7, 'eta': 0.06641038029751659, 'gamma': 0.07952797892099901, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 8.488325310912301e-06, 'skip_drop': 0.0016765850162170314}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:51:15,164]\u001b[0m Trial 23 finished with value: 0.5405907138270665 and parameters: {'booster': 'dart', 'lambda': 7.996172589437028e-06, 'alpha': 1.3690031658045198e-08, 'subsample': 0.8423525949011075, 'colsample_bytree': 0.2655067591743974, 'max_depth': 5, 'min_child_weight': 6, 'eta': 0.10508810846604037, 'gamma': 0.05605998310843704, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 9.456951810755121e-06, 'skip_drop': 0.0022766526768802746}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 00:57:53,894]\u001b[0m Trial 24 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 6.489479209080106e-08, 'alpha': 4.867666871920444e-08, 'subsample': 0.6365816219687933, 'colsample_bytree': 0.41019747796675776, 'max_depth': 3, 'min_child_weight': 9, 'eta': 0.0020509301274940083, 'gamma': 0.0010838826270006612, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 3.6905415069934525e-05, 'skip_drop': 0.00046238612187314977}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:04:48,497]\u001b[0m Trial 25 finished with value: 0.550655129239471 and parameters: {'booster': 'dart', 'lambda': 2.042117242707046e-05, 'alpha': 1.0517480815309564e-07, 'subsample': 0.7245210613942303, 'colsample_bytree': 0.29645033977313234, 'max_depth': 5, 'min_child_weight': 5, 'eta': 0.1677998993499808, 'gamma': 0.007562137395781867, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 5.979265051293457e-07, 'skip_drop': 0.5310592506960105}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:11:30,990]\u001b[0m Trial 26 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 1.2963563796282022e-06, 'alpha': 1.0381346972784896e-08, 'subsample': 0.7928071953685133, 'colsample_bytree': 0.20848044937737958, 'max_depth': 3, 'min_child_weight': 7, 'eta': 0.013349759131523318, 'gamma': 0.0002896285269162836, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.0008382228512000788, 'skip_drop': 5.17586634941488e-05}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:18:28,977]\u001b[0m Trial 27 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.000236226081589842, 'alpha': 1.5365215249524769e-06, 'subsample': 0.6392807461904128, 'colsample_bytree': 0.8895440574685487, 'max_depth': 3, 'min_child_weight': 9, 'eta': 0.0011350407049614795, 'gamma': 0.14389771918356295, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.1143818268135906e-06, 'skip_drop': 1.2666220717847667e-08}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:25:30,579]\u001b[0m Trial 28 finished with value: 0.5603088927660734 and parameters: {'booster': 'dart', 'lambda': 0.000798314029847777, 'alpha': 0.02911202996575005, 'subsample': 0.8687023838921972, 'colsample_bytree': 0.5147063573869282, 'max_depth': 5, 'min_child_weight': 5, 'eta': 0.2328039197082319, 'gamma': 5.809799487290681e-06, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.646121430569021e-08, 'skip_drop': 0.019876217638377717}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:26:31,603]\u001b[0m Trial 29 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 0.011646362576388713, 'alpha': 0.044126445083535366, 'subsample': 0.6778782988981994, 'colsample_bytree': 0.6652587588755287}. Best is trial 4 with value: 0.5715224657616713.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "  Value: 0.5715224657616713\n",
      "  Params: \n",
      "    booster: gbtree\n",
      "    lambda: 2.6580291627497087e-05\n",
      "    alpha: 0.00035117433161679327\n",
      "    subsample: 0.6864536953686452\n",
      "    colsample_bytree: 0.5974147772320402\n",
      "    max_depth: 3\n",
      "    min_child_weight: 10\n",
      "    eta: 0.17352920427247187\n",
      "    gamma: 0.003769732785934237\n",
      "    grow_policy: lossguide\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_min)\n",
    "    return metric.hybrid_mcc\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_xgboost_regression_sparse_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-12 01:26:31,869]\u001b[0m A new study created in memory with name: no-name-7dc14b7a-2cdb-480c-885c-71b16de62098\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:29:42,490]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.00017964929792141853, 'alpha': 0.0031648351078432616, 'subsample': 0.9922238477483589, 'colsample_bytree': 0.9982216755867606, 'max_depth': 5, 'min_child_weight': 10, 'eta': 8.045469349544027e-06, 'gamma': 1.1369260088691975e-08, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.07281184535791267, 'skip_drop': 0.01201102293824107}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:30:06,154]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 1.747370093688566e-08, 'alpha': 0.24763129602696315, 'subsample': 0.8047080395237836, 'colsample_bytree': 0.22418456830145833}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:31:15,142]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.00010872223174576919, 'alpha': 7.617338130740583e-07, 'subsample': 0.8798796041352137, 'colsample_bytree': 0.4764784789825539, 'max_depth': 7, 'min_child_weight': 5, 'eta': 3.5315705568542105e-07, 'gamma': 1.973358718430905e-05, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:32:40,091]\u001b[0m Trial 3 finished with value: 0.3269086946483022 and parameters: {'booster': 'gbtree', 'lambda': 0.0011326621486079224, 'alpha': 0.013574188987797327, 'subsample': 0.849548250885275, 'colsample_bytree': 0.4710651557300974, 'max_depth': 9, 'min_child_weight': 10, 'eta': 0.31656832088223485, 'gamma': 0.00043069660112402214, 'grow_policy': 'lossguide'}. Best is trial 3 with value: 0.3269086946483022.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:33:04,860]\u001b[0m Trial 4 finished with value: 0.3746614788593371 and parameters: {'booster': 'gblinear', 'lambda': 0.10118392557898274, 'alpha': 6.170043703595508e-08, 'subsample': 0.42762603823567646, 'colsample_bytree': 0.4056602636716276}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:36:15,916]\u001b[0m Trial 5 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 5.985663405416221e-08, 'alpha': 0.016144393700330602, 'subsample': 0.490063052570776, 'colsample_bytree': 0.48694862385958565, 'max_depth': 9, 'min_child_weight': 5, 'eta': 1.3597174061511685e-05, 'gamma': 0.1734913193649085, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.06238841959977436, 'skip_drop': 0.02559965867084457}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:36:40,817]\u001b[0m Trial 6 finished with value: 0.37277464913287345 and parameters: {'booster': 'gblinear', 'lambda': 0.0008406549468294132, 'alpha': 2.374158484286386e-05, 'subsample': 0.34978045317078754, 'colsample_bytree': 0.6220458553051077}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:37:31,787]\u001b[0m Trial 7 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.004036362649642839, 'alpha': 0.2030404450596102, 'subsample': 0.880638586652762, 'colsample_bytree': 0.38098073049797077, 'max_depth': 7, 'min_child_weight': 4, 'eta': 0.00016840870456083084, 'gamma': 2.0103398417027245e-05, 'grow_policy': 'depthwise'}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:38:52,702]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.086600627418054, 'alpha': 0.20255422501262768, 'subsample': 0.9850816432405427, 'colsample_bytree': 0.9537766330071247, 'max_depth': 9, 'min_child_weight': 10, 'eta': 1.9883712323383813e-06, 'gamma': 1.0738963212074095e-06, 'grow_policy': 'depthwise'}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:42:01,887]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.7820152848590634, 'alpha': 0.05408605190222965, 'subsample': 0.6186455392024772, 'colsample_bytree': 0.9352275736386295, 'max_depth': 9, 'min_child_weight': 2, 'eta': 2.96551551892213e-06, 'gamma': 0.18825369577498058, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.05559864435082536, 'skip_drop': 1.2793708704280678e-08}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:42:26,861]\u001b[0m Trial 10 finished with value: 0.36770828481992546 and parameters: {'booster': 'gblinear', 'lambda': 2.4734989077681484e-06, 'alpha': 2.1577881609174828e-08, 'subsample': 0.22499621639686473, 'colsample_bytree': 0.7217481047389116}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:42:51,151]\u001b[0m Trial 11 finished with value: 0.3708852086967793 and parameters: {'booster': 'gblinear', 'lambda': 0.02070305355678319, 'alpha': 1.580370093978568e-05, 'subsample': 0.3305129686726548, 'colsample_bytree': 0.6970318850439714}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:43:16,255]\u001b[0m Trial 12 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 0.9262844887795313, 'alpha': 1.1425903826572218e-08, 'subsample': 0.43146048477980725, 'colsample_bytree': 0.6370906369490391}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:43:40,718]\u001b[0m Trial 13 finished with value: 0.3720769675224893 and parameters: {'booster': 'gblinear', 'lambda': 7.726984092621495e-06, 'alpha': 0.00012126644462690245, 'subsample': 0.6002516094138923, 'colsample_bytree': 0.2751048890392622}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:44:05,306]\u001b[0m Trial 14 finished with value: 0.36874780842656973 and parameters: {'booster': 'gblinear', 'lambda': 0.031192540849183985, 'alpha': 8.643429786270097e-07, 'subsample': 0.4303036024832282, 'colsample_bytree': 0.8047282450218618}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:44:30,244]\u001b[0m Trial 15 finished with value: 0.3723551794689135 and parameters: {'booster': 'gblinear', 'lambda': 0.0016016252074923612, 'alpha': 0.0002297856621691767, 'subsample': 0.2152260800427635, 'colsample_bytree': 0.3521092476603329}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:44:54,401]\u001b[0m Trial 16 finished with value: 0.36824126352085995 and parameters: {'booster': 'gblinear', 'lambda': 3.700695473283093e-06, 'alpha': 4.631105005533341e-07, 'subsample': 0.3286730064386545, 'colsample_bytree': 0.56374703691264}. Best is trial 4 with value: 0.3746614788593371.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:45:18,677]\u001b[0m Trial 17 finished with value: 0.3750660137624894 and parameters: {'booster': 'gblinear', 'lambda': 0.08710594732711865, 'alpha': 5.237133976012023e-06, 'subsample': 0.6916927250206837, 'colsample_bytree': 0.7897769982266968}. Best is trial 17 with value: 0.3750660137624894.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:45:44,054]\u001b[0m Trial 18 finished with value: 0.374809756266065 and parameters: {'booster': 'gblinear', 'lambda': 0.08434734254900055, 'alpha': 6.630867848712093e-08, 'subsample': 0.7305744823601337, 'colsample_bytree': 0.8463096966026685}. Best is trial 17 with value: 0.3750660137624894.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:48:35,295]\u001b[0m Trial 19 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.010747537216877637, 'alpha': 8.128997814692087e-06, 'subsample': 0.7138094554907847, 'colsample_bytree': 0.8756230286780043, 'max_depth': 3, 'min_child_weight': 7, 'eta': 1.1537657908960373e-08, 'gamma': 0.002872890831747984, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.6874203298160956e-08, 'skip_drop': 2.4067304762426814e-07}. Best is trial 17 with value: 0.3750660137624894.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:48:59,917]\u001b[0m Trial 20 finished with value: 0.3731118897206846 and parameters: {'booster': 'gblinear', 'lambda': 0.18540910300354602, 'alpha': 1.5037831047804968e-07, 'subsample': 0.7094834613780092, 'colsample_bytree': 0.8127346573959734}. Best is trial 17 with value: 0.3750660137624894.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:49:24,124]\u001b[0m Trial 21 finished with value: 0.37781070621818075 and parameters: {'booster': 'gblinear', 'lambda': 0.11866430098752791, 'alpha': 7.678107916591738e-08, 'subsample': 0.68972185719216, 'colsample_bytree': 0.7875582283226814}. Best is trial 21 with value: 0.37781070621818075.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:49:48,323]\u001b[0m Trial 22 finished with value: 0.30021708996853064 and parameters: {'booster': 'gblinear', 'lambda': 0.35193701223407586, 'alpha': 3.053331684650618e-06, 'subsample': 0.7054367769523452, 'colsample_bytree': 0.7872952155229872}. Best is trial 21 with value: 0.37781070621818075.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:50:13,204]\u001b[0m Trial 23 finished with value: 0.36763040138131575 and parameters: {'booster': 'gblinear', 'lambda': 0.04150814047497477, 'alpha': 1.1384425390874232e-07, 'subsample': 0.6465532447115154, 'colsample_bytree': 0.8764670559623107}. Best is trial 21 with value: 0.37781070621818075.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:50:38,067]\u001b[0m Trial 24 finished with value: 0.36231489407763745 and parameters: {'booster': 'gblinear', 'lambda': 0.010596436134845493, 'alpha': 0.0007128289598698895, 'subsample': 0.7763100020647775, 'colsample_bytree': 0.7250851517368734}. Best is trial 21 with value: 0.37781070621818075.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:51:02,311]\u001b[0m Trial 25 finished with value: 0.37095163902091477 and parameters: {'booster': 'gblinear', 'lambda': 0.0036960553999389923, 'alpha': 2.1721293509905405e-06, 'subsample': 0.764033128388788, 'colsample_bytree': 0.8711624910833531}. Best is trial 21 with value: 0.37781070621818075.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:51:26,466]\u001b[0m Trial 26 finished with value: 0.3627969617516246 and parameters: {'booster': 'gblinear', 'lambda': 0.20216061449648023, 'alpha': 2.4282001068043457e-07, 'subsample': 0.53733145293688, 'colsample_bytree': 0.7585956006243962}. Best is trial 21 with value: 0.37781070621818075.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:51:50,743]\u001b[0m Trial 27 finished with value: 0.36935043275531104 and parameters: {'booster': 'gblinear', 'lambda': 2.662260127197355e-05, 'alpha': 4.2423742055682696e-08, 'subsample': 0.5496346776468475, 'colsample_bytree': 0.6661794472269212}. Best is trial 21 with value: 0.37781070621818075.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:52:42,920]\u001b[0m Trial 28 finished with value: 0.24840840773025036 and parameters: {'booster': 'gbtree', 'lambda': 0.0002519491904975218, 'alpha': 1.0114633636328688e-08, 'subsample': 0.6688654555180672, 'colsample_bytree': 0.8462144028279771, 'max_depth': 3, 'min_child_weight': 7, 'eta': 0.6587444026174945, 'gamma': 1.0777160337068785e-08, 'grow_policy': 'lossguide'}. Best is trial 21 with value: 0.37781070621818075.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:56:11,919]\u001b[0m Trial 29 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 7.353596809927322e-07, 'alpha': 3.263271981604514e-05, 'subsample': 0.9509819653732061, 'colsample_bytree': 0.9836545703977853, 'max_depth': 5, 'min_child_weight': 2, 'eta': 0.007668202923953244, 'gamma': 4.3860829542908486e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 5.196507871679943e-07, 'skip_drop': 1.5536270863596663e-05}. Best is trial 21 with value: 0.37781070621818075.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "  Value: 0.37781070621818075\n",
      "  Params: \n",
      "    booster: gblinear\n",
      "    lambda: 0.11866430098752791\n",
      "    alpha: 7.678107916591738e-08\n",
      "    subsample: 0.68972185719216\n",
      "    colsample_bytree: 0.7875582283226814\n"
     ]
    }
   ],
   "source": [
    "# Filtered dataset\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u[utils.cols_with_positive_values(y_max_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_max)\n",
    "    return metric.hybrid_mcc\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_xgboost_regression_filtered_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-12 01:56:12,039]\u001b[0m A new study created in memory with name: no-name-918ea2be-dc23-4399-89c6-fbbb65954b0e\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:57:05,329]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.13003119089952495, 'alpha': 0.00012180234152179062, 'subsample': 0.4803928298401274, 'colsample_bytree': 0.5640353488154003, 'max_depth': 7, 'min_child_weight': 10, 'eta': 1.1909090593899545e-07, 'gamma': 0.02102995036875599, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:57:41,242]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.006867768124413219, 'alpha': 0.06802782990327548, 'subsample': 0.5473302496546077, 'colsample_bytree': 0.5033349968198105, 'max_depth': 3, 'min_child_weight': 6, 'eta': 0.0014426047819873337, 'gamma': 4.3175875977814396e-08, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:58:23,527]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.6267285634262673, 'alpha': 0.2812467855736987, 'subsample': 0.3513238740745811, 'colsample_bytree': 0.5272759494554748, 'max_depth': 9, 'min_child_weight': 4, 'eta': 1.207923215558726e-06, 'gamma': 4.282852582347085e-06, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 01:58:43,809]\u001b[0m Trial 3 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 6.987024044400568e-06, 'alpha': 0.010077257890201665, 'subsample': 0.7310693597144127, 'colsample_bytree': 0.918139214937532}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:02:43,184]\u001b[0m Trial 4 finished with value: 0.4935531948367856 and parameters: {'booster': 'dart', 'lambda': 2.6617071957899074e-08, 'alpha': 0.2050857241726396, 'subsample': 0.635274323429734, 'colsample_bytree': 0.9580377707447685, 'max_depth': 9, 'min_child_weight': 10, 'eta': 0.6166199436712834, 'gamma': 6.465852068278406e-06, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.0001553418730455465, 'skip_drop': 4.5589455750307743e-07}. Best is trial 4 with value: 0.4935531948367856.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:04:00,348]\u001b[0m Trial 5 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 1.6645443868356906e-05, 'alpha': 5.818031800573416e-07, 'subsample': 0.8492787024058934, 'colsample_bytree': 0.5323958171741097, 'max_depth': 9, 'min_child_weight': 4, 'eta': 0.0011234515824151048, 'gamma': 4.04340223995153e-06, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.4935531948367856.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:04:21,142]\u001b[0m Trial 6 finished with value: 0.0 and parameters: {'booster': 'gblinear', 'lambda': 2.263739616468765e-07, 'alpha': 0.007290884646064507, 'subsample': 0.7182806149053054, 'colsample_bytree': 0.4334861573047539}. Best is trial 4 with value: 0.4935531948367856.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:04:41,759]\u001b[0m Trial 7 finished with value: 0.30264760203133373 and parameters: {'booster': 'gblinear', 'lambda': 0.0005960271901618273, 'alpha': 0.00040172149331542373, 'subsample': 0.4238678187389958, 'colsample_bytree': 0.519183893601701}. Best is trial 4 with value: 0.4935531948367856.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:05:48,560]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 2.978819263983111e-08, 'alpha': 0.0007272284690991056, 'subsample': 0.845857377598292, 'colsample_bytree': 0.37919726746323135, 'max_depth': 9, 'min_child_weight': 4, 'eta': 6.856267750018891e-08, 'gamma': 2.4541656287082202e-08, 'grow_policy': 'depthwise'}. Best is trial 4 with value: 0.4935531948367856.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:06:21,909]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'booster': 'gbtree', 'lambda': 0.3083179486056979, 'alpha': 6.365573620583165e-05, 'subsample': 0.23653734002820326, 'colsample_bytree': 0.321555572695132, 'max_depth': 5, 'min_child_weight': 5, 'eta': 4.642448401596315e-05, 'gamma': 3.410104540913846e-08, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.4935531948367856.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:09:54,372]\u001b[0m Trial 10 finished with value: 0.49638884337238803 and parameters: {'booster': 'dart', 'lambda': 1.1025675131229225e-06, 'alpha': 8.21397557717605e-08, 'subsample': 0.9944443140771942, 'colsample_bytree': 0.9872321237615512, 'max_depth': 7, 'min_child_weight': 10, 'eta': 0.6650110743223804, 'gamma': 0.004002253896595571, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.0002742975579870929, 'skip_drop': 4.1434565536469434e-07}. Best is trial 10 with value: 0.49638884337238803.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:13:17,362]\u001b[0m Trial 11 finished with value: 0.46846406649366634 and parameters: {'booster': 'dart', 'lambda': 4.161977503632759e-07, 'alpha': 4.896032826194252e-07, 'subsample': 0.9946739533511085, 'colsample_bytree': 0.9910372757715535, 'max_depth': 7, 'min_child_weight': 10, 'eta': 0.8814413842238189, 'gamma': 0.009428517087600388, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.00026569481852684046, 'skip_drop': 2.777813985200716e-07}. Best is trial 10 with value: 0.49638884337238803.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:16:45,591]\u001b[0m Trial 12 finished with value: 0.4723586891614195 and parameters: {'booster': 'dart', 'lambda': 1.1110415234367272e-08, 'alpha': 2.091085953772177e-08, 'subsample': 0.9807005344387311, 'colsample_bytree': 0.7864132763178472, 'max_depth': 7, 'min_child_weight': 8, 'eta': 0.6743522693775731, 'gamma': 0.0003227652330859802, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.00014329255664528894, 'skip_drop': 4.3089669287386504e-07}. Best is trial 10 with value: 0.49638884337238803.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:19:19,734]\u001b[0m Trial 13 finished with value: 0.5507230040277439 and parameters: {'booster': 'dart', 'lambda': 9.622429881324058e-07, 'alpha': 3.483838226207578e-06, 'subsample': 0.6803509962236005, 'colsample_bytree': 0.7119788964194889, 'max_depth': 5, 'min_child_weight': 8, 'eta': 0.040633195507637214, 'gamma': 0.26321737837296744, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.11874387174133837, 'skip_drop': 0.0013915464172011593}. Best is trial 13 with value: 0.5507230040277439.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:21:07,362]\u001b[0m Trial 14 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 3.243852861645491e-06, 'alpha': 4.790000055856466e-06, 'subsample': 0.85363203026669, 'colsample_bytree': 0.7266692971863535, 'max_depth': 5, 'min_child_weight': 8, 'eta': 0.030083348647577807, 'gamma': 0.6263133867330963, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.6238827504327413, 'skip_drop': 0.08345407064064507}. Best is trial 13 with value: 0.5507230040277439.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:22:58,140]\u001b[0m Trial 15 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 5.473557420566518e-05, 'alpha': 3.453580630990646e-08, 'subsample': 0.6858033165831222, 'colsample_bytree': 0.204843724765555, 'max_depth': 3, 'min_child_weight': 8, 'eta': 0.016599549130614587, 'gamma': 0.496393509672306, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.2462334491682513, 'skip_drop': 0.0023751935034302743}. Best is trial 13 with value: 0.5507230040277439.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:25:52,405]\u001b[0m Trial 16 finished with value: 0.30642233075430136 and parameters: {'booster': 'dart', 'lambda': 6.485123646435502e-07, 'alpha': 4.819322732347956e-06, 'subsample': 0.7842238116308324, 'colsample_bytree': 0.7028174422098294, 'max_depth': 5, 'min_child_weight': 2, 'eta': 0.021193899278940373, 'gamma': 0.004594585753057801, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.3046484589862563e-08, 'skip_drop': 0.00011628800789780743}. Best is trial 13 with value: 0.5507230040277439.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:28:46,047]\u001b[0m Trial 17 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.0004479456524318682, 'alpha': 5.651740479725831e-07, 'subsample': 0.5677567016695498, 'colsample_bytree': 0.8543964129677599, 'max_depth': 5, 'min_child_weight': 9, 'eta': 2.922685621945991e-05, 'gamma': 0.06867022392711324, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.020635744711492977, 'skip_drop': 8.12661769380728e-05}. Best is trial 13 with value: 0.5507230040277439.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:32:20,295]\u001b[0m Trial 18 finished with value: 0.5349009554313296 and parameters: {'booster': 'dart', 'lambda': 1.645175228083237e-07, 'alpha': 8.788713570513743e-06, 'subsample': 0.9514765935488962, 'colsample_bytree': 0.8628507914984571, 'max_depth': 7, 'min_child_weight': 7, 'eta': 0.0939208079484401, 'gamma': 0.000699437675501173, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 2.571037967039448e-07, 'skip_drop': 4.353384839343185e-05}. Best is trial 13 with value: 0.5507230040277439.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:34:46,402]\u001b[0m Trial 19 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 1.1250863914619637e-07, 'alpha': 2.9961244230449537e-05, 'subsample': 0.910845534013972, 'colsample_bytree': 0.6382386074957842, 'max_depth': 3, 'min_child_weight': 7, 'eta': 0.0009867689295622118, 'gamma': 0.00021828385722022747, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.1494885745683481e-07, 'skip_drop': 0.00010072481698363062}. Best is trial 13 with value: 0.5507230040277439.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:37:44,059]\u001b[0m Trial 20 finished with value: 0.5594055560376083 and parameters: {'booster': 'dart', 'lambda': 4.470628135447379e-05, 'alpha': 3.4792234772270514e-06, 'subsample': 0.6259758681315603, 'colsample_bytree': 0.8280148084846369, 'max_depth': 5, 'min_child_weight': 6, 'eta': 0.04701116977074125, 'gamma': 0.07967427068202983, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.9636884121498285e-06, 'skip_drop': 0.006287780198645211}. Best is trial 20 with value: 0.5594055560376083.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:40:38,405]\u001b[0m Trial 21 finished with value: 0.5657099535987722 and parameters: {'booster': 'dart', 'lambda': 7.65428788598672e-05, 'alpha': 9.035084363006114e-06, 'subsample': 0.6206650972079631, 'colsample_bytree': 0.846414969144526, 'max_depth': 5, 'min_child_weight': 6, 'eta': 0.06490336971367261, 'gamma': 0.09927032777500494, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.2631791834733519e-06, 'skip_drop': 0.006856460931734863}. Best is trial 21 with value: 0.5657099535987722.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:43:29,108]\u001b[0m Trial 22 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.0003819738543358502, 'alpha': 2.1441767905075074e-06, 'subsample': 0.6335428287108862, 'colsample_bytree': 0.7652299193062502, 'max_depth': 5, 'min_child_weight': 6, 'eta': 0.004615394593901356, 'gamma': 0.12283282258458142, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 4.1150061795953406e-06, 'skip_drop': 0.01555088284009262}. Best is trial 21 with value: 0.5657099535987722.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:46:18,939]\u001b[0m Trial 23 finished with value: 0.5693427502449294 and parameters: {'booster': 'dart', 'lambda': 0.005458178901423617, 'alpha': 1.6298642466112607e-05, 'subsample': 0.50984523082815, 'colsample_bytree': 0.8403762589775882, 'max_depth': 5, 'min_child_weight': 5, 'eta': 0.10008488866912961, 'gamma': 0.08234411404596774, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 3.4743978423745048e-06, 'skip_drop': 0.9115118249667867}. Best is trial 23 with value: 0.5693427502449294.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:48:46,816]\u001b[0m Trial 24 finished with value: 0.544949292527675 and parameters: {'booster': 'dart', 'lambda': 0.006298312443441367, 'alpha': 4.080886962364021e-05, 'subsample': 0.5006698320516644, 'colsample_bytree': 0.8446863467683313, 'max_depth': 3, 'min_child_weight': 5, 'eta': 0.108319150853278, 'gamma': 0.07161639880751822, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 4.18689555186241e-06, 'skip_drop': 0.4216939204221417}. Best is trial 23 with value: 0.5693427502449294.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:49:07,899]\u001b[0m Trial 25 finished with value: 0.2878896950877828 and parameters: {'booster': 'gblinear', 'lambda': 0.006114976380740556, 'alpha': 0.0005140641029041789, 'subsample': 0.39909905791161593, 'colsample_bytree': 0.6345470328146374}. Best is trial 23 with value: 0.5693427502449294.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:52:04,145]\u001b[0m Trial 26 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 8.363141890946727e-05, 'alpha': 1.943104566458087e-05, 'subsample': 0.5961124280912112, 'colsample_bytree': 0.8867789834584059, 'max_depth': 5, 'min_child_weight': 5, 'eta': 0.00019098427503864748, 'gamma': 0.0017262339198628853, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 3.7757574091957894e-06, 'skip_drop': 0.7027096092101833}. Best is trial 23 with value: 0.5693427502449294.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:54:49,163]\u001b[0m Trial 27 finished with value: 0.5552207346534699 and parameters: {'booster': 'dart', 'lambda': 0.031396697963098505, 'alpha': 2.1253134523145564e-07, 'subsample': 0.308989830202888, 'colsample_bytree': 0.8002506445637525, 'max_depth': 5, 'min_child_weight': 7, 'eta': 0.12120952482567914, 'gamma': 0.024967441812226484, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 5.370449229110389e-07, 'skip_drop': 0.020832631208733538}. Best is trial 23 with value: 0.5693427502449294.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:57:20,035]\u001b[0m Trial 28 finished with value: 0.0 and parameters: {'booster': 'dart', 'lambda': 0.0010528250763866518, 'alpha': 1.2540716886598342e-06, 'subsample': 0.5240668818704328, 'colsample_bytree': 0.9132664515235841, 'max_depth': 3, 'min_child_weight': 3, 'eta': 0.00490239000087741, 'gamma': 4.453038971548774e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 9.07861404102689e-06, 'skip_drop': 0.00411664514487653}. Best is trial 23 with value: 0.5693427502449294.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 02:57:41,495]\u001b[0m Trial 29 finished with value: 0.3096927996317428 and parameters: {'booster': 'gblinear', 'lambda': 1.891667715525064e-05, 'alpha': 0.00011531024332652975, 'subsample': 0.45908499396091285, 'colsample_bytree': 0.8159231745929135}. Best is trial 23 with value: 0.5693427502449294.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "  Value: 0.5693427502449294\n",
      "  Params: \n",
      "    booster: dart\n",
      "    lambda: 0.005458178901423617\n",
      "    alpha: 1.6298642466112607e-05\n",
      "    subsample: 0.50984523082815\n",
      "    colsample_bytree: 0.8403762589775882\n",
      "    max_depth: 5\n",
      "    min_child_weight: 5\n",
      "    eta: 0.10008488866912961\n",
      "    gamma: 0.08234411404596774\n",
      "    grow_policy: depthwise\n",
      "    sample_type: uniform\n",
      "    normalize_type: tree\n",
      "    rate_drop: 3.4743978423745048e-06\n",
      "    skip_drop: 0.9115118249667867\n"
     ]
    }
   ],
   "source": [
    "# FILTERED DATASET\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u[utils.cols_with_positive_values(y_min_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    model = my_ai.Context(my_ai.XGBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_min)\n",
    "    return metric.hybrid_mcc\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_xgboost_regression_filtered_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-12 02:57:41,657]\u001b[0m A new study created in memory with name: no-name-15793cbf-27bf-4ab2-812c-ff6da50973d4\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 03:01:02,552]\u001b[0m Trial 0 finished with value: 0.4436261349663361 and parameters: {'n_estimators': 68, 'learning_rate': 0.7324976560528268, 'loss': 'absolute_error'}. Best is trial 0 with value: 0.4436261349663361.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 03:22:50,594]\u001b[0m Trial 1 finished with value: 0.2929447559263532 and parameters: {'n_estimators': 554, 'learning_rate': 0.8268173671691834, 'loss': 'squared_error'}. Best is trial 0 with value: 0.4436261349663361.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 03:52:39,049]\u001b[0m Trial 2 finished with value: 0.4922003079283852 and parameters: {'n_estimators': 776, 'learning_rate': 0.4959842937898038, 'loss': 'absolute_error'}. Best is trial 2 with value: 0.4922003079283852.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 04:26:36,476]\u001b[0m Trial 3 finished with value: 0.2741372296059747 and parameters: {'n_estimators': 838, 'learning_rate': 0.541835087979734, 'loss': 'squared_error'}. Best is trial 2 with value: 0.4922003079283852.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 04:27:37,632]\u001b[0m Trial 4 finished with value: 0.5613813425074791 and parameters: {'n_estimators': 12, 'learning_rate': 0.4364883972500652, 'loss': 'squared_error'}. Best is trial 4 with value: 0.5613813425074791.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 04:28:49,446]\u001b[0m Trial 5 finished with value: 0.5536788542048462 and parameters: {'n_estimators': 13, 'learning_rate': 0.10656731885239631, 'loss': 'squared_error'}. Best is trial 4 with value: 0.5613813425074791.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 04:51:47,575]\u001b[0m Trial 6 finished with value: 0.4794884073944671 and parameters: {'n_estimators': 603, 'learning_rate': 0.3419020715327206, 'loss': 'absolute_error'}. Best is trial 4 with value: 0.5613813425074791.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 04:52:47,908]\u001b[0m Trial 7 finished with value: 0.5339490458844917 and parameters: {'n_estimators': 12, 'learning_rate': 0.7485711232984937, 'loss': 'squared_error'}. Best is trial 4 with value: 0.5613813425074791.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 04:53:58,596]\u001b[0m Trial 8 finished with value: 0.43517756735378627 and parameters: {'n_estimators': 18, 'learning_rate': 0.15883890604107326, 'loss': 'absolute_error'}. Best is trial 4 with value: 0.5613813425074791.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:23:13,705]\u001b[0m Trial 9 finished with value: 0.2621309507438707 and parameters: {'n_estimators': 749, 'learning_rate': 0.5233272737085887, 'loss': 'squared_error'}. Best is trial 4 with value: 0.5613813425074791.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:26:08,879]\u001b[0m Trial 10 finished with value: 0.5449249323529112 and parameters: {'n_estimators': 56, 'learning_rate': 0.234502867321217, 'loss': 'squared_error'}. Best is trial 4 with value: 0.5613813425074791.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:28:07,500]\u001b[0m Trial 11 finished with value: 0.5830610559968892 and parameters: {'n_estimators': 32, 'learning_rate': 0.10731887947622534, 'loss': 'squared_error'}. Best is trial 11 with value: 0.5830610559968892.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:30:02,309]\u001b[0m Trial 12 finished with value: 0.5262533298652251 and parameters: {'n_estimators': 30, 'learning_rate': 0.27491011504588797, 'loss': 'squared_error'}. Best is trial 11 with value: 0.5830610559968892.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:31:55,573]\u001b[0m Trial 13 finished with value: 0.5832949193199729 and parameters: {'n_estimators': 35, 'learning_rate': 0.10151303474774298, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:41:21,394]\u001b[0m Trial 14 finished with value: 0.3236705765035564 and parameters: {'n_estimators': 234, 'learning_rate': 0.11647038858504614, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:43:09,989]\u001b[0m Trial 15 finished with value: 0.5801000975959072 and parameters: {'n_estimators': 34, 'learning_rate': 0.16590653682177103, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:49:24,249]\u001b[0m Trial 16 finished with value: 0.4036959247910608 and parameters: {'n_estimators': 143, 'learning_rate': 0.15325262957674962, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:51:03,638]\u001b[0m Trial 17 finished with value: 0.5753379439102358 and parameters: {'n_estimators': 26, 'learning_rate': 0.20795616130090372, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:53:43,245]\u001b[0m Trial 18 finished with value: 0.462785837354031 and parameters: {'n_estimators': 51, 'learning_rate': 0.1276037315699407, 'loss': 'absolute_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 05:59:32,676]\u001b[0m Trial 19 finished with value: 0.5685921585745073 and parameters: {'n_estimators': 123, 'learning_rate': 0.10027999988610686, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:10:34,211]\u001b[0m Trial 20 finished with value: 0.3028770244184285 and parameters: {'n_estimators': 251, 'learning_rate': 0.20161680513022354, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:12:41,747]\u001b[0m Trial 21 finished with value: 0.5742554734242191 and parameters: {'n_estimators': 35, 'learning_rate': 0.14566384330307783, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:14:14,442]\u001b[0m Trial 22 finished with value: 0.5794321670104174 and parameters: {'n_estimators': 20, 'learning_rate': 0.17746866914845152, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:16:32,508]\u001b[0m Trial 23 finished with value: 0.5764945753283008 and parameters: {'n_estimators': 40, 'learning_rate': 0.13014322236438988, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:20:16,617]\u001b[0m Trial 24 finished with value: 0.5742250002026785 and parameters: {'n_estimators': 76, 'learning_rate': 0.10035255151374992, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:21:46,299]\u001b[0m Trial 25 finished with value: 0.5831941601569064 and parameters: {'n_estimators': 19, 'learning_rate': 0.1292554921152197, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:23:15,049]\u001b[0m Trial 26 finished with value: 0.4285793281534024 and parameters: {'n_estimators': 18, 'learning_rate': 0.12860025474081674, 'loss': 'absolute_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:24:54,769]\u001b[0m Trial 27 finished with value: 0.5670324251424779 and parameters: {'n_estimators': 23, 'learning_rate': 0.31531256938568897, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:27:42,288]\u001b[0m Trial 28 finished with value: 0.5775561985382086 and parameters: {'n_estimators': 45, 'learning_rate': 0.12167068535947802, 'loss': 'squared_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:31:56,223]\u001b[0m Trial 29 finished with value: 0.48075383608658334 and parameters: {'n_estimators': 80, 'learning_rate': 0.961875129627278, 'loss': 'absolute_error'}. Best is trial 13 with value: 0.5832949193199729.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "  Value: 0.5832949193199729\n",
      "  Params: \n",
      "    n_estimators: 35\n",
      "    learning_rate: 0.10151303474774298\n",
      "    loss: squared_error\n"
     ]
    }
   ],
   "source": [
    "# Same implementation as above, but for Gradient Boosting Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_max)\n",
    "    return metric.hybrid_mcc\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_gradient_boost_regression_sparse_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-12 06:31:56,336]\u001b[0m A new study created in memory with name: no-name-0de84fcd-729c-4095-ab17-923a4fa1fa4c\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:37:43,508]\u001b[0m Trial 0 finished with value: 0.5011759326079707 and parameters: {'n_estimators': 135, 'learning_rate': 0.4753077586826183, 'loss': 'absolute_error'}. Best is trial 0 with value: 0.5011759326079707.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:39:33,552]\u001b[0m Trial 1 finished with value: 0.4967104412408474 and parameters: {'n_estimators': 29, 'learning_rate': 0.1536040432198943, 'loss': 'absolute_error'}. Best is trial 0 with value: 0.5011759326079707.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:48:23,594]\u001b[0m Trial 2 finished with value: 0.530351602545954 and parameters: {'n_estimators': 292, 'learning_rate': 0.42999779581535297, 'loss': 'squared_error'}. Best is trial 2 with value: 0.530351602545954.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:50:30,660]\u001b[0m Trial 3 finished with value: 0.49515805695015075 and parameters: {'n_estimators': 55, 'learning_rate': 0.8854087148777033, 'loss': 'absolute_error'}. Best is trial 2 with value: 0.530351602545954.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 06:55:23,986]\u001b[0m Trial 4 finished with value: 0.5567152278375864 and parameters: {'n_estimators': 181, 'learning_rate': 0.2889247743547775, 'loss': 'squared_error'}. Best is trial 4 with value: 0.5567152278375864.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:03:18,643]\u001b[0m Trial 5 finished with value: 0.5390878050318011 and parameters: {'n_estimators': 311, 'learning_rate': 0.5145137867376456, 'loss': 'squared_error'}. Best is trial 4 with value: 0.5567152278375864.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:23:04,936]\u001b[0m Trial 6 finished with value: 0.5073623564493835 and parameters: {'n_estimators': 760, 'learning_rate': 0.7147534756767496, 'loss': 'squared_error'}. Best is trial 4 with value: 0.5567152278375864.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:39:46,744]\u001b[0m Trial 7 finished with value: 0.502416998648385 and parameters: {'n_estimators': 710, 'learning_rate': 0.5615663298653091, 'loss': 'absolute_error'}. Best is trial 4 with value: 0.5567152278375864.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:42:05,674]\u001b[0m Trial 8 finished with value: 0.5557326301323661 and parameters: {'n_estimators': 89, 'learning_rate': 0.36854074808548765, 'loss': 'squared_error'}. Best is trial 4 with value: 0.5567152278375864.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:43:01,805]\u001b[0m Trial 9 finished with value: 0.49373602805175915 and parameters: {'n_estimators': 21, 'learning_rate': 0.15112658547073957, 'loss': 'absolute_error'}. Best is trial 4 with value: 0.5567152278375864.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:43:38,127]\u001b[0m Trial 10 finished with value: 0.5585366136286235 and parameters: {'n_estimators': 10, 'learning_rate': 0.23558077080551124, 'loss': 'squared_error'}. Best is trial 10 with value: 0.5585366136286235.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:44:19,554]\u001b[0m Trial 11 finished with value: 0.5699723855583361 and parameters: {'n_estimators': 14, 'learning_rate': 0.252682131899224, 'loss': 'squared_error'}. Best is trial 11 with value: 0.5699723855583361.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:44:55,541]\u001b[0m Trial 12 finished with value: 0.5679058544798401 and parameters: {'n_estimators': 10, 'learning_rate': 0.22572478404606136, 'loss': 'squared_error'}. Best is trial 11 with value: 0.5699723855583361.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:45:32,064]\u001b[0m Trial 13 finished with value: 0.5558528034916818 and parameters: {'n_estimators': 10, 'learning_rate': 0.20806576776433977, 'loss': 'squared_error'}. Best is trial 11 with value: 0.5699723855583361.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:46:29,579]\u001b[0m Trial 14 finished with value: 0.5644098598853446 and parameters: {'n_estimators': 26, 'learning_rate': 0.10441991572275969, 'loss': 'squared_error'}. Best is trial 11 with value: 0.5699723855583361.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:47:16,439]\u001b[0m Trial 15 finished with value: 0.5630422693971533 and parameters: {'n_estimators': 18, 'learning_rate': 0.2890258930901476, 'loss': 'squared_error'}. Best is trial 11 with value: 0.5699723855583361.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:48:37,348]\u001b[0m Trial 16 finished with value: 0.5702675427364597 and parameters: {'n_estimators': 44, 'learning_rate': 0.18451836980227043, 'loss': 'squared_error'}. Best is trial 16 with value: 0.5702675427364597.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:50:07,181]\u001b[0m Trial 17 finished with value: 0.5666019169065274 and parameters: {'n_estimators': 50, 'learning_rate': 0.1552054080148121, 'loss': 'squared_error'}. Best is trial 16 with value: 0.5702675427364597.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:51:30,114]\u001b[0m Trial 18 finished with value: 0.5770484591592312 and parameters: {'n_estimators': 45, 'learning_rate': 0.10053825502708152, 'loss': 'squared_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:52:55,560]\u001b[0m Trial 19 finished with value: 0.5742020872944352 and parameters: {'n_estimators': 47, 'learning_rate': 0.10139503418316327, 'loss': 'squared_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:54:51,546]\u001b[0m Trial 20 finished with value: 0.5704274363616346 and parameters: {'n_estimators': 70, 'learning_rate': 0.10035029394913213, 'loss': 'squared_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:57:00,842]\u001b[0m Trial 21 finished with value: 0.5691299986868921 and parameters: {'n_estimators': 80, 'learning_rate': 0.10146161699208764, 'loss': 'squared_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:58:11,554]\u001b[0m Trial 22 finished with value: 0.5738895504025172 and parameters: {'n_estimators': 36, 'learning_rate': 0.12171042259729058, 'loss': 'squared_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 07:59:23,813]\u001b[0m Trial 23 finished with value: 0.5759263760573172 and parameters: {'n_estimators': 36, 'learning_rate': 0.12919017469490673, 'loss': 'squared_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:00:33,643]\u001b[0m Trial 24 finished with value: 0.5735474324881176 and parameters: {'n_estimators': 35, 'learning_rate': 0.12662398749228676, 'loss': 'squared_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:03:42,589]\u001b[0m Trial 25 finished with value: 0.5763537998738502 and parameters: {'n_estimators': 127, 'learning_rate': 0.126968545363802, 'loss': 'squared_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:07:06,520]\u001b[0m Trial 26 finished with value: 0.5205608842749847 and parameters: {'n_estimators': 129, 'learning_rate': 0.1301559801366665, 'loss': 'absolute_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:13:46,295]\u001b[0m Trial 27 finished with value: 0.5677250148004167 and parameters: {'n_estimators': 291, 'learning_rate': 0.17493922986120677, 'loss': 'squared_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:18:03,029]\u001b[0m Trial 28 finished with value: 0.5710635615408813 and parameters: {'n_estimators': 179, 'learning_rate': 0.12458389953413675, 'loss': 'squared_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:21:14,670]\u001b[0m Trial 29 finished with value: 0.521484847593253 and parameters: {'n_estimators': 120, 'learning_rate': 0.17986766164100862, 'loss': 'absolute_error'}. Best is trial 18 with value: 0.5770484591592312.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "  Value: 0.5770484591592312\n",
      "  Params: \n",
      "    n_estimators: 45\n",
      "    learning_rate: 0.10053825502708152\n",
      "    loss: squared_error\n"
     ]
    }
   ],
   "source": [
    "# Same implementation as above, but for Gradient Boosting Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_max)\n",
    "    return metric.hybrid_mcc\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_gradient_boost_regression_sparse_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-12 08:21:14,852]\u001b[0m A new study created in memory with name: no-name-476b1b42-b579-4075-bf2e-7ca7a2dc756b\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:32:13,608]\u001b[0m Trial 0 finished with value: 0.4372020519488369 and parameters: {'n_estimators': 492, 'learning_rate': 0.7692160543645895, 'loss': 'absolute_error'}. Best is trial 0 with value: 0.4372020519488369.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:41:29,789]\u001b[0m Trial 1 finished with value: 0.24859420447800437 and parameters: {'n_estimators': 332, 'learning_rate': 0.2124691816434439, 'loss': 'squared_error'}. Best is trial 0 with value: 0.4372020519488369.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:44:00,306]\u001b[0m Trial 2 finished with value: 0.4465581606115185 and parameters: {'n_estimators': 92, 'learning_rate': 0.37775206744331363, 'loss': 'absolute_error'}. Best is trial 2 with value: 0.4465581606115185.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:46:58,772]\u001b[0m Trial 3 finished with value: 0.25670397345101914 and parameters: {'n_estimators': 103, 'learning_rate': 0.4086253587422581, 'loss': 'squared_error'}. Best is trial 2 with value: 0.4465581606115185.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:48:09,401]\u001b[0m Trial 4 finished with value: 0.45436569259011844 and parameters: {'n_estimators': 40, 'learning_rate': 0.17173721473186995, 'loss': 'absolute_error'}. Best is trial 4 with value: 0.45436569259011844.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:48:52,256]\u001b[0m Trial 5 finished with value: 0.47372998921899107 and parameters: {'n_estimators': 22, 'learning_rate': 0.6799245722728012, 'loss': 'absolute_error'}. Best is trial 5 with value: 0.47372998921899107.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 08:54:14,039]\u001b[0m Trial 6 finished with value: 0.4729710834934998 and parameters: {'n_estimators': 203, 'learning_rate': 0.23252439140964384, 'loss': 'absolute_error'}. Best is trial 5 with value: 0.47372998921899107.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 09:06:06,383]\u001b[0m Trial 7 finished with value: 0.225876192601603 and parameters: {'n_estimators': 426, 'learning_rate': 0.2036359244742391, 'loss': 'squared_error'}. Best is trial 5 with value: 0.47372998921899107.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 09:13:16,316]\u001b[0m Trial 8 finished with value: 0.46407568017519324 and parameters: {'n_estimators': 274, 'learning_rate': 0.24875755313113285, 'loss': 'absolute_error'}. Best is trial 5 with value: 0.47372998921899107.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 09:33:15,757]\u001b[0m Trial 9 finished with value: 0.4943841615568088 and parameters: {'n_estimators': 896, 'learning_rate': 0.5030561636951839, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 09:57:56,538]\u001b[0m Trial 10 finished with value: 0.23780564060329185 and parameters: {'n_estimators': 895, 'learning_rate': 0.11097844783337014, 'loss': 'squared_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 09:58:25,809]\u001b[0m Trial 11 finished with value: 0.4369419967783465 and parameters: {'n_estimators': 13, 'learning_rate': 0.7158750894877736, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 09:58:53,343]\u001b[0m Trial 12 finished with value: 0.4615795082252942 and parameters: {'n_estimators': 12, 'learning_rate': 0.5428786651996697, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 09:59:49,520]\u001b[0m Trial 13 finished with value: 0.4770700526443091 and parameters: {'n_estimators': 32, 'learning_rate': 0.986039787582684, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 10:01:21,028]\u001b[0m Trial 14 finished with value: 0.45393043773359665 and parameters: {'n_estimators': 56, 'learning_rate': 0.9065301037129048, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 10:23:33,399]\u001b[0m Trial 15 finished with value: 0.49055474044266884 and parameters: {'n_estimators': 1000, 'learning_rate': 0.5093656882537718, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 10:44:37,931]\u001b[0m Trial 16 finished with value: 0.4782319857979874 and parameters: {'n_estimators': 742, 'learning_rate': 0.48318379836352454, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 10:50:22,633]\u001b[0m Trial 17 finished with value: 0.4598567163209096 and parameters: {'n_estimators': 175, 'learning_rate': 0.31779458003114713, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 11:23:23,023]\u001b[0m Trial 18 finished with value: 0.22279730651717858 and parameters: {'n_estimators': 960, 'learning_rate': 0.5456777686100615, 'loss': 'squared_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 11:39:42,579]\u001b[0m Trial 19 finished with value: 0.46452867407334486 and parameters: {'n_estimators': 563, 'learning_rate': 0.3553888404479633, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 11:44:22,316]\u001b[0m Trial 20 finished with value: 0.4676014179639845 and parameters: {'n_estimators': 157, 'learning_rate': 0.47662269768167725, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 12:03:02,760]\u001b[0m Trial 21 finished with value: 0.46754961546115403 and parameters: {'n_estimators': 688, 'learning_rate': 0.47429085636032997, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 12:24:00,774]\u001b[0m Trial 22 finished with value: 0.46587729672195893 and parameters: {'n_estimators': 974, 'learning_rate': 0.585319323260233, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 12:41:27,540]\u001b[0m Trial 23 finished with value: 0.4423030230987487 and parameters: {'n_estimators': 692, 'learning_rate': 0.4323782042722278, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 12:51:05,148]\u001b[0m Trial 24 finished with value: 0.4570528646494934 and parameters: {'n_estimators': 379, 'learning_rate': 0.2801559867917532, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 13:05:19,917]\u001b[0m Trial 25 finished with value: 0.4733282215713193 and parameters: {'n_estimators': 649, 'learning_rate': 0.608528284129593, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 13:12:29,597]\u001b[0m Trial 26 finished with value: 0.2562992558444884 and parameters: {'n_estimators': 273, 'learning_rate': 0.8255522073005738, 'loss': 'squared_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 13:26:07,732]\u001b[0m Trial 27 finished with value: 0.4597362551245962 and parameters: {'n_estimators': 539, 'learning_rate': 0.3181598126551394, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 13:45:59,921]\u001b[0m Trial 28 finished with value: 0.4823724791849132 and parameters: {'n_estimators': 776, 'learning_rate': 0.49037714109825453, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 13:56:48,586]\u001b[0m Trial 29 finished with value: 0.4714334611893526 and parameters: {'n_estimators': 490, 'learning_rate': 0.6773434992361159, 'loss': 'absolute_error'}. Best is trial 9 with value: 0.4943841615568088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "  Value: 0.4943841615568088\n",
      "  Params: \n",
      "    n_estimators: 896\n",
      "    learning_rate: 0.5030561636951839\n",
      "    loss: absolute_error\n"
     ]
    }
   ],
   "source": [
    "# FILTERED DATA\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u[utils.cols_with_positive_values(y_max_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_max)\n",
    "    return metric.hybrid_mcc\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_gradient_boost_regression_filtered_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-12 13:56:48,670]\u001b[0m A new study created in memory with name: no-name-b66e76e4-2708-4477-a025-6b52ca748825\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 13:57:13,974]\u001b[0m Trial 0 finished with value: 0.5451986637661478 and parameters: {'n_estimators': 13, 'learning_rate': 0.21973739213760063, 'loss': 'squared_error'}. Best is trial 0 with value: 0.5451986637661478.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 13:57:55,003]\u001b[0m Trial 1 finished with value: 0.4324163991846127 and parameters: {'n_estimators': 25, 'learning_rate': 0.2695519266391299, 'loss': 'absolute_error'}. Best is trial 0 with value: 0.5451986637661478.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 13:58:43,689]\u001b[0m Trial 2 finished with value: 0.4335527183022466 and parameters: {'n_estimators': 31, 'learning_rate': 0.34905028895291246, 'loss': 'absolute_error'}. Best is trial 0 with value: 0.5451986637661478.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:10:35,038]\u001b[0m Trial 3 finished with value: 0.5190477803355567 and parameters: {'n_estimators': 544, 'learning_rate': 0.31645450039025347, 'loss': 'squared_error'}. Best is trial 0 with value: 0.5451986637661478.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:11:25,138]\u001b[0m Trial 4 finished with value: 0.50165307986077 and parameters: {'n_estimators': 32, 'learning_rate': 0.9012006331920576, 'loss': 'squared_error'}. Best is trial 0 with value: 0.5451986637661478.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:12:53,892]\u001b[0m Trial 5 finished with value: 0.43280958341943226 and parameters: {'n_estimators': 57, 'learning_rate': 0.37280378633748484, 'loss': 'absolute_error'}. Best is trial 0 with value: 0.5451986637661478.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:13:25,747]\u001b[0m Trial 6 finished with value: 0.5349946301254195 and parameters: {'n_estimators': 18, 'learning_rate': 0.1924150396152654, 'loss': 'squared_error'}. Best is trial 0 with value: 0.5451986637661478.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:15:59,406]\u001b[0m Trial 7 finished with value: 0.4283758210851994 and parameters: {'n_estimators': 103, 'learning_rate': 0.14513996711739793, 'loss': 'absolute_error'}. Best is trial 0 with value: 0.5451986637661478.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:18:16,758]\u001b[0m Trial 8 finished with value: 0.42849006954751784 and parameters: {'n_estimators': 76, 'learning_rate': 0.14776708743017034, 'loss': 'absolute_error'}. Best is trial 0 with value: 0.5451986637661478.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:18:43,296]\u001b[0m Trial 9 finished with value: 0.5181286230673089 and parameters: {'n_estimators': 10, 'learning_rate': 0.18549100543168523, 'loss': 'squared_error'}. Best is trial 0 with value: 0.5451986637661478.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:23:51,081]\u001b[0m Trial 10 finished with value: 0.5568849743607843 and parameters: {'n_estimators': 195, 'learning_rate': 0.10256296408865478, 'loss': 'squared_error'}. Best is trial 10 with value: 0.5568849743607843.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:29:25,948]\u001b[0m Trial 11 finished with value: 0.508124301007389 and parameters: {'n_estimators': 229, 'learning_rate': 0.5853824170507226, 'loss': 'squared_error'}. Best is trial 10 with value: 0.5568849743607843.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:34:35,747]\u001b[0m Trial 12 finished with value: 0.5640003205376345 and parameters: {'n_estimators': 222, 'learning_rate': 0.11191482162568948, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 14:42:02,041]\u001b[0m Trial 13 finished with value: 0.5583182067543755 and parameters: {'n_estimators': 284, 'learning_rate': 0.1004551841015807, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 15:01:13,967]\u001b[0m Trial 14 finished with value: 0.5448501635847464 and parameters: {'n_estimators': 743, 'learning_rate': 0.10502545521944073, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 15:08:32,056]\u001b[0m Trial 15 finished with value: 0.5543032432715912 and parameters: {'n_estimators': 303, 'learning_rate': 0.1352290909944427, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 15:17:40,298]\u001b[0m Trial 16 finished with value: 0.5564955323021916 and parameters: {'n_estimators': 393, 'learning_rate': 0.10369799416295258, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 15:21:03,211]\u001b[0m Trial 17 finished with value: 0.5256056503654651 and parameters: {'n_estimators': 137, 'learning_rate': 0.4768519955756716, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 15:43:44,543]\u001b[0m Trial 18 finished with value: 0.536863542928787 and parameters: {'n_estimators': 932, 'learning_rate': 0.13063263585725682, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 15:47:56,243]\u001b[0m Trial 19 finished with value: 0.5515304309567267 and parameters: {'n_estimators': 157, 'learning_rate': 0.2424842688942894, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 15:58:14,247]\u001b[0m Trial 20 finished with value: 0.5369230097563906 and parameters: {'n_estimators': 391, 'learning_rate': 0.18073308597912968, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 16:03:15,404]\u001b[0m Trial 21 finished with value: 0.5575516620722585 and parameters: {'n_estimators': 194, 'learning_rate': 0.10293209658090564, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 16:10:05,121]\u001b[0m Trial 22 finished with value: 0.5603471033828181 and parameters: {'n_estimators': 267, 'learning_rate': 0.11931886128143185, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 16:17:31,367]\u001b[0m Trial 23 finished with value: 0.5589086810569538 and parameters: {'n_estimators': 281, 'learning_rate': 0.12479368423516773, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 16:31:19,987]\u001b[0m Trial 24 finished with value: 0.5398312522347104 and parameters: {'n_estimators': 549, 'learning_rate': 0.15465567425811305, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 16:33:49,169]\u001b[0m Trial 25 finished with value: 0.5602104994555167 and parameters: {'n_estimators': 104, 'learning_rate': 0.12175029619801994, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 16:36:12,096]\u001b[0m Trial 26 finished with value: 0.4292740669139736 and parameters: {'n_estimators': 96, 'learning_rate': 0.16749772746098113, 'loss': 'absolute_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 16:37:21,126]\u001b[0m Trial 27 finished with value: 0.5537691414990692 and parameters: {'n_estimators': 44, 'learning_rate': 0.1226563261441691, 'loss': 'squared_error'}. Best is trial 12 with value: 0.5640003205376345.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 16:40:25,609]\u001b[0m Trial 28 finished with value: 0.5655126692733458 and parameters: {'n_estimators': 120, 'learning_rate': 0.21976816397532475, 'loss': 'squared_error'}. Best is trial 28 with value: 0.5655126692733458.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 16:43:49,363]\u001b[0m Trial 29 finished with value: 0.559275858616579 and parameters: {'n_estimators': 134, 'learning_rate': 0.2282321689126864, 'loss': 'squared_error'}. Best is trial 28 with value: 0.5655126692733458.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "  Value: 0.5655126692733458\n",
      "  Params: \n",
      "    n_estimators: 120\n",
      "    learning_rate: 0.21976816397532475\n",
      "    loss: squared_error\n"
     ]
    }
   ],
   "source": [
    "# FILTERED DATA\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u[utils.cols_with_positive_values(y_min_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error'])\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_min)\n",
    "    return metric.hybrid_mcc\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_gradient_boost_regression_filtered_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-12 16:43:49,736]\u001b[0m A new study created in memory with name: no-name-41f46623-fb85-4f1b-91e1-6baf0fe9726a\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 17:00:27,027]\u001b[0m Trial 0 finished with value: 0.1848224343384773 and parameters: {'n_estimators': 700, 'learning_rate': 0.9467454850439413, 'loss': 'log_loss'}. Best is trial 0 with value: 0.1848224343384773.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 17:01:46,969]\u001b[0m Trial 1 finished with value: 0.4564203796934115 and parameters: {'n_estimators': 52, 'learning_rate': 0.15488788414815047, 'loss': 'exponential'}. Best is trial 1 with value: 0.4564203796934115.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 17:02:14,227]\u001b[0m Trial 2 finished with value: 0.3189708387625145 and parameters: {'n_estimators': 18, 'learning_rate': 0.12158016444499965, 'loss': 'exponential'}. Best is trial 1 with value: 0.4564203796934115.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 17:04:22,196]\u001b[0m Trial 3 finished with value: 0.4673112425103831 and parameters: {'n_estimators': 83, 'learning_rate': 0.1363999135976785, 'loss': 'deviance'}. Best is trial 3 with value: 0.4673112425103831.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 17:06:14,315]\u001b[0m Trial 4 finished with value: 0.4648160108169469 and parameters: {'n_estimators': 71, 'learning_rate': 0.20874151093378068, 'loss': 'exponential'}. Best is trial 3 with value: 0.4673112425103831.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 17:07:05,112]\u001b[0m Trial 5 finished with value: 0.20286540413280835 and parameters: {'n_estimators': 32, 'learning_rate': 0.893027723423164, 'loss': 'log_loss'}. Best is trial 3 with value: 0.4673112425103831.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 17:15:43,217]\u001b[0m Trial 6 finished with value: 0.46998954103929463 and parameters: {'n_estimators': 370, 'learning_rate': 0.15669125371126016, 'loss': 'log_loss'}. Best is trial 6 with value: 0.46998954103929463.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 17:39:35,643]\u001b[0m Trial 7 finished with value: 0.477002237195314 and parameters: {'n_estimators': 858, 'learning_rate': 0.13993243340540698, 'loss': 'exponential'}. Best is trial 7 with value: 0.477002237195314.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 17:42:55,346]\u001b[0m Trial 8 finished with value: 0.47401324820528884 and parameters: {'n_estimators': 111, 'learning_rate': 0.3772764224906319, 'loss': 'deviance'}. Best is trial 7 with value: 0.477002237195314.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 17:43:21,952]\u001b[0m Trial 9 finished with value: 0.4481801069696025 and parameters: {'n_estimators': 15, 'learning_rate': 0.1704332611187455, 'loss': 'deviance'}. Best is trial 7 with value: 0.477002237195314.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 17:50:07,559]\u001b[0m Trial 10 finished with value: 0.48429288569904566 and parameters: {'n_estimators': 219, 'learning_rate': 0.37295196433512895, 'loss': 'exponential'}. Best is trial 10 with value: 0.48429288569904566.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 17:56:03,773]\u001b[0m Trial 11 finished with value: 0.4827684612729004 and parameters: {'n_estimators': 253, 'learning_rate': 0.3422508878758288, 'loss': 'exponential'}. Best is trial 10 with value: 0.48429288569904566.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 18:01:19,289]\u001b[0m Trial 12 finished with value: 0.48713193815485917 and parameters: {'n_estimators': 233, 'learning_rate': 0.42165059508345, 'loss': 'exponential'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 18:05:59,657]\u001b[0m Trial 13 finished with value: 0.47728267499534965 and parameters: {'n_estimators': 208, 'learning_rate': 0.5331519154834052, 'loss': 'exponential'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 18:10:01,769]\u001b[0m Trial 14 finished with value: 0.4747053113126177 and parameters: {'n_estimators': 180, 'learning_rate': 0.5161400960495137, 'loss': 'exponential'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 18:19:26,708]\u001b[0m Trial 15 finished with value: 0.4781131115891838 and parameters: {'n_estimators': 418, 'learning_rate': 0.25643703267267093, 'loss': 'exponential'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 18:22:55,606]\u001b[0m Trial 16 finished with value: 0.46877844065505003 and parameters: {'n_estimators': 155, 'learning_rate': 0.4977159629328792, 'loss': 'exponential'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 18:32:19,205]\u001b[0m Trial 17 finished with value: 0.48576507444212935 and parameters: {'n_estimators': 441, 'learning_rate': 0.6601493557395876, 'loss': 'exponential'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 18:42:49,724]\u001b[0m Trial 18 finished with value: 0.45576148917721204 and parameters: {'n_estimators': 478, 'learning_rate': 0.693024564602364, 'loss': 'deviance'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 18:55:24,198]\u001b[0m Trial 19 finished with value: 0.47214624936434735 and parameters: {'n_estimators': 589, 'learning_rate': 0.6828330062307132, 'loss': 'log_loss'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 19:02:36,721]\u001b[0m Trial 20 finished with value: 0.48136493098928135 and parameters: {'n_estimators': 333, 'learning_rate': 0.6579613075798836, 'loss': 'exponential'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 19:05:50,699]\u001b[0m Trial 21 finished with value: 0.4743488201357369 and parameters: {'n_estimators': 137, 'learning_rate': 0.3794242263043516, 'loss': 'exponential'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 19:11:54,570]\u001b[0m Trial 22 finished with value: 0.47483719640650696 and parameters: {'n_estimators': 273, 'learning_rate': 0.28993406522907567, 'loss': 'exponential'}. Best is trial 12 with value: 0.48713193815485917.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 19:30:47,653]\u001b[0m Trial 23 finished with value: 0.4930738124699334 and parameters: {'n_estimators': 913, 'learning_rate': 0.4468180639904589, 'loss': 'exponential'}. Best is trial 23 with value: 0.4930738124699334.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 19:47:54,430]\u001b[0m Trial 24 finished with value: 0.4871997596241166 and parameters: {'n_estimators': 823, 'learning_rate': 0.44895227654025355, 'loss': 'exponential'}. Best is trial 23 with value: 0.4930738124699334.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 20:07:26,852]\u001b[0m Trial 25 finished with value: 0.48358586066869436 and parameters: {'n_estimators': 943, 'learning_rate': 0.4325990018542079, 'loss': 'exponential'}. Best is trial 23 with value: 0.4930738124699334.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 20:20:48,165]\u001b[0m Trial 26 finished with value: 0.4774614812682171 and parameters: {'n_estimators': 609, 'learning_rate': 0.2575878246318396, 'loss': 'exponential'}. Best is trial 23 with value: 0.4930738124699334.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 20:39:57,168]\u001b[0m Trial 27 finished with value: 0.48796960024663455 and parameters: {'n_estimators': 923, 'learning_rate': 0.45075151520529066, 'loss': 'exponential'}. Best is trial 23 with value: 0.4930738124699334.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 20:59:23,699]\u001b[0m Trial 28 finished with value: 0.48524772531935917 and parameters: {'n_estimators': 934, 'learning_rate': 0.30362733887092347, 'loss': 'deviance'}. Best is trial 23 with value: 0.4930738124699334.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:11:46,024]\u001b[0m Trial 29 finished with value: 0.3560151308481393 and parameters: {'n_estimators': 604, 'learning_rate': 0.9619600590257781, 'loss': 'log_loss'}. Best is trial 23 with value: 0.4930738124699334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "  Value: 0.4930738124699334\n",
      "  Params: \n",
      "    n_estimators: 913\n",
      "    learning_rate: 0.4468180639904589\n",
      "    loss: exponential\n"
     ]
    }
   ],
   "source": [
    "# Implement as above but for multi-output classification grandient boost classifier.\n",
    "def objective(trial):\n",
    "    # import data0 \n",
    "    y_max_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    # y_max_u_sparse_bool = utils.convert_df_to_bool(y_max_u_sparse_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse_bool[utils.cols_with_positive_values(y_max_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'deviance', 'exponential']) \n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction, columns=utils.cols_with_positive_values(y_max_u_sparse_bool))\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for bus in prediction.columns:\n",
    "        tp += sum((prediction[bus] == 1) & (valid_y[bus] == 1))\n",
    "        tn += sum((prediction[bus] == 0) & (valid_y[bus] == 0))\n",
    "        fp += sum((prediction[bus] == 1) & (valid_y[bus] == 0))\n",
    "        fn += sum((prediction[bus] == 0) & (valid_y[bus] == 1))\n",
    "    mcc_score = (tp*tn - fp*fn) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    return mcc_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_gradient_boost_sparse_classifier_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-12 21:11:46,224]\u001b[0m A new study created in memory with name: no-name-93f1c8d1-b896-47bb-a141-297d8e2b78fc\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:13:12,384]\u001b[0m Trial 0 finished with value: 0.5630408364497607 and parameters: {'n_estimators': 68, 'learning_rate': 0.13734141464671806, 'loss': 'deviance'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:18:42,822]\u001b[0m Trial 1 finished with value: 0.5304288662479494 and parameters: {'n_estimators': 258, 'learning_rate': 0.17157498917476832, 'loss': 'exponential'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:30:19,001]\u001b[0m Trial 2 finished with value: 0.4667997309376381 and parameters: {'n_estimators': 563, 'learning_rate': 0.4327540467018431, 'loss': 'exponential'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:32:41,082]\u001b[0m Trial 3 finished with value: 0.5340181621250789 and parameters: {'n_estimators': 114, 'learning_rate': 0.38847539555189914, 'loss': 'log_loss'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:34:40,854]\u001b[0m Trial 4 finished with value: 0.4986986107828549 and parameters: {'n_estimators': 96, 'learning_rate': 0.5359006427556531, 'loss': 'deviance'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:40:09,492]\u001b[0m Trial 5 finished with value: 0.5139374234622635 and parameters: {'n_estimators': 263, 'learning_rate': 0.2378303204402137, 'loss': 'exponential'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:41:51,608]\u001b[0m Trial 6 finished with value: 0.5320670174321902 and parameters: {'n_estimators': 81, 'learning_rate': 0.4812119658988621, 'loss': 'exponential'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:46:59,839]\u001b[0m Trial 7 finished with value: 0.4498874534532563 and parameters: {'n_estimators': 248, 'learning_rate': 0.7105708897629189, 'loss': 'deviance'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:47:53,057]\u001b[0m Trial 8 finished with value: 0.5215480441850775 and parameters: {'n_estimators': 42, 'learning_rate': 0.8802242096540985, 'loss': 'exponential'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:50:44,567]\u001b[0m Trial 9 finished with value: 0.5062530776600475 and parameters: {'n_estimators': 138, 'learning_rate': 0.43829378946823655, 'loss': 'log_loss'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:50:59,243]\u001b[0m Trial 10 finished with value: 0.49683056060808023 and parameters: {'n_estimators': 11, 'learning_rate': 0.10181343621449412, 'loss': 'deviance'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:51:42,375]\u001b[0m Trial 11 finished with value: 0.5568184572469193 and parameters: {'n_estimators': 34, 'learning_rate': 0.2546296049212303, 'loss': 'log_loss'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:52:20,827]\u001b[0m Trial 12 finished with value: 0.560802366266971 and parameters: {'n_estimators': 30, 'learning_rate': 0.14127063742199933, 'loss': 'log_loss'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 21:52:44,234]\u001b[0m Trial 13 finished with value: 0.5559231877683269 and parameters: {'n_estimators': 18, 'learning_rate': 0.11347673600312652, 'loss': 'log_loss'}. Best is trial 0 with value: 0.5630408364497607.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:53:33,971]\u001b[0m Trial 14 finished with value: 0.5683690676301841 and parameters: {'n_estimators': 39, 'learning_rate': 0.14180135653763062, 'loss': 'deviance'}. Best is trial 14 with value: 0.5683690676301841.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:54:43,321]\u001b[0m Trial 15 finished with value: 0.5688292679956949 and parameters: {'n_estimators': 55, 'learning_rate': 0.18367020564656633, 'loss': 'deviance'}. Best is trial 15 with value: 0.5688292679956949.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:55:47,736]\u001b[0m Trial 16 finished with value: 0.5643883693861663 and parameters: {'n_estimators': 51, 'learning_rate': 0.20535501124007727, 'loss': 'deviance'}. Best is trial 15 with value: 0.5688292679956949.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:56:12,452]\u001b[0m Trial 17 finished with value: 0.580687085134165 and parameters: {'n_estimators': 19, 'learning_rate': 0.3212945724462194, 'loss': 'deviance'}. Best is trial 17 with value: 0.580687085134165.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:56:34,488]\u001b[0m Trial 18 finished with value: 0.5707134323527726 and parameters: {'n_estimators': 17, 'learning_rate': 0.297452156272479, 'loss': 'deviance'}. Best is trial 17 with value: 0.580687085134165.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:56:50,596]\u001b[0m Trial 19 finished with value: 0.5919063880540413 and parameters: {'n_estimators': 12, 'learning_rate': 0.31841154564479685, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:57:05,141]\u001b[0m Trial 20 finished with value: 0.5740641652086793 and parameters: {'n_estimators': 11, 'learning_rate': 0.34151070126793115, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:57:18,512]\u001b[0m Trial 21 finished with value: 0.560781195369296 and parameters: {'n_estimators': 10, 'learning_rate': 0.3324831319910651, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:57:41,858]\u001b[0m Trial 22 finished with value: 0.5775241943545403 and parameters: {'n_estimators': 18, 'learning_rate': 0.3259469121852256, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:58:07,077]\u001b[0m Trial 23 finished with value: 0.5854061549469435 and parameters: {'n_estimators': 19, 'learning_rate': 0.27058466648336876, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:58:37,963]\u001b[0m Trial 24 finished with value: 0.5863250092744297 and parameters: {'n_estimators': 24, 'learning_rate': 0.26288781044056075, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:59:10,756]\u001b[0m Trial 25 finished with value: 0.5844261758003766 and parameters: {'n_estimators': 26, 'learning_rate': 0.2666763308399583, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:59:27,688]\u001b[0m Trial 26 finished with value: 0.5687453609682993 and parameters: {'n_estimators': 13, 'learning_rate': 0.5800757716543115, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 21:59:58,144]\u001b[0m Trial 27 finished with value: 0.5742749730790365 and parameters: {'n_estimators': 24, 'learning_rate': 0.21149259765254302, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 22:18:55,742]\u001b[0m Trial 28 finished with value: 0.4759249429357956 and parameters: {'n_estimators': 920, 'learning_rate': 0.27136068504058936, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jamil\\Documents\\IST\\Thesis\\new_thesis\\code\\AI-to-forecast-constraints-in-the-energy-systems\\env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-12 22:19:13,596]\u001b[0m Trial 29 finished with value: 0.562478240883538 and parameters: {'n_estimators': 14, 'learning_rate': 0.21660959748762812, 'loss': 'deviance'}. Best is trial 19 with value: 0.5919063880540413.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "  Value: 0.5919063880540413\n",
      "  Params: \n",
      "    n_estimators: 12\n",
      "    learning_rate: 0.31841154564479685\n",
      "    loss: deviance\n"
     ]
    }
   ],
   "source": [
    "# Implement as above but for multi-output classification grandient boost classifier.\n",
    "def objective(trial):\n",
    "    # import data0 \n",
    "    y_min_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    # y_min_u_sparse_bool = utils.convert_df_to_bool(y_min_u_sparse_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse_bool[utils.cols_with_positive_values(y_min_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0, log=True) ,\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'deviance', 'exponential']) \n",
    "    }\n",
    "    model = my_ai.Context(my_ai.GradientBoostClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction, columns=utils.cols_with_positive_values(y_min_u_sparse_bool))\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for bus in prediction.columns:\n",
    "        tp += sum((prediction[bus] == 1) & (valid_y[bus] == 1))\n",
    "        tn += sum((prediction[bus] == 0) & (valid_y[bus] == 0))\n",
    "        fp += sum((prediction[bus] == 1) & (valid_y[bus] == 0))\n",
    "        fn += sum((prediction[bus] == 0) & (valid_y[bus] == 1))\n",
    "    mcc_score = (tp*tn - fp*fn) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    return mcc_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_gradient_boost_sparse_classifier_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-12 22:19:13,790]\u001b[0m A new study created in memory with name: no-name-2b44118b-9d6a-4f81-8e86-eafe6cede960\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 22:29:03,587]\u001b[0m Trial 0 finished with value: 0.17935944056633119 and parameters: {'kernel': 'poly', 'C': 6.559448577815634e-05, 'degree': 4, 'gamma': 0.00013943996378102115}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 22:36:30,148]\u001b[0m Trial 1 finished with value: 0.17934301269870043 and parameters: {'kernel': 'rbf', 'C': 0.03736561630204268, 'degree': 1, 'gamma': 1.3937317777251808e-07}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 22:45:02,732]\u001b[0m Trial 2 finished with value: 0.17935514379230397 and parameters: {'kernel': 'rbf', 'C': 0.0021209065232645034, 'degree': 4, 'gamma': 7.682328756615118e-07}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 22:56:54,285]\u001b[0m Trial 3 finished with value: 0.17935473210159725 and parameters: {'kernel': 'rbf', 'C': 6.747379407198382e-06, 'degree': 2, 'gamma': 6.071233165025852e-07}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 23:08:18,065]\u001b[0m Trial 4 finished with value: 0.17935718823978636 and parameters: {'kernel': 'rbf', 'C': 1.5900475866659574e-05, 'degree': 2, 'gamma': 0.0021169751340216958}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 23:15:25,096]\u001b[0m Trial 5 finished with value: 0.17930767625495686 and parameters: {'kernel': 'poly', 'C': 0.0010711868669663871, 'degree': 2, 'gamma': 0.01762916452765538}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 23:24:16,212]\u001b[0m Trial 6 finished with value: 0.179358218715766 and parameters: {'kernel': 'poly', 'C': 1.1022288584112701e-08, 'degree': 2, 'gamma': 0.00018679667219659204}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 23:32:44,989]\u001b[0m Trial 7 finished with value: 0.17935944056633116 and parameters: {'kernel': 'poly', 'C': 9.71951093270275e-06, 'degree': 5, 'gamma': 2.3468728313246382e-06}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 23:40:02,686]\u001b[0m Trial 8 finished with value: 0.1793420547816236 and parameters: {'kernel': 'rbf', 'C': 0.601516930922958, 'degree': 4, 'gamma': 9.396538700100323e-08}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 23:51:39,899]\u001b[0m Trial 9 finished with value: 0.17932375768519493 and parameters: {'kernel': 'rbf', 'C': 0.004697237389403513, 'degree': 5, 'gamma': 0.0003496888117088165}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-12 23:59:23,181]\u001b[0m Trial 10 finished with value: 0.1793544146934007 and parameters: {'kernel': 'poly', 'C': 9.99117001594562e-08, 'degree': 4, 'gamma': 0.29341796968584294}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 00:08:15,559]\u001b[0m Trial 11 finished with value: 0.17935944056633116 and parameters: {'kernel': 'poly', 'C': 1.6973354501104013e-06, 'degree': 5, 'gamma': 1.1239591103184058e-05}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 00:16:53,034]\u001b[0m Trial 12 finished with value: 0.17935944056633116 and parameters: {'kernel': 'poly', 'C': 0.00010931555519381481, 'degree': 5, 'gamma': 1.0241590821025913e-05}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 00:24:39,018]\u001b[0m Trial 13 finished with value: 0.17935944056633116 and parameters: {'kernel': 'poly', 'C': 5.795614713322845e-07, 'degree': 4, 'gamma': 1.1802265973238242e-05}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 00:32:20,591]\u001b[0m Trial 14 finished with value: 0.17935821871580152 and parameters: {'kernel': 'poly', 'C': 0.00012836521233783584, 'degree': 3, 'gamma': 1.0530359867530333e-08}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 00:40:22,734]\u001b[0m Trial 15 finished with value: 0.1793582167917603 and parameters: {'kernel': 'poly', 'C': 3.7674658266967694e-05, 'degree': 3, 'gamma': 0.0041298892903515036}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 00:48:23,138]\u001b[0m Trial 16 finished with value: 0.17935944056633116 and parameters: {'kernel': 'poly', 'C': 0.0003298612508853692, 'degree': 5, 'gamma': 4.0508658122496474e-05}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 00:55:57,677]\u001b[0m Trial 17 finished with value: 0.17935944056632938 and parameters: {'kernel': 'poly', 'C': 5.711352944930376e-07, 'degree': 4, 'gamma': 0.0009403165796426937}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 01:03:12,104]\u001b[0m Trial 18 finished with value: 0.1793573633533949 and parameters: {'kernel': 'poly', 'C': 2.0506547523259095e-07, 'degree': 3, 'gamma': 0.1409682551915424}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 01:10:42,685]\u001b[0m Trial 19 finished with value: 0.17935944056633116 and parameters: {'kernel': 'poly', 'C': 0.00027930974938960944, 'degree': 5, 'gamma': 6.1976903631643e-05}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 01:18:06,920]\u001b[0m Trial 20 finished with value: 0.17924897740493004 and parameters: {'kernel': 'poly', 'C': 0.022552280067375585, 'degree': 4, 'gamma': 0.027420496171373963}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 01:25:36,622]\u001b[0m Trial 21 finished with value: 0.17935944056633116 and parameters: {'kernel': 'poly', 'C': 4.891154900237995e-06, 'degree': 5, 'gamma': 5.1769833049576815e-05}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 01:33:02,658]\u001b[0m Trial 22 finished with value: 0.17935944056633116 and parameters: {'kernel': 'poly', 'C': 0.0005066732098226709, 'degree': 5, 'gamma': 7.038585115500833e-05}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 01:40:29,802]\u001b[0m Trial 23 finished with value: 0.17935944056633116 and parameters: {'kernel': 'poly', 'C': 0.00043660827796137706, 'degree': 5, 'gamma': 2.791772803861481e-05}. Best is trial 0 with value: 0.17935944056633119.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 01:47:53,717]\u001b[0m Trial 24 finished with value: 0.1793594405663312 and parameters: {'kernel': 'poly', 'C': 4.976664382061963e-05, 'degree': 4, 'gamma': 0.00014663661202550966}. Best is trial 24 with value: 0.1793594405663312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 01:55:19,984]\u001b[0m Trial 25 finished with value: 0.17935944056633116 and parameters: {'kernel': 'poly', 'C': 1.4521116876804822e-08, 'degree': 4, 'gamma': 4.51316106374398e-06}. Best is trial 24 with value: 0.1793594405663312.\u001b[0m\n",
      "\u001b[32m[I 2022-10-13 02:02:34,747]\u001b[0m Trial 26 finished with value: 0.1793582187064834 and parameters: {'kernel': 'poly', 'C': 3.407589711311996e-05, 'degree': 3, 'gamma': 0.0007224945523867043}. Best is trial 24 with value: 0.1793594405663312.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Same implementation as above, but for Support Vector Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=y_max_u.columns)\n",
    "    train_y = pd.DataFrame(train_y, columns=y_max_u.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_max)\n",
    "    return metric.hybrid_mcc\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_support_vector_regression_sparse_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Support Vector Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u, test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=y_min_u.columns)\n",
    "    train_y = pd.DataFrame(train_y, columns=y_min_u.columns)\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_min)\n",
    "    return metric.hybrid_mcc\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_support_vector_regression_sparse_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERED DATA\n",
    "# Same implementation as above, but for Support Vector Regression.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u[utils.cols_with_positive_values(y_max_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=utils.cols_with_positive_values(y_max_u))\n",
    "    valid_y = pd.DataFrame(valid_y, columns=utils.cols_with_positive_values(y_max_u))\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_max)\n",
    "    return metric.hybrid_mcc\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_support_vector_regression_filtered_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA FILTERED\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_constr.csv').drop(columns=['timestamps'])\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u[utils.cols_with_positive_values(y_min_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorRegressorStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=utils.cols_with_positive_values(y_min_u))\n",
    "    train_y = pd.DataFrame(train_y, columns=utils.cols_with_positive_values(y_min_u))\n",
    "    # evaluate the regression performance with my metrics\n",
    "    metric = metrics.Metrics()\n",
    "    metric.get_prediction_scores(prediction, valid_y, threshold=threshold_min)\n",
    "    return metric.hybrid_mcc\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_support_vector_regression_filtered_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    #y_max_u = utils.convert_df_to_bool(y_max_u)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u[utils.cols_with_positive_values(y_max_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for bus in prediction.columns:\n",
    "        tp += sum((prediction[bus] == 1) & (valid_y[bus] == 1))\n",
    "        tn += sum((prediction[bus] == 0) & (valid_y[bus] == 0))\n",
    "        fp += sum((prediction[bus] == 1) & (valid_y[bus] == 0))\n",
    "        fn += sum((prediction[bus] == 0) & (valid_y[bus] == 1))\n",
    "    mcc_score = (tp*tn - fp*fn) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    return mcc_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_xgboost_sparse_classifier_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    #y_min_u = utils.convert_df_to_bool(y_min_u)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u[utils.cols_with_positive_values(y_min_u)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x, 'X_test': valid_x, 'y_train': train_y, 'y_test': valid_y}\n",
    "    param = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "    model = my_ai.Context(my_ai.XGBoostClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction , columns=valid_y.columns)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for bus in prediction.columns:\n",
    "        tp += sum((prediction[bus] == 1) & (valid_y[bus] == 1))\n",
    "        tn += sum((prediction[bus] == 0) & (valid_y[bus] == 0))\n",
    "        fp += sum((prediction[bus] == 1) & (valid_y[bus] == 0))\n",
    "        fn += sum((prediction[bus] == 0) & (valid_y[bus] == 1))\n",
    "    if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) > 0:\n",
    "        mcc_score = (tp*tn - fp*fn) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    else: \n",
    "        mcc_score = 0\n",
    "    return mcc_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_xgboost_sparse_classifier_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Support Vector Classifier.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_max_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    # y_max_u_sparse_bool = utils.convert_df_to_bool(y_max_u_sparse_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_max_u_sparse_bool[utils.cols_with_positive_values(y_max_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction, columns=valid_y.columns)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for bus in prediction.columns:\n",
    "        tp += sum((prediction[bus] == 1) & (valid_y[bus] == 1))\n",
    "        tn += sum((prediction[bus] == 0) & (valid_y[bus] == 0))\n",
    "        fp += sum((prediction[bus] == 1) & (valid_y[bus] == 0))\n",
    "        fn += sum((prediction[bus] == 0) & (valid_y[bus] == 1))\n",
    "    if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) > 0:\n",
    "        mcc_score = (tp*tn - fp*fn) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    else:\n",
    "        mcc_score = 0\n",
    "    return mcc_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_support_vector_sparse_classifier_max_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same implementation as above, but for Support Vector Classifier.\n",
    "def objective(trial):\n",
    "    # import data\n",
    "    y_min_u_sparse_bool = pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_min_sparse_bool_constr.csv').drop(columns=['timestamps'])\n",
    "    # y_min_u_sparse_bool = utils.convert_df_to_bool(y_min_u_sparse_bool)\n",
    "    exogenous_data = pd.read_csv('..\\data\\processed\\production\\exogenous_data_extended.csv').drop(columns=['date'])\n",
    "    train_x, valid_x, train_y, valid_y, scaler = utils.split_and_suffle(exogenous_data, y_min_u_sparse_bool[utils.cols_with_positive_values(y_min_u_sparse_bool)], test_size=0.2, scaling=True)\n",
    "    data = {'X_train': train_x,\n",
    "            'X_test': valid_x,\n",
    "            'y_train': train_y,\n",
    "            'y_test': valid_y}\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['poly', 'rbf']),\n",
    "        'C': trial.suggest_float('C', 1e-8, 1.0, log=True),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    }\n",
    "    model = my_ai.Context(my_ai.SupportVectorClassifierStrategy(param))\n",
    "    model.fit(data)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = pd.DataFrame(prediction, columns=valid_y.columns)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for bus in prediction.columns:\n",
    "        tp += sum((prediction[bus] == 1) & (valid_y[bus] == 1))\n",
    "        tn += sum((prediction[bus] == 0) & (valid_y[bus] == 0))\n",
    "        fp += sum((prediction[bus] == 1) & (valid_y[bus] == 0))\n",
    "        fn += sum((prediction[bus] == 0) & (valid_y[bus] == 1))\n",
    "    if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) > 0:\n",
    "        mcc_score = (tp*tn - fp*fn) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    else:\n",
    "        mcc_score = 0\n",
    "    return mcc_score\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "# Write the results to a csv file.\n",
    "with open(\"./hyper_params_results_mcc/params_support_vector_sparse_classifier_min_u.csv\", \"w\") as f:\n",
    "    f.write(\"params,value\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"{},{}\\n\".format(key, value))\n",
    "    f.write(\"value,{}\\n\".format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import beepy\n",
    "beepy.beep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('..\\data\\ground_truth\\\\res_bus_vm_pu_max_sparse_bool_constr.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe4baa4d27e3b73db55d4bb4674105e8dd41faaf9e559c3cc8381041ce15293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
